name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: false
        default: 'standard'
        type: choice
        options:
        - standard
        - comprehensive
        - performance

env:
  PYTHON_VERSION_DEFAULT: '3.11'

jobs:
  # Linting and Code Quality
  lint:
    name: Lint and Format Check
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black isort flake8 mypy
        pip install -r requirements.txt

    - name: Run Black (code formatting)
      run: black --check --diff .
      continue-on-error: true

    - name: Run isort (import sorting)
      run: isort --check-only --diff .
      continue-on-error: true

    - name: Run Flake8 (style guide)
      run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
      continue-on-error: true

    - name: Run Flake8 (warnings)
      run: flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Run MyPy (type checking)
      run: mypy . --ignore-missing-imports --follow-imports=skip
      continue-on-error: true

  # Security Scan
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
        cache: 'pip'

    - name: Install safety
      run: pip install safety

    - name: Run safety check
      run: safety check --json --output safety-report.json || true

    - name: Upload safety report
      uses: actions/upload-artifact@v4
      with:
        name: safety-report
        path: safety-report.json

  # Unit Tests Matrix
  test-matrix:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist  # For parallel test execution

    - name: Create test environment file
      run: |
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run unit tests
      run: |
        pytest tests/ -v \
          --tb=short \
          --cov=. \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term \
          --junit-xml=test-results.xml \
          -m "not e2e and not performance and not network" \
          -x \
          --numprocesses=auto
      env:
        PYTEST_CURRENT_TEST: 1

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-ubuntu-py${{ matrix.python-version }}
        path: |
          test-results.xml
          htmlcov/
          .coverage

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.python-version == env.PYTHON_VERSION_DEFAULT
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [lint, test-matrix]
    if: github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.test_level != 'standard')

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create test directories
      run: |
        mkdir -p test_output/{data,audio,logs}
        chmod 755 test_output

    - name: Run integration tests
      run: |
        pytest tests/test_integration.py -v \
          --tb=short \
          --junit-xml=integration-results.xml \
          --timeout=300
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: |
          integration-results.xml
          test_output/

  # End-to-End Tests
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [integration-tests]
    if: github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.test_level == 'comprehensive')

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Setup test environment
      run: |
        mkdir -p test_output/{data,audio,logs}
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run CLI E2E tests
      run: |
        pytest tests/e2e/test_cli_commands.py -v \
          --tb=short \
          --junit-xml=e2e-cli-results.xml \
          --timeout=600 \
          -m "not network"
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Run pipeline E2E tests
      run: |
        pytest tests/e2e/test_full_pipeline.py -v \
          --tb=short \
          --junit-xml=e2e-pipeline-results.xml \
          --timeout=600
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Run data validation E2E tests
      run: |
        pytest tests/e2e/test_data_validation.py -v \
          --tb=short \
          --junit-xml=e2e-validation-results.xml \
          --timeout=300
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Upload E2E test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-results
        path: |
          e2e-*-results.xml
          test_output/

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [e2e-tests]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.test_level == 'performance'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil  # For memory monitoring

    - name: Setup performance test environment
      run: |
        mkdir -p test_output/{data,audio,logs}
        mkdir -p performance_reports

    - name: Run performance tests
      run: |
        pytest tests/e2e/test_performance.py -v \
          --tb=short \
          --junit-xml=performance-results.xml \
          --timeout=900 \
          -m "performance"
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Generate performance report
      run: |
        echo "Performance test completed" > performance_reports/summary.txt
        if [ -f performance-results.xml ]; then
          echo "Results saved to performance-results.xml" >> performance_reports/summary.txt
        fi

    - name: Upload performance results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          performance-results.xml
          performance_reports/
          test_output/

  # Build and Package
  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: [test-matrix]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
        cache: 'pip'

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build wheel

    - name: Build package
      run: |
        python -m build

    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: python-package
        path: dist/

  # Test Coverage Report
  coverage-report:
    name: Coverage Report
    runs-on: ubuntu-latest
    needs: [test-matrix, integration-tests]
    if: always()

    steps:
    - uses: actions/checkout@v4

    - name: Download coverage artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: test-results-*
        merge-multiple: true
        path: coverage-data/

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION_DEFAULT }}

    - name: Install coverage tools
      run: |
        pip install coverage

    - name: Combine coverage data
      run: |
        coverage combine coverage-data/
        coverage report --show-missing
        coverage html
      continue-on-error: true

    - name: Upload combined coverage
      uses: actions/upload-artifact@v4
      with:
        name: combined-coverage-report
        path: htmlcov/

  # Final Status Check
  ci-success:
    name: CI Success
    runs-on: ubuntu-latest
    needs: [lint, security, test-matrix, integration-tests, build]
    if: always()

    steps:
    - name: Check all jobs status
      run: |
        echo "Lint: ${{ needs.lint.result }}"
        echo "Security: ${{ needs.security.result }}"
        echo "Tests: ${{ needs.test-matrix.result }}"
        echo "Integration: ${{ needs.integration-tests.result }}"
        echo "Build: ${{ needs.build.result }}"

        if [[ "${{ needs.lint.result }}" == "failure" ]]; then
          echo "❌ Linting failed"
          exit 1
        fi

        if [[ "${{ needs.test-matrix.result }}" == "failure" ]]; then
          echo "❌ Unit tests failed"
          exit 1
        fi

        if [[ "${{ needs.integration-tests.result }}" == "failure" ]]; then
          echo "❌ Integration tests failed"
          exit 1
        fi

        if [[ "${{ needs.build.result }}" == "failure" ]]; then
          echo "❌ Build failed"
          exit 1
        fi

        echo "✅ All required checks passed!"

  # Notification (optional)
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [ci-success]
    if: always() && github.event_name == 'push'

    steps:
    - name: Create status summary
      run: |
        if [[ "${{ needs.ci-success.result }}" == "success" ]]; then
          echo "✅ CI Pipeline completed successfully!" >> $GITHUB_STEP_SUMMARY
          echo "- All tests passed" >> $GITHUB_STEP_SUMMARY
          echo "- Code quality checks passed" >> $GITHUB_STEP_SUMMARY
          echo "- Build succeeded" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ CI Pipeline failed" >> $GITHUB_STEP_SUMMARY
          echo "Check the failed jobs for details" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "**Author:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY