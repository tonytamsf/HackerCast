name: E2E Tests

on:
  schedule:
    # Run E2E tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'E2E test scope'
        required: false
        default: 'standard'
        type: choice
        options:
        - standard
        - comprehensive
        - network-included
      environment:
        description: 'Test environment'
        required: false
        default: 'staging'
        type: choice
        options:
        - staging
        - production-like

env:
  PYTHON_VERSION: '3.11'

jobs:
  # CLI Commands E2E Tests
  cli-e2e:
    name: CLI Commands E2E
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Setup test environment
      shell: bash
      run: |
        mkdir -p test_output/{data,audio,logs}
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run CLI E2E tests (without network)
      run: |
        pytest tests/e2e/test_cli_commands.py -v \
          --tb=short \
          --junit-xml=cli-e2e-results-${{ matrix.os }}.xml \
          --timeout=600 \
          -m "not network"
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Run CLI E2E tests (with network)
      if: github.event.inputs.test_scope == 'network-included'
      run: |
        pytest tests/e2e/test_cli_commands.py -v \
          --tb=short \
          --junit-xml=cli-e2e-network-results-${{ matrix.os }}.xml \
          --timeout=900 \
          -m "network"
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Upload CLI E2E results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: cli-e2e-results-${{ matrix.os }}
        path: |
          cli-e2e-*-results-${{ matrix.os }}.xml
          test_output/

  # Full Pipeline E2E Tests
  pipeline-e2e:
    name: Pipeline E2E
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil  # For memory monitoring

    - name: Setup test environment
      run: |
        mkdir -p test_output/{data,audio,logs}
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run pipeline E2E tests
      run: |
        pytest tests/e2e/test_full_pipeline.py -v \
          --tb=short \
          --junit-xml=pipeline-e2e-results.xml \
          --timeout=900 \
          --capture=no
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Collect pipeline artifacts
      if: always()
      run: |
        echo "=== Test Output Directory Contents ===" > pipeline-artifacts.log
        find test_output -type f -ls >> pipeline-artifacts.log
        echo "" >> pipeline-artifacts.log
        echo "=== Disk Usage ===" >> pipeline-artifacts.log
        du -sh test_output/* >> pipeline-artifacts.log

    - name: Upload pipeline E2E results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: pipeline-e2e-results
        path: |
          pipeline-e2e-results.xml
          pipeline-artifacts.log
          test_output/

  # Data Validation E2E Tests
  data-validation-e2e:
    name: Data Validation E2E
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Setup test environment
      run: |
        mkdir -p test_output/{data,audio,logs}
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run data validation E2E tests
      run: |
        pytest tests/e2e/test_data_validation.py -v \
          --tb=short \
          --junit-xml=data-validation-e2e-results.xml \
          --timeout=600
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Upload data validation results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: data-validation-e2e-results
        path: |
          data-validation-e2e-results.xml
          test_output/

  # Performance E2E Tests
  performance-e2e:
    name: Performance E2E
    runs-on: ubuntu-latest
    if: github.event.inputs.test_scope == 'comprehensive'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil memory_profiler

    - name: Setup performance test environment
      run: |
        mkdir -p test_output/{data,audio,logs}
        mkdir -p performance_reports
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run performance E2E tests
      run: |
        pytest tests/e2e/test_performance.py -v \
          --tb=short \
          --junit-xml=performance-e2e-results.xml \
          --timeout=1200 \
          -m "performance"
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Generate performance report
      if: always()
      run: |
        echo "# Performance Test Report" > performance_reports/report.md
        echo "**Date:** $(date)" >> performance_reports/report.md
        echo "**Environment:** E2E Test" >> performance_reports/report.md
        echo "" >> performance_reports/report.md

        if [ -f performance-e2e-results.xml ]; then
          echo "## Test Results" >> performance_reports/report.md
          echo "Performance tests completed successfully" >> performance_reports/report.md
        else
          echo "## Test Results" >> performance_reports/report.md
          echo "Performance tests failed or did not complete" >> performance_reports/report.md
        fi

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-e2e-results
        path: |
          performance-e2e-results.xml
          performance_reports/
          test_output/

  # Comprehensive E2E Test Suite
  comprehensive-e2e:
    name: Comprehensive E2E Suite
    runs-on: ubuntu-latest
    needs: [cli-e2e, pipeline-e2e, data-validation-e2e]
    if: github.event.inputs.test_scope == 'comprehensive'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Setup comprehensive test environment
      run: |
        mkdir -p test_output/{data,audio,logs}
        mkdir -p comprehensive_reports
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run comprehensive E2E test suite
      run: |
        pytest tests/e2e/ -v \
          --tb=short \
          --junit-xml=comprehensive-e2e-results.xml \
          --timeout=1800 \
          --capture=no \
          -m "not network or (network and comprehensive)"
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Generate comprehensive report
      if: always()
      run: |
        echo "# Comprehensive E2E Test Report" > comprehensive_reports/report.md
        echo "**Date:** $(date)" >> comprehensive_reports/report.md
        echo "**Test Scope:** Comprehensive" >> comprehensive_reports/report.md
        echo "" >> comprehensive_reports/report.md

        echo "## Test Summary" >> comprehensive_reports/report.md
        if [ -f comprehensive-e2e-results.xml ]; then
          echo "- Comprehensive E2E tests completed" >> comprehensive_reports/report.md
        fi

        echo "" >> comprehensive_reports/report.md
        echo "## Artifacts Generated" >> comprehensive_reports/report.md
        find test_output -name "*.json" -o -name "*.txt" -o -name "*.mp3" | head -10 >> comprehensive_reports/report.md

    - name: Upload comprehensive results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: comprehensive-e2e-results
        path: |
          comprehensive-e2e-results.xml
          comprehensive_reports/
          test_output/

  # E2E Test Summary
  e2e-summary:
    name: E2E Test Summary
    runs-on: ubuntu-latest
    needs: [cli-e2e, pipeline-e2e, data-validation-e2e]
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: all-artifacts/

    - name: Generate E2E summary
      run: |
        echo "# E2E Test Summary" > e2e-summary.md
        echo "**Date:** $(date)" >> e2e-summary.md
        echo "**Trigger:** ${{ github.event_name }}" >> e2e-summary.md
        echo "" >> e2e-summary.md

        echo "## Job Results" >> e2e-summary.md
        echo "- CLI E2E: ${{ needs.cli-e2e.result }}" >> e2e-summary.md
        echo "- Pipeline E2E: ${{ needs.pipeline-e2e.result }}" >> e2e-summary.md
        echo "- Data Validation E2E: ${{ needs.data-validation-e2e.result }}" >> e2e-summary.md
        echo "" >> e2e-summary.md

        echo "## Artifacts" >> e2e-summary.md
        find all-artifacts/ -name "*.xml" -o -name "*.log" | wc -l > artifact_count
        echo "- Total artifacts: $(cat artifact_count)" >> e2e-summary.md

        echo "" >> e2e-summary.md
        echo "## Next Steps" >> e2e-summary.md
        if [[ "${{ needs.cli-e2e.result }}" == "failure" || "${{ needs.pipeline-e2e.result }}" == "failure" || "${{ needs.data-validation-e2e.result }}" == "failure" ]]; then
          echo "❌ Some E2E tests failed. Review the failed jobs and artifacts." >> e2e-summary.md
        else
          echo "✅ All E2E tests passed successfully!" >> e2e-summary.md
        fi

    - name: Create summary artifact
      uses: actions/upload-artifact@v3
      with:
        name: e2e-test-summary
        path: e2e-summary.md

    - name: Add summary to job summary
      run: |
        cat e2e-summary.md >> $GITHUB_STEP_SUMMARY