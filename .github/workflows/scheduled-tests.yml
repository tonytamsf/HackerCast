name: Scheduled Tests

on:
  schedule:
    # Run comprehensive tests daily at 3 AM UTC
    - cron: '0 3 * * *'
    # Run quick health check every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of scheduled test'
        required: false
        default: 'health-check'
        type: choice
        options:
        - health-check
        - comprehensive
        - performance-benchmark
        - stress-test

env:
  PYTHON_VERSION: '3.11'

jobs:
  # Health Check (runs every 6 hours)
  health-check:
    name: Health Check
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 */6 * * *' || github.event.inputs.test_type == 'health-check'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Quick health check
      run: |
        # Test basic imports
        python -c "import main; import hn_api; import scraper; import tts_converter; import config"

        # Test CLI help
        python main.py --help

        # Test configuration loading
        python -c "from config import initialize_config; initialize_config()"

    - name: Basic functionality test
      run: |
        pytest tests/test_config.py -v -k "test_default_config" --tb=short

    - name: Report health status
      run: |
        echo "✅ Health check passed at $(date)" >> $GITHUB_STEP_SUMMARY
        echo "- All modules import successfully" >> $GITHUB_STEP_SUMMARY
        echo "- CLI help works" >> $GITHUB_STEP_SUMMARY
        echo "- Configuration loading works" >> $GITHUB_STEP_SUMMARY

  # Comprehensive Daily Tests
  comprehensive-daily:
    name: Comprehensive Daily Tests
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 3 * * *' || github.event.inputs.test_type == 'comprehensive'
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.11', '3.12']

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Setup test environment
      shell: bash
      run: |
        mkdir -p test_output/{data,audio,logs}
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run unit tests
      run: |
        pytest tests/ -v \
          --tb=short \
          --cov=. \
          --cov-report=xml \
          --junit-xml=unit-test-results-ubuntu-py${{ matrix.python-version }}.xml \
          -m "not e2e and not performance and not network" \
          --timeout=300
      env:
        HACKERCAST_ENVIRONMENT: test

    - name: Run integration tests
      run: |
        pytest tests/test_integration.py -v \
          --tb=short \
          --junit-xml=integration-test-results-ubuntu-py${{ matrix.python-version }}.xml \
          --timeout=300
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Upload daily test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: daily-test-results-ubuntu-py${{ matrix.python-version }}
        path: |
          *-test-results-ubuntu-py${{ matrix.python-version }}.xml
          coverage.xml
          test_output/

  # Performance Benchmark Tests
  performance-benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'performance-benchmark'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil memory_profiler pytest-benchmark

    - name: Setup benchmark environment
      run: |
        mkdir -p test_output/{data,audio,logs}
        mkdir -p benchmark_reports
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run performance benchmarks
      run: |
        pytest tests/e2e/test_performance.py -v \
          --tb=short \
          --junit-xml=benchmark-results.xml \
          --timeout=1800 \
          -m "performance" \
          --benchmark-only \
          --benchmark-json=benchmark_results.json
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Generate benchmark report
      if: always()
      run: |
        echo "# Performance Benchmark Report" > benchmark_reports/report.md
        echo "**Date:** $(date)" >> benchmark_reports/report.md
        echo "**Environment:** Scheduled Benchmark" >> benchmark_reports/report.md
        echo "" >> benchmark_reports/report.md

        if [ -f benchmark_results.json ]; then
          echo "## Benchmark Results" >> benchmark_reports/report.md
          echo "Benchmark data saved to benchmark_results.json" >> benchmark_reports/report.md
        else
          echo "## Benchmark Results" >> benchmark_reports/report.md
          echo "No benchmark data available" >> benchmark_reports/report.md
        fi

        echo "" >> benchmark_reports/report.md
        echo "## System Information" >> benchmark_reports/report.md
        echo "- OS: $(uname -a)" >> benchmark_reports/report.md
        echo "- Python: $(python --version)" >> benchmark_reports/report.md
        echo "- Memory: $(free -h | grep Mem)" >> benchmark_reports/report.md

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-benchmark-results
        path: |
          benchmark-results.xml
          benchmark_results.json
          benchmark_reports/
          test_output/

  # Stress Tests
  stress-test:
    name: Stress Test
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'stress-test'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil

    - name: Setup stress test environment
      run: |
        mkdir -p test_output/{data,audio,logs}
        mkdir -p stress_reports
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Run stress tests
      run: |
        # Test with large story limits
        pytest tests/e2e/test_performance.py::TestPerformance::test_large_dataset_performance -v \
          --tb=short \
          --junit-xml=stress-test-results.xml \
          --timeout=3600

        # Test concurrent execution
        pytest tests/e2e/test_performance.py::TestPerformance::test_concurrent_pipeline_performance -v \
          --tb=short \
          --junit-xml=stress-concurrent-results.xml \
          --timeout=3600
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Monitor system resources
      if: always()
      run: |
        echo "# Stress Test System Report" > stress_reports/system.md
        echo "**Date:** $(date)" >> stress_reports/system.md
        echo "" >> stress_reports/system.md
        echo "## System Resources" >> stress_reports/system.md
        echo "### Memory Usage" >> stress_reports/system.md
        echo '```' >> stress_reports/system.md
        free -h >> stress_reports/system.md
        echo '```' >> stress_reports/system.md
        echo "" >> stress_reports/system.md
        echo "### Disk Usage" >> stress_reports/system.md
        echo '```' >> stress_reports/system.md
        df -h >> stress_reports/system.md
        echo '```' >> stress_reports/system.md
        echo "" >> stress_reports/system.md
        echo "### Test Output Size" >> stress_reports/system.md
        echo '```' >> stress_reports/system.md
        du -sh test_output/* >> stress_reports/system.md
        echo '```' >> stress_reports/system.md

    - name: Upload stress test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: stress-test-results
        path: |
          stress-*-results.xml
          stress_reports/
          test_output/

  # Network Integration Tests (with real APIs)
  network-integration:
    name: Network Integration
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 3 * * *' || github.event.inputs.test_type == 'comprehensive'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Setup network test environment
      run: |
        mkdir -p test_output/{data,audio,logs}
        echo "HACKERCAST_ENVIRONMENT=test" > .env.test
        echo "HACKERCAST_OUTPUT_DIR=./test_output" >> .env.test

    - name: Test HackerNews API connectivity
      run: |
        python -c "
        from hn_api import HackerNewsAPI
        api = HackerNewsAPI()
        stories = api.get_top_stories(3)
        print(f'Successfully fetched {len(stories)} stories')
        assert len(stories) > 0, 'No stories fetched'
        "

    - name: Test web scraping capability
      run: |
        pytest tests/e2e/test_cli_commands.py -v \
          --tb=short \
          --junit-xml=network-integration-results.xml \
          --timeout=900 \
          -m "network" \
          -k "not invalid_url"
      env:
        HACKERCAST_ENVIRONMENT: test
        HACKERCAST_OUTPUT_DIR: ./test_output

    - name: Upload network test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: network-integration-results
        path: |
          network-integration-results.xml
          test_output/

  # Test Results Summary
  scheduled-summary:
    name: Scheduled Test Summary
    runs-on: ubuntu-latest
    needs: [health-check, comprehensive-daily, network-integration]
    if: always()

    steps:
    - name: Create scheduled test summary
      run: |
        echo "# Scheduled Test Summary" > summary.md
        echo "**Date:** $(date)" >> summary.md
        echo "**Trigger:** ${{ github.event_name }}" >> summary.md
        echo "" >> summary.md

        echo "## Test Results" >> summary.md
        echo "- Health Check: ${{ needs.health-check.result }}" >> summary.md
        echo "- Comprehensive Daily: ${{ needs.comprehensive-daily.result }}" >> summary.md
        echo "- Network Integration: ${{ needs.network-integration.result }}" >> summary.md
        echo "" >> summary.md

        # Overall status
        if [[ "${{ needs.health-check.result }}" == "success" && "${{ needs.comprehensive-daily.result }}" == "success" ]]; then
          echo "## Overall Status: ✅ PASS" >> summary.md
          echo "All scheduled tests completed successfully" >> summary.md
        else
          echo "## Overall Status: ❌ FAIL" >> summary.md
          echo "Some scheduled tests failed - investigation required" >> summary.md
        fi

        echo "" >> summary.md
        echo "## Next Scheduled Run" >> summary.md
        echo "- Health Check: Every 6 hours" >> summary.md
        echo "- Comprehensive: Daily at 3 AM UTC" >> summary.md

    - name: Upload summary
      uses: actions/upload-artifact@v4
      with:
        name: scheduled-test-summary
        path: summary.md

    - name: Add to GitHub summary
      run: |
        cat summary.md >> $GITHUB_STEP_SUMMARY

    - name: Notify on failure
      if: failure()
      run: |
        echo "🚨 Scheduled tests failed!" >> $GITHUB_STEP_SUMMARY
        echo "This indicates potential issues with the application." >> $GITHUB_STEP_SUMMARY
        echo "Manual investigation is recommended." >> $GITHUB_STEP_SUMMARY