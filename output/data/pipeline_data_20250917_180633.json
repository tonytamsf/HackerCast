{
  "timestamp": "20250917_180633",
  "run_date": "2025-09-17T18:06:33.072788",
  "config": {
    "environment": "development",
    "max_stories": 5,
    "tts_voice": "en-US-Neural2-D"
  },
  "stories": [
    {
      "id": 45283306,
      "title": "Meta Ray-Ban Display",
      "url": "https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/",
      "score": 213,
      "by": "martpie",
      "time": 1758155444,
      "descendants": 279,
      "type": "story",
      "created_at": "2025-09-17T14:30:44"
    },
    {
      "id": 45279384,
      "title": "WASM 3.0 Completed",
      "url": "https://webassembly.org/news/2025-09-17-wasm-3.0/",
      "score": 743,
      "by": "todsacerdoti",
      "time": 1758133013,
      "descendants": 286,
      "type": "story",
      "created_at": "2025-09-17T08:16:53"
    },
    {
      "id": 45274277,
      "title": "Apple Photos app corrupts images",
      "url": "https://tenderlovemaking.com/2025/09/17/apple-photos-app-corrupts-images/",
      "score": 1011,
      "by": "pattyj",
      "time": 1758107264,
      "descendants": 374,
      "type": "story",
      "created_at": "2025-09-17T01:07:44"
    },
    {
      "id": 45281139,
      "title": "A postmortem of three recent issues",
      "url": "https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues",
      "score": 216,
      "by": "moatmoat",
      "time": 1758141667,
      "descendants": 78,
      "type": "story",
      "created_at": "2025-09-17T10:41:07"
    },
    {
      "id": 45282497,
      "title": "One Token to rule them all – Obtaining Global Admin in every Entra ID tenant",
      "url": "https://dirkjanm.io/obtaining-global-admin-in-every-entra-id-tenant-with-actor-tokens/",
      "score": 87,
      "by": "colinprince",
      "time": 1758150201,
      "descendants": 8,
      "type": "story",
      "created_at": "2025-09-17T13:03:21"
    }
  ],
  "scraped_content": [
    {
      "url": "https://webassembly.org/news/2025-09-17-wasm-3.0/",
      "title": "WASM 3.0 Completed",
      "content": "Three years ago, version 2.0 of the Wasm standard was (essentially) finished, which brought a number of new features, such as vector instructions, bulk memory operations, multiple return values, and simple reference types.\n\nIn the meantime, the Wasm W3C Community Group and Working Group have not been lazy. Today, we are happy to announce the release of Wasm 3.0 as the new “live” standard.\n\nThis is a substantially larger update: several big features, some of which have been in the making for six or eight years, finally made it over the finishing line.\n• 64-bit address space. Memories and tables can now be declared to use as their address type instead of just . That expands the available address space of Wasm applications from 4 gigabytes to (theoretically) 16 exabytes, to the extent that physical hardware allows. While the web will necessarily keep enforcing certain limits — on the web, a 64-bit memory is limited to 16 gigabytes — the new flexibility is especially interesting for non-web ecosystems using Wasm, as they can support much, much larger applications and data sets now.\n• Multiple memories. Contrary to popular belief, Wasm applications were always able to use multiple memory objects — and hence multiple address spaces — simultaneously. However, previously that was only possible by declaring and accessing each of them in separate modules. This gap has been closed, a single module can now declare (define or import) multiple memories and directly access them, including directly copying data between them. This finally allows tools like wasm-merge, which perform “static linking” on two or more Wasm modules by merging them into one, to work for all Wasm modules. It also paves the way for new uses of separate address spaces, e.g., for security (separating private data), for buffering, or for instrumentation.\n• Garbage collection. In addition to expanding the capabilities of raw linear memories, Wasm also adds support for a new (and separate) form of storage that is automatically managed by the Wasm runtime via a garbage collector. Staying true to the spirit of Wasm as a low-level language, Wasm GC is low-level as well: a compiler targeting Wasm can declare the memory layout of its runtime data structures in terms of struct and array types, plus unboxed tagged integers, whose allocation and lifetime is then handled by Wasm. But that’s it. Everything else, such as engineering suitable representations for source-language values, including implementation details like method tables, remains the responsibility of compilers targeting Wasm. There are no built-in object systems, nor closures or other higher-level constructs — which would inevitably be heavily biased towards specific languages. Instead, Wasm only provides the basic building blocks for representing such constructs and focuses purely on the memory management aspect.\n• Typed references. The GC extension is built upon a substantial extension to the Wasm type system, which now supports much richer forms of references. Reference types can now describe the exact shape of the referenced heap value, avoiding additional runtime checks that would otherwise be needed to ensure safety. This more expressive typing mechanism, including subtyping and type recursion, is also available for function references, making it possible to perform safe indirect function calls without any runtime type or bounds check, through the new instruction.\n• Tail calls. Tail calls are a variant of function calls that immediately exit the current function, and thereby avoid taking up additional stack space. Tail calls are an important mechanism that is used in various language implementations both in user-visible ways (e.g., in functional languages) and for internal techniques (e.g., to implement stubs). Wasm tail calls are fully general and work for callees both selected statically (by function index) and dynamically (by reference or table).\n• Exception handling. Exceptions provide a way to locally abort execution, and are a common feature in modern programming languages. Previously, there was no efficient way to compile exception handling to Wasm, and existing compilers typically resorted to convoluted ways of implementing them by escaping to the host language, e.g., JavaScript. This was neither portable nor efficient. Wasm 3.0 hence provides native exception handling within Wasm. Exceptions are defined by declaring exception tags with associated payload data. As one would expect, an exception can be thrown, and selectively be caught by a surrounding handler, based on its tag. Exception handlers are a new form of block instruction that includes a dispatch list of tag/label pairs or catch-all labels to define where to jump when an exception occurs.\n• Relaxed vector instructions. Wasm 2.0 added a large set of vector (SIMD) instructions, but due to differences in hardware, some of these instructions have to do extra work on some platforms to achieve the specified semantics. In order to squeeze out maximum performance, Wasm 3.0 introduces “relaxed” variants of these instructions that are allowed to have implementation-dependent behavior in certain edge cases. This behavior must be selected from a pre-specified set of legal choices.\n• Deterministic profile. To make up for the added semantic fuzziness of relaxed vector instructions, and in order to support settings that demand or need deterministic execution semantics (such as blockchains, or replayable systems), the Wasm standard now specifies a deterministic default behavior for every instruction with otherwise non-deterministic results — currently, this includes floating-point operators and their generated NaN values and the aforementioned relaxed vector instructions. Between platforms choosing to implement this deterministic execution profile, Wasm thereby is fully deterministic, reproducible, and portable.\n• Custom annotation syntax. Finally, the Wasm text format has been enriched with generic syntax for placing annotations in Wasm source code. Analogous to custom sections in the binary format, these annotations are not assigned any meaning by the Wasm standard itself, and can be chosen to be ignored by implementations. However, they provide a way to represent the information stored in custom sections in human-readable and writable form, and concrete annotations can be specified by downstream standards.\n\nIn addition to these core features, embeddings of Wasm into JavaScript benefit from a new extension to the JS API:\n• JS string builtins. JavaScript string values can already be passed to Wasm as externrefs. Functions from this new primitive library can be imported into a Wasm module to directly access and manipulate such external string values inside Wasm.\n\nWith these new features, Wasm has much better support for compiling high-level programming languages. Enabled by this, we have seen various new languages popping up to target Wasm, such as Java, OCaml, Scala, Kotlin, Scheme, or Dart, all of which use the new GC feature.\n\nOn top of all these goodies, Wasm 3.0 also is the first version of the standard that has been produced with the new SpecTec tool chain. We believe that this makes for an even more reliable specification.\n\nWasm 3.0 is already shipping in most major web browsers, and support in stand-alone engines like Wasmtime is on track to completion as well. The Wasm feature status page tracks support across engines.",
      "author": null,
      "published_date": null,
      "meta_description": "",
      "word_count": 1155,
      "scraping_method": "goose3"
    },
    {
      "url": "https://tenderlovemaking.com/2025/09/17/apple-photos-app-corrupts-images/",
      "title": "Apple Photos app corrupts images",
      "content": "Apple Photos App Corrupts Images\nSep 17, 2025 @\n8:59 am\nThe Apple Photos app sometimes corrupts images when importing from my camera.\nI just wanted to make a blog post about it in case anyone else runs into the problem.\nI’ve seen other references to this online, but most of the people gave up trying to fix it, and none of them went as far as I did to debug the issue.\nI’ll try to describe the problem, and the things I’ve tried to do to fix it.\nBut also note that I’ve (sort of) given up on the Photos app too.\nSince I can’t trust it to import photos from my camera, I switched to a different workflow.\nHere is a screenshot of a corrupted image in the Photos app:\nHow I used to import images\nI’ve got an OM System OM-1 camera.\nI used to shoot in RAW + jpg, then when I would import to Photos app, I would check the “delete photos after import” checkbox in order to empty the SD card.\nTurns out “delete after import” was a huge mistake.\nGetting corrupted images\nI’m pretty sure I’d been getting corrupted images for a while, but it would only be 1 or 2 images out of thousands, so I thought nothing of it (it was probably my fault anyway, right?)\nBut the problem really got me upset when last year I went to a family member’s wedding and took tons of photos.\nApple Photos combines RAW + jpg photos so you don’t have a bunch of duplicates, and when you view the images in the photos app, it just shows you the jpg version by default.\nAfter I imported all of the wedding photos I noticed some of them were corrupted.\nUpon closer inspection, I found that it sometimes had corrupted the jpg, sometimes corrupted the RAW file, and sometimes both.\nSince I had been checking the “delete after import” box, I didn’t know if the images on the SD card were corrupted before importing or not.\nAfter all, the files had been deleted so there was no way to check.\nI estimate I completely lost about 30% of the images I took that day.\nLosing so many photos really rattled me, but I wanted to figure out the problem so I didn’t lose images in the future.\nNarrowing down the problem\nI was worried this was somehow a hardware problem.\nCopying files seems so basic, I didn’t think there was any way a massively deployed app like Photos could fuck it up (especially since its main job is managing photo files).\nSo, to narrow down the issue I changed out all of the hardware.\nHere are all the things I did:\nSwitched USB-C cables\nBought a new SD card direct from the manufacturer (to eliminate the possibility of buying a bootleg SD card)\nSwitched to only shooting in RAW (if importing messes up 30% of my images, but I cut the number of images I import by half, then that should be fewer corrupted images right? lol)\nBought a new laptop\nBought a new camera: the OM System OM-1 MKii\nI did each of these steps over time, as to only change one variable at a time, and still the image corruption persisted.\nI didn’t really want to buy a new camera, the MKii is not really a big improvement over the OM-1, but we had a family trip coming up and the idea that pressing the shutter button on the camera might not actually record the image didn’t sit well with me.\nFinally a smoking gun\nSince I had replaced literally all of the hardware involved, I knew it must be a software problem.\nI stopped checking the “delete after import” button, and started reviewing all of the photos after import.\nAfter verifying none of them were corrupt, then I would format the SD card.\nI did this for months without finding any corrupt files.\nAt this point I figured it was somehow a race condition or something when copying the photo files and deleting them at the same time.\nHowever, after I got home from RailsConf and imported my photos, I found one corrupt image (the one above).\nI was able to verify that the image was not corrupt on the SD card, so the camera was working fine (meaning I probably didn’t need to buy a new camera body at all).\nI tried deleting the corrupt file and re-importing the original to see if it was something about that particular image, but it re-imported just fine.\nIn other words, it seems like the Photos app will corrupt files randomly.\nI don’t know if this is a problem that is specific to OM System cameras, and I’m not particularly interested in investing in a new camera system just to find out.\nIf I compare the corrupted image with the non-corrupted image, the file sizes are exactly the same, but the bytes are different:\nChecksums:\naaron@tc ~/Downloads> md5sum P7110136-from-camera.ORF Exports/P7110136.ORF\n17ce895fd809a43bad1fe8832c811848\nP7110136-from-camera.ORF\n828a33005f6b71aea16d9c2f2991a997\nExports/P7110136.ORF\nFile sizes:\naaron@tc ~/Downloads> ls -al P7110136-from-camera.ORF Exports/P7110136.ORF\n-rw-------@ 1 aaron\nstaff\n18673943 Jul 12 04:38 Exports/P7110136.ORF\n-rwx------\n1 aaron\nstaff\n18673943 Jul 17 09:29 P7110136-from-camera.ORF*\nThe P7110136-from-camera.ORF is the non-corrupted file, and Exports/P7110136.ORF is the corrupted file from Photos app.\nHere’s a screenshot of the preview of the non-corrupted photo:\nHere is the binary diff between the files.\nI ran both files through xxd then diffed them.\nMy new workflow\nI’m not going to put any more effort into debugging this problem, but I wanted to blog about it in case anyone else is seeing the issue.\nI take a lot of photos, and to be frank, most of them are not very good.\nI don’t want to look through a bunch of bad photos every time I look at my library, so culling photos is important.\nCulling photos in the Photos app is way too cumbersome, so I’ve switched to using Darktable.\nMy current process is:\nImport images to Darktable\nDelete the ones I don’t like\nProcess ones I do like\nExport both the jpg and the original raw file\nImport those to the Photos app so they’re easy to view and share\nPeriodically format my SD card\nI’ve not seen any file corruption when importing to Darktable, so I am convinced this is a problem with the Photos app.\nBut now, since all of my images land in Darktable before making their way to the Photos app, I don’t really care anymore.\nThe bad news is that I’ve spent a lot of time and money trying to debug this.\nI guess the good news is that now I have redundant hardware!",
      "author": null,
      "published_date": null,
      "meta_description": null,
      "word_count": 1115,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues",
      "title": "A postmortem of three recent issues",
      "content": "Between August and early September, three infrastructure bugs intermittently degraded Claude's response quality. We've now resolved these issues and want to explain what happened.\n\nIn early August, a number of users began reporting degraded responses from Claude. These initial reports were difficult to distinguish from normal variation in user feedback. By late August, the increasing frequency and persistence of these reports prompted us to open an investigation that led us to uncover three separate infrastructure bugs.\n\nTo state it plainly: We never reduce model quality due to demand, time of day, or server load. The problems our users reported were due to infrastructure bugs alone.\n\nWe recognize users expect consistent quality from Claude, and we maintain an extremely high bar for ensuring infrastructure changes don't affect model outputs. In these recent incidents, we didn't meet that bar. The following postmortem explains what went wrong, why detection and resolution took longer than we would have wanted, and what we're changing to prevent similar future incidents.\n\nWe don't typically share this level of technical detail about our infrastructure, but the scope and complexity of these issues justified a more comprehensive explanation.\n\nHow we serve Claude at scale\n\nWe serve Claude to millions of users via our first-party API, Amazon Bedrock, and Google Cloud's Vertex AI. We deploy Claude across multiple hardware platforms, namely AWS Trainium, NVIDIA GPUs, and Google TPUs. This approach provides the capacity and geographic distribution necessary to serve users worldwide.\n\nEach hardware platform has different characteristics and requires specific optimizations. Despite these variations, we have strict equivalence standards for model implementations. Our aim is that users should get the same quality responses regardless of which platform serves their request. This complexity means that any infrastructure change requires careful validation across all platforms and configurations.\n\nThe overlapping nature of these bugs made diagnosis particularly challenging. The first bug was introduced on August 5, affecting approximately 0.8% of requests made to Sonnet 4. Two more bugs arose from deployments on August 25 and 26.\n\nAlthough initial impacts were limited, a load balancing change on August 29 started to increase affected traffic. This caused many more users to experience issues while others continued to see normal performance, creating confusing and contradictory reports.\n\nBelow we describe the three bugs that caused the degradation, when they occurred, and how we resolved them:\n\nOn August 5, some Sonnet 4 requests were misrouted to servers configured for the upcoming 1M token context window. This bug initially affected 0.8% of requests. On August 29, a routine load balancing change unintentionally increased the number of short-context requests routed to the 1M context servers. At the worst impacted hour on August 31, 16% of Sonnet 4 requests were affected.\n\nApproximately 30% of Claude Code users who made requests during this period had at least one message routed to the wrong server type, resulting in degraded responses. On Amazon Bedrock, misrouted traffic peaked at 0.18% of all Sonnet 4 requests from August 12. Incorrect routing affected less than 0.0004% of requests on Google Cloud's Vertex AI between August 27 and September 16.\n\nHowever, some users were affected more severely, as our routing is \"sticky\". This meant that once a request was served by the incorrect server, subsequent follow-ups were likely to be served by the same incorrect server.\n\nResolution: We fixed the routing logic to ensure short- and long-context requests were directed to the correct server pools. We deployed the fix on September 4. A rollout to our first-party platforms and Google Cloud’s Vertex was completed by September 16. The fix is in the process of being rolled out on Bedrock.\n\nOn August 25, we deployed a misconfiguration to the Claude API TPU servers that caused an error during token generation. An issue caused by a runtime performance optimization occasionally assigned a high probability to tokens that should rarely be produced given the context, for example producing Thai or Chinese characters in response to English prompts, or producing obvious syntax errors in code. A small subset of users that asked a question in English might have seen \"สวัสดี\" in the middle of the response, for example.\n\nThis corruption affected requests made to Opus 4.1 and Opus 4 on August 25-28, and requests to Sonnet 4 August 25–September 2. Third-party platforms were not affected by this issue.\n\nResolution: We identified the issue and rolled back the change on September 2. We've added detection tests for unexpected character outputs to our deployment process.\n\nOn August 25, we deployed code to improve how Claude selects tokens during text generation. This change inadvertently triggered a latent bug in the XLA:TPU[1] compiler, which has been confirmed to affect requests to Claude Haiku 3.5.\n\nWe also believe this could have impacted a subset of Sonnet 4 and Opus 3 on the Claude API. Third-party platforms were not affected by this issue.\n\nResolution: We first observed the bug affecting Haiku 3.5 and rolled it back on September 4. We later noticed user reports of problems with Opus 3 that were compatible with this bug, and rolled it back on September 12. After extensive investigation we were unable to reproduce this bug on Sonnet 4 but decided to also roll it back out of an abundance of caution.\n\nSimultaneously, we have (a) been working with the XLA:TPU team on a fix for the compiler bug and (b) rolled out a fix to use exact top-k with enhanced precision. For details, see the deep dive below.\n\nA closer look at the XLA compiler bug\n\nTo illustrate the complexity of these issues, here's how the XLA compiler bug manifested and why it proved particularly challenging to diagnose.\n\nWhen Claude generates text, it calculates probabilities for each possible next word, then randomly chooses a sample from this probability distribution. We use \"top-p sampling\" to avoid nonsensical outputs—only considering words whose cumulative probability reaches a threshold (typically 0.99 or 0.999). On TPUs, our models run across multiple chips, with probability calculations happening in different locations. To sort these probabilities, we need to coordinate data between chips, which is complex.[2]\n\nIn December 2024, we discovered our TPU implementation would occasionally drop the most probable token when temperature was zero. We deployed a workaround to fix this case.\n\nThe root cause involved mixed precision arithmetic. Our models compute next-token probabilities in bf16 (16-bit floating point). However, the vector processor is fp32-native, so the TPU compiler (XLA) can optimize runtime by converting some operations to fp32 (32-bit). This optimization pass is guarded by the flag which defaults to true.\n\nThis caused a mismatch: operations that should have agreed on the highest probability token were running at different precision levels. The precision mismatch meant they didn't agree on which token had the highest probability. This caused the highest probability token to sometimes disappear from consideration entirely.\n\nOn August 26, we deployed a rewrite of our sampling code to fix the precision issues and improve how we handled probabilities at the limit that reach the top-p threshold. But in fixing these problems, we exposed a trickier one.\n\nOur fix removed the December workaround because we believed we'd solved the root cause. This led to a deeper bug in the approximate top-k operation—a performance optimization that quickly finds the highest probability tokens.[3] This approximation sometimes returned completely wrong results, but only for certain batch sizes and model configurations. The December workaround had been inadvertently masking this problem.\n\nThe bug's behavior was frustratingly inconsistent. It changed depending on unrelated factors such as what operations ran before or after it, and whether debugging tools were enabled. The same prompt might work perfectly on one request and fail on the next.\n\nWhile investigating, we also discovered that the exact top-k operation no longer had the prohibitive performance penalty it once did. We switched from approximate to exact top-k and standardized some additional operations on fp32 precision.[4] Model quality is non-negotiable, so we accepted the minor efficiency impact.\n\nOur validation process ordinarily relies on benchmarks alongside safety evaluations and performance metrics. Engineering teams perform spot checks and deploy to small \"canary\" groups first.\n\nThese issues exposed critical gaps that we should have identified earlier. The evaluations we ran simply didn't capture the degradation users were reporting, in part because Claude often recovers well from isolated mistakes. Our own privacy practices also created challenges in investigating reports. Our internal privacy and security controls limit how and when engineers can access user interactions with Claude, in particular when those interactions are not reported to us as feedback. This protects user privacy but prevents engineers from examining the problematic interactions needed to identify or reproduce bugs.\n\nEach bug produced different symptoms on different platforms at different rates. This created a confusing mix of reports that didn't point to any single cause. It looked like random, inconsistent degradation.\n\nMore fundamentally, we relied too heavily on noisy evaluations. Although we were aware of an increase in reports online, we lacked a clear way to connect these to each of our recent changes. When negative reports spiked on August 29, we didn't immediately make the connection to an otherwise standard load balancing change.\n\nAs we continue to improve our infrastructure, we're also improving the way we evaluate and prevent bugs like those discussed above across all platforms where we serve Claude. Here's what we're changing:\n• More sensitive evaluations: To help discover the root cause of any given issue, we’ve developed evaluations that can more reliably differentiate between working and broken implementations. We’ll keep improving these evaluations to keep a closer eye on model quality.\n• Quality evaluations in more places: Although we run regular evaluations on our systems, we will run them continuously on true production systems to catch issues such as the context window load balancing error.\n• Faster debugging tooling: We'll develop infrastructure and tooling to better debug community-sourced feedback without sacrificing user privacy. Additionally, some bespoke tools developed here will be used to reduce the remediation time in future similar incidents, if those should occur.\n\nEvals and monitoring are important. But these incidents have shown that we also need continuous signal from users when responses from Claude aren't up to the usual standard. Reports of specific changes observed, examples of unexpected behavior encountered, and patterns across different use cases all helped us isolate the issues.\n\nIt remains particularly helpful for users to continue to send us their feedback directly. You can use the command in Claude Code or you can use the \"thumbs down\" button in the Claude apps to do so. Developers and researchers often create new and interesting ways to evaluate model quality that complement our internal testing. If you'd like to share yours, reach out to feedback@anthropic.com.\n\nWe remain grateful to our community for these contributions.",
      "author": null,
      "published_date": null,
      "meta_description": "This is a technical report on three bugs that intermittently degraded responses from Claude. Below we explain what happened, why it took time to fix, and what we're changing.",
      "word_count": 1785,
      "scraping_method": "goose3"
    },
    {
      "url": "https://dirkjanm.io/obtaining-global-admin-in-every-entra-id-tenant-with-actor-tokens/",
      "title": "One Token to rule them all – Obtaining Global Admin in every Entra ID tenant",
      "content": "While preparing for my Black Hat and DEF CON talks in July of this year, I found the most impactful Entra ID vulnerability that I will probably ever find. This vulnerability could have allowed me to compromise every Entra ID tenant in the world (except probably those in national cloud deployments1). If you are an Entra ID admin reading this, yes that means complete access to your tenant. The vulnerability consisted of two components: undocumented impersonation tokens, called “Actor tokens”, that Microsoft uses in their backend for service-to-service (S2S) communication. Additionally, there was a critical flaw in the (legacy) Azure AD Graph API that failed to properly validate the originating tenant, allowing these tokens to be used for cross-tenant access.\nEffectively this means that with a token I requested in my lab tenant I could authenticate as any user, including Global Admins, in any other tenant. Because of the nature of these Actor tokens, they are not subject to security policies like Conditional Access, which means there was no setting that could have mitigated this for specific hardened tenants. Since the Azure AD Graph API is an older API for managing the core Azure AD / Entra ID service, access to this API could have been used to make any modification in the tenant that Global Admins can do, including taking over or creating new identities and granting them any permission in the tenant. With these compromised identities the access could also be extended to Microsoft 365 and Azure.\nI reported this vulnerability the same day to the Microsoft Security Response Center (MSRC). Microsoft fixed this vulnerability on their side within days of the report being submitted and has rolled out further mitigations that block applications from requesting these Actor tokens for the Azure AD Graph API. Microsoft also issued CVE-2025-55241 for this vulnerability.\nImpact\nThese tokens allowed full access to the Azure AD Graph API in any tenant. Requesting Actor tokens does not generate logs. Even if it did they would be generated in my tenant instead of in the victim tenant, which means there is no record of the existence of these tokens.\nFurthermore, the Azure AD Graph API does not have API level logging. Its successor, the Microsoft Graph, does have this logging, but for the Azure AD Graph this telemetry source is still in a very limited preview and I’m not aware of any tenant that currently has this available. Since there is no API level logging, it means the following Entra ID data could be accessed without any traces:\nUser information including all their personal details stored in Entra ID.\nGroup and role information.\nTenant settings and (Conditional Access) policies.\nApplications, Service Principals, and any application permission assignment.\nDevice information and BitLocker keys synced to Entra ID.\nThis information could be accessed by impersonating a regular user in the victim tenant. If you want to know the full impact, my tool roadrecon uses the same API, if you run it then everything you find in the GUI of the tool could have been accessed and modified by an attacker abusing this flaw.\nIf a Global Admin was impersonated, it would also be possible to modify any of the above objects and settings. This would result in full tenant compromise with access to any service that uses Entra ID for authentication, such as SharePoint Online and Exchange Online. It would also provide full access to any resource hosted in Azure, since these resources are controlled from the tenant level and Global Admins can grant themselves rights on Azure subscriptions. Modifying objects in the tenant does (usually) result in audit logs being generated. That means that while theoretically all data in Microsoft 365 could have been compromised, doing anything other than reading the directory information would leave audit logs that could alert defenders, though without knowledge of the specific artifacts that modifications with these Actor tokens generate, it would appear as if a legitimate Global Admin performed the actions.\nBased on Microsoft’s internal telemetry, they did not detect any abuse of this vulnerability. If you want to search for possible abuse artifacts in your own environment, a KQL detection is included at the end of this post.\nTechnical details\nActor tokens\nActor tokens are tokens that are issued by the “Access Control Service”. I don’t know the exact origins of this service, but it appears to be a legacy service that is used for authentication with SharePoint applications and also seems to be used by Microsoft internally. I came across this service while investigating hybrid Exchange setups. These hybrid setups used to provision a certificate credential on the Exchange Online Service Principal (SP) in the tenant, with which it can perform authentication. These hybrid attacks were the topic of some talks I did this summer, the slides are on the talks page. In this case the hybrid part is not relevant, as in my lab I could also have added a credential on the Exchange Online SP without the complete hybrid setup. Exchange is not the only app which can do this, but since I found this in Exchange we will keep talking about these tokens in the context of Exchange.\nExchange will request Actor tokens when it wants to communicate with other services on behalf of a user. The Actor token allows it to “act” as another user in the tenant when talking to Exchange Online, SharePoint and as it turns out the Azure AD Graph. The Actor token (a JSON Web Token / JWT) looks as follows when decoded:\n{\n\"alg\": \"RS256\",\n\"kid\": \"_jNwjeSnvTTK8XEdr5QUPkBRLLo\",\n\"typ\": \"JWT\",\n\"x5t\": \"_jNwjeSnvTTK8XEdr5QUPkBRLLo\"\n}\n{\n\"aud\": \"00000002-0000-0000-c000-000000000000/graph.windows.net@6287f28f-4f7f-4322-9651-a8697d8fe1bc\",\n\"exp\": 1752593816,\n\"iat\": 1752507116,\n\"identityprovider\": \"00000001-0000-0000-c000-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc\",\n\"iss\": \"00000001-0000-0000-c000-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc\",\n\"nameid\": \"00000002-0000-0ff1-ce00-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc\",\n\"nbf\": 1752507116,\n\"oid\": \"a761cbb2-fbb6-4c80-aa50-504962316eb2\",\n\"rh\": \"1.AXQAj_KHYn9PIkOWUahpfY_hvAIAAAAAAAAAwAAAAAAAAACtAQB0AA.\",\n\"sub\": \"a761cbb2-fbb6-4c80-aa50-504962316eb2\",\n\"trustedfordelegation\": \"true\",\n\"xms_spcu\": \"true\"\n}.[signature from Entra ID]\nThere are a few fields here that differ from regular Entra ID access tokens:\nThe aud field contains the GUID of the Azure AD Graph API, as well as the URL graph.windows.net and the tenant it was issued to 6287f28f-4f7f-4322-9651-a8697d8fe1bc.\nThe expiry is exactly 24 hours after the token was issued.\nThe iss contains the GUID of the Entra ID token service itself, called “Azure ESTS Service”, and again the tenant GUID where it was issued.\nThe token contains the claim trustedfordelegation, which is True in this case, meaning we can use this token to impersonate other identities. Many Microsoft apps could request such tokens. Non-Microsoft apps requesting an Actor token would receive a token with this field set to False instead.\nWhen using this Actor token, Exchange would embed this in an unsigned JWT that is then sent to the resource provider, in this case the Azure AD graph. In the rest of the blog I call these impersonation tokens since they are used to impersonate users.\n{\n\"alg\": \"none\",\n\"typ\": \"JWT\"\n}\n{\n\"actortoken\": \"eyJ0eXAiOiJKV1Qi<snip>TxeLkNB8v2rWWMLGpaAaFJlhA\",\n\"aud\": \"00000002-0000-0000-c000-000000000000/graph.windows.net@6287f28f-4f7f-4322-9651-a8697d8fe1bc\",\n\"exp\": 1756926566,\n\"iat\": 1756926266,\n\"iss\": \"00000002-0000-0ff1-ce00-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc\",\n\"nameid\": \"10032001E2CBE43B\",\n\"nbf\": 1756926266,\n\"nii\": \"urn:federation:MicrosoftOnline\",\n\"sip\": \"doesnt@matter.com\",\n\"smtp\": \"doesnt@matter.com\",\n\"upn\": \"doesnt@matter.com\"\n}.[no signature]\nThe sip, smtp, upn fields are used when accessing resources in Exchange online or SharePoint, but are ignored when talking to the Azure AD Graph, which only cares about the nameid. This nameid originates from an attribute of the user that is called the netId on the Azure AD Graph. You will also see it reflected in tokens issued to users, in the puid claim, which stands for Passport UID. I believe these identifiers are an artifact from the original codebase which Microsoft used for its Microsoft Accounts (consumer accounts or MSA). They are still used in Entra ID, for example to map guest users to the original identity in their home tenant.\nAs I mentioned before, these impersonation tokens are not signed. That means that once Exchange has an Actor token, it can use the one Actor token to impersonate anyone against the target service it was requested for, for 24 hours. In my personal opinion, this whole Actor token design is something that never should have existed. It lacks almost every security control that you would want:\nThere are no logs when Actor tokens are issued.\nSince these services can craft the unsigned impersonation tokens without talking to Entra ID, there are also no logs when they are created or used.\nThey cannot be revoked within their 24 hours validity.\nThey completely bypass any restrictions configured in Conditional Access.\nWe have to rely on logging from the resource provider to even know these tokens were used in the tenant.\nMicrosoft uses these tokens to talk to other services in their backend, something that Microsoft calls service-to-service (S2S) communication. If one of these tokens leaks, it can be used to access all the data in an entire tenant without any useful telemetry or mitigation. In July of this year, Microsoft did publish a blog about removing these insecure legacy practices from their environment, but they do not provide any transparency about how many services still use these tokens.\nThe fatal flaw leading to cross-tenant compromise\nAs I was refining my slide deck and polished up my proof-of-concept code for requesting and generating these tokens, I tested more variants of using these tokens, changing various fields to see if the tokens still worked with the modified information. As one of the tests I changed the tenant ID of the impersonation token to a different tenant in which none of my test accounts existed. The Actor tokens tenant ID was my iminyour.cloud tenant, with tenant ID 6287f28f-4f7f-4322-9651-a8697d8fe1bc and the unsigned JWT generated had the tenant ID b9fb93c1-c0c8-4580-99f3-d1b540cada32.\nI sent this token to graph.windows.net using my CLI tool roadtx, expecting a generic access denied since I had a tenant ID mismatch. However, I was instead greeted by a curious error message:\nNote that these are the actual screenshots I made during my research, which is why the formatting may not work as well in this blog\nThe error message suggested that while my token was valid, the identity could not be found in the tenant. Somehow the API seemed to accept my token even with the mismatching tenant. I quickly looked up the netId of a user that did exist in the target tenant, crafted a token and the Azure AD Graph happily returned the data I requested. I tested this in a few more test tenants I had access to, to make sure I was not crazy, but I could indeed access data in other tenants, as long as I knew their tenant ID (which is public information) and the netId of a user in that tenant.\nTo demonstrate the vulnerability, here I am using a Guest user in the target tenant to query the netId of a Global Admin. Then I impersonate the Global Admin using the same Actor token, and can perform any action in the tenant as that Global Admin over the Azure AD Graph.\nFirst I craft an impersonation token for a Guest user in my victim tenant:\nI use this token to query the netId of a Global Admin:\nThen I create an impersonation token for this Global Admin (the UPN is kept the same since it is not validated by the API):\nAnd finally this token is used to access the tenant as the Global Admin, listing the users, something the guest user was not able to do:\nI can even run roadrecon with this impersonation token, which queries all Azure AD Graph API endpoints to enumerate the available information in the tenant.\nNone of these actions would generate any logs in the victim tenant.\nPractical abuse\nWith this vulnerability it would be possible to compromise any Entra ID tenant. Starting with an Actor token from an attacker controlled tenant, the following steps would lead to full control over the victim tenant:\nFind the tenant ID for the victim tenant, this can be done using public APIs based on the domain name.\nFind a valid netId of a regular user in the tenant. Methods for this will be discussed below.\nCraft an impersonation token with the Actor token from the attacker tenant, using the tenant ID and netId of the user in the victim tenant.\nList all Global Admins in the tenant and their netId.\nCraft an impersonation token for the Global Admin account.\nPerform any read or write action over the Azure AD Graph API.\nIf an attacker makes any modifications in the tenant in step 6, that would be the only event in this chain that generates any telemetry in the victim tenant. An attacker could for example create new user accounts, grant these Global Admin privileges and then sign in interactively to any Entra ID, Microsoft 365 or third party application that integrates with the victim tenant. Alternatively they could add credentials on existing applications, grant these apps API permissions and use that to exfiltrate emails or files from Microsoft 365, a technique that is popular among threat actors. An attacker could also add credentials to Microsoft Service Principals in the victim tenant, several of which can request Actor tokens that allow impersonation against SharePoint or Exchange. For my DEF CON and Black Hat talks I made a demo video about using these Actor tokens to obtain Global Admin access. The video uses Actor tokens within a tenant, but the same technique could have been applied to any other tenant by abusing this vulnerability.\nFinding netIds\nSince tenant IDs can be resolved when the domain name of a tenant is known, the only identifier that is not immediately available to the attacker is a valid netId for a user in that specific tenant. As I mentioned above, these IDs are added to Entra ID access tokens as the puid claim. Any token found online, in screenshots, examples or logs, even those that are long expired or with an obfuscated signature, would provide an attacker with enough information to breach the tenant. Threat actors that still have old tokens for any tenant from previous breaches can immediately access those tenants again as long as the victim account still exists.\nThe above is probably not a very common occurrence. What is a more realistic attack is simply brute-forcing the netId. Unlike object IDs, which are randomly generated, netIds are actually incremental. Looking at the differences in netIds between my tenant and those of some tenants I analyzed, I found the difference between a newly created user in my tenant and their newest user to be in the range of 100.000 to 100 million. Simply brute forcing the netId could be accomplished in minutes to hours for any target tenant, and the more user exist in a tenant the easier it is to find a match. Since this does not generate any logs it isn’t a noisy attack either. Because of the possibility to brute force these netIds I would say this vulnerability could have been used to take over any tenant without any prerequisites. There is however a third technique which is even more effective (and more fun from a technical level).\nCompromising tenants by hopping over B2B trusts\nI previously mentioned that a users netId is used to establish links between a user account in multiple tenants. This is something that I researched a few years ago when I gave a talk at Black Hat USA 22 about external identities. The below screenshot is taken from one of my slides, which illustrates this:\nThe way this works is as follows. Suppose we have tenant A and tenant B. A user in tenant B is invited into tenant A. In the new guest account that is created in tenant A, their netId is stored on the alternativeSecurityIds attribute. That means that an attacker wanting to abuse this bug can simply read that attribute in tenant A, put it in an impersonation token for tenant B and then impersonate the victim in their home tenant. It should be noted that this works against the direction of invite. Any user in any tenant where you accept an invite will be able to read your netId, and with this bug could have impersonated you in your home tenant. In your home tenant you have a full user account, which can enumerate other users. This is not a bug or risk with B2B trusts, but is simply an unintended consequence of the B2B design mechanism. A guest account in someone else’s tenant would also be sufficient with the default Entra ID guest settings because the default settings allow users to query the netId of a user as long as the UPN is known.\nTo abuse this, a threat actor could perform the following steps, given that they have access to at least one tenant with a guest user:\nQuery the guest users and their alternativeSecurityIds attribute which gives the netId.\nQuery the tenant ID of the guest users home tenant based on the domain name in their UPN.\nCreate an impersonation token, impersonating the victim in their home tenant.\nOptionally list Global Admins and impersonate those to compromise the entire tenant.\nRepeat step 1 for each tenant that was compromised.\nThe steps above can be done in 2 API calls per tenant, which do not generate any logs. Most tenants will have guest users from multiple distinct other tenants. This means the number of tenants you compromise with this scales exponentially and the information needed to compromise the majority of all tenants worldwide could have been gathered within minutes using a single Actor token. After at least 1 user is known per victim tenant, the attacker can selectively perform post-compromise actions in these tenants by impersonating Global Admins.\nLooking at the list of guest users in the tenants of some of my clients, this technique would be extremely powerful. I also observed that one of the first tenants you will likely compromise is Microsoft’s own tenant, since Microsoft consultants often get invited to customer tenants. Many MSPs and Microsoft Partners will have a guest account in the Microsoft tenant, so from the Microsoft tenant a compromise of most major service provider tenants is one step away.\nNeedless to say, as much as I would have liked to test this technique in practice to see how fast this would spread out, I only tested the individual steps in my own tenants and did not access any data I’m not authorized to.\nDetection\nWhile querying data over the Azure AD Graph does not leave any logs, modifying data does (usually) generate audit logs. If modifications are done with Actor tokens, these logs look a bit curious.\nSince Actor tokens involve both the app and the user being impersonated, it seems Entra ID gets confused about who actually made the change, and it will log the UPN of the impersonated Global Admin, but the display name of Exchange. Luckily for defenders this creates a nice giveaway when Actor tokens are used in the tenant. After some testing and filtering with some fellow researchers that work on the blue side (thanks to Fabian Bader and Olaf Hartong) we came up with the following detection query:\nAuditLogs\n| where not(OperationName has \"group\")\n| where not(OperationName == \"Set directory feature on tenant\")\n| where InitiatedBy has \"user\"\n| where InitiatedBy.user.displayName has_any ( \"Office 365 Exchange Online\", \"Skype for Business Online\", \"Dataverse\", \"Office 365 SharePoint Online\", \"Microsoft Dynamics ERP\")\nThe exclusion for group operations is there because some of these products do actually use Actor tokens to perform operations on your behalf. For example creating specific groups via the Exchange Online PowerShell module will make Exchange use an Actor token on your behalf and create the group in Entra ID.\nConclusion\nThis blog discussed a critical token validation failure in the Azure AD Graph API. While the vulnerability itself was a bad oversight in the token handling, the whole concept of Actor tokens is a protocol that was designed to behave with all the properties mentioned in the paragraphs above. If it weren’t for the complete lack of security measures in these tokens, I don’t think such a big impact with such limited telemetry would have been possible.\nThanks to the people at MSRC who immediately picked up the vulnerability report, searched for potential variants in other resources, and to the engineers who followed up with fixes for the Azure AD Graph and blocked Actor tokens for the Azure AD Graph API requested with credentials stored on Service Principals, essentially restricting the usage of these Actor tokens to only Microsoft internal services.\nDisclosure timeline\nJuly 14, 2025 - reported issue to MSRC.\nJuly 14, 2025 - MSRC case opened.\nJuly 15, 2025 - reported further details on the impact.\nJuly 15, 2025 - MSRC requested to halt further testing of this vulnerability.\nJuly 17, 2025 - Microsoft pushed a fix for the issue globally into production.\nJuly 23, 2025 - Issue confirmed as resolved by MSRC.\nAugust 6, 2025 - Further mitigations pushed out preventing Actor tokens being issued for the Azure AD Graph with SP credentials.\nSeptember 4, 2025 - CVE-2025-55241 issued.\nSeptember 17, 2025 - Release of this blogpost.\nI do not have access to any tenants in a national cloud deployment, so I was not able to test whether the vulnerability existed there. Since national cloud deployments use their own token signing keys, it is unlikely that it would have been possible to execute this attack from a tenant in the public cloud to one of these national clouds. I do consider it likely that this attack would have worked across tenants in the same national cloud deployments, but that is speculation. ↩\nTwitter\nFacebook\nGoogle+\nLinkedIn",
      "author": null,
      "published_date": null,
      "meta_description": "While preparing for my Black Hat and DEF CON talks in July of this year, I found the most impactful Entra ID vulnerability that I will probably ever find. One that could have allowed me to compromise every Entra ID tenant in the world (except probably those in national cloud deployments). If you are an Entra ID admin reading this, yes that means complete access to your tenant. The vulnerability consisted of two components: undocumented impersonation tokens that Microsoft uses in their backend for service-to-service (S2S) communication, called “Actor tokens”, and a critical vulnerability in the (legacy) Azure AD Graph API that did not properly validate the originating tenant, allowing these tokens to be used for cross-tenant access.",
      "word_count": 3597,
      "scraping_method": "beautifulsoup"
    }
  ],
  "audio_files": [],
  "stats": {
    "stories_fetched": 5,
    "articles_scraped": 4,
    "total_words": 7652,
    "audio_files_generated": 0
  }
}