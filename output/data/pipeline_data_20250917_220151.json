{
  "timestamp": "20250917_220151",
  "run_date": "2025-09-17T22:01:51.561476",
  "config": {
    "environment": "development",
    "max_stories": 5,
    "tts_voice": "en-US-Neural2-D"
  },
  "stories": [
    {
      "id": 45283887,
      "title": "Slack has raised our charges by $195k per year",
      "url": "https://skyfall.dev/posts/slack",
      "score": 869,
      "by": "JustSkyfall",
      "time": 1758159431,
      "descendants": 418,
      "type": "story",
      "created_at": "2025-09-17T15:37:11"
    },
    {
      "id": 45283306,
      "title": "Meta Ray-Ban Display",
      "url": "https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/",
      "score": 354,
      "by": "martpie",
      "time": 1758155444,
      "descendants": 493,
      "type": "story",
      "created_at": "2025-09-17T14:30:44"
    },
    {
      "id": 45279384,
      "title": "WASM 3.0 Completed",
      "url": "https://webassembly.org/news/2025-09-17-wasm-3.0/",
      "score": 839,
      "by": "todsacerdoti",
      "time": 1758133013,
      "descendants": 340,
      "type": "story",
      "created_at": "2025-09-17T08:16:53"
    },
    {
      "id": 45284311,
      "title": "Show HN: The text disappears when you screenshot it",
      "url": "https://unscreenshottable.vercel.app/?text=Hello",
      "score": 173,
      "by": "zikero",
      "time": 1758161925,
      "descendants": 62,
      "type": "story",
      "created_at": "2025-09-17T16:18:45"
    },
    {
      "id": 45251111,
      "title": "Orange Pi RV2 $40 RISC-V SBC: Friendly Gateway to IoT and AI Projects",
      "url": "https://riscv.org/ecosystem-news/2025/09/orange-pi-rv2-40-risc-v-sbc-friendly-gateway-to-iot-and-ai-projects/",
      "score": 37,
      "by": "warrenm",
      "time": 1757951162,
      "descendants": 27,
      "type": "story",
      "created_at": "2025-09-15T05:46:02"
    },
    {
      "id": 45281139,
      "title": "A postmortem of three recent issues",
      "url": "https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues",
      "score": 273,
      "by": "moatmoat",
      "time": 1758141667,
      "descendants": 87,
      "type": "story",
      "created_at": "2025-09-17T10:41:07"
    }
  ],
  "scraped_content": [
    {
      "url": "https://skyfall.dev/posts/slack",
      "title": "Slack has raised our charges by $195k per year",
      "content": "Slack is extorting us with a $195k/yr bill increase\nAn open letter, or something\nSeptember 18th 2025\nFor nearly 11 years, Hack Club - a nonprofit that provides coding education and community to teenagers worldwide - has used Slack as the tool for communication. We weren’t freeloaders. A few years ago, when Slack transitioned us from their free nonprofit plan to a $5,000/year arrangement, we happily paid. It was reasonable, and we valued the service they provided to our community.\nHowever, two days ago, Slack reached out to us and said that if we don’t agree to pay an extra $50k this week and $200k a year, they’ll deactivate our Slack workspace and delete all of our message history.\nOne could argue that Slack is free to stop providing us the nonprofit offer at any time, but in my opinion, a six month grace period is the bare minimum for a massive hike like this, if not more. Essentially, Salesforce (a $230 billion company) is strong-arming a small nonprofit for teens, by providing less than a week to pony up a pretty massive sum of money, or risk cutting off all our communications. That’s absurd.\nThe impact\nThe small amount of notice has also been catastrophic for the programs that we run. Dozens of our staff and volunteers are now scrambling to update systems, rebuild integrations and migrate years of institutional knowledge. The opportunity cost of this forced migration is simply staggering.\nAnyway, we’re moving to Mattermost. This experience has taught us that owning your data is incredibly important, and if you’re a small business especially, then I’d advise you move away too.\nThis post was rushed out because, well, this has been a shock! If you’d like any additional details then feel free to send me an email.",
      "author": null,
      "published_date": null,
      "meta_description": "An open letter, or something",
      "word_count": 299,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://webassembly.org/news/2025-09-17-wasm-3.0/",
      "title": "WASM 3.0 Completed",
      "content": "Three years ago, version 2.0 of the Wasm standard was (essentially) finished, which brought a number of new features, such as vector instructions, bulk memory operations, multiple return values, and simple reference types.\n\nIn the meantime, the Wasm W3C Community Group and Working Group have not been lazy. Today, we are happy to announce the release of Wasm 3.0 as the new “live” standard.\n\nThis is a substantially larger update: several big features, some of which have been in the making for six or eight years, finally made it over the finishing line.\n• 64-bit address space. Memories and tables can now be declared to use as their address type instead of just . That expands the available address space of Wasm applications from 4 gigabytes to (theoretically) 16 exabytes, to the extent that physical hardware allows. While the web will necessarily keep enforcing certain limits — on the web, a 64-bit memory is limited to 16 gigabytes — the new flexibility is especially interesting for non-web ecosystems using Wasm, as they can support much, much larger applications and data sets now.\n• Multiple memories. Contrary to popular belief, Wasm applications were always able to use multiple memory objects — and hence multiple address spaces — simultaneously. However, previously that was only possible by declaring and accessing each of them in separate modules. This gap has been closed, a single module can now declare (define or import) multiple memories and directly access them, including directly copying data between them. This finally allows tools like wasm-merge, which perform “static linking” on two or more Wasm modules by merging them into one, to work for all Wasm modules. It also paves the way for new uses of separate address spaces, e.g., for security (separating private data), for buffering, or for instrumentation.\n• Garbage collection. In addition to expanding the capabilities of raw linear memories, Wasm also adds support for a new (and separate) form of storage that is automatically managed by the Wasm runtime via a garbage collector. Staying true to the spirit of Wasm as a low-level language, Wasm GC is low-level as well: a compiler targeting Wasm can declare the memory layout of its runtime data structures in terms of struct and array types, plus unboxed tagged integers, whose allocation and lifetime is then handled by Wasm. But that’s it. Everything else, such as engineering suitable representations for source-language values, including implementation details like method tables, remains the responsibility of compilers targeting Wasm. There are no built-in object systems, nor closures or other higher-level constructs — which would inevitably be heavily biased towards specific languages. Instead, Wasm only provides the basic building blocks for representing such constructs and focuses purely on the memory management aspect.\n• Typed references. The GC extension is built upon a substantial extension to the Wasm type system, which now supports much richer forms of references. Reference types can now describe the exact shape of the referenced heap value, avoiding additional runtime checks that would otherwise be needed to ensure safety. This more expressive typing mechanism, including subtyping and type recursion, is also available for function references, making it possible to perform safe indirect function calls without any runtime type or bounds check, through the new instruction.\n• Tail calls. Tail calls are a variant of function calls that immediately exit the current function, and thereby avoid taking up additional stack space. Tail calls are an important mechanism that is used in various language implementations both in user-visible ways (e.g., in functional languages) and for internal techniques (e.g., to implement stubs). Wasm tail calls are fully general and work for callees both selected statically (by function index) and dynamically (by reference or table).\n• Exception handling. Exceptions provide a way to locally abort execution, and are a common feature in modern programming languages. Previously, there was no efficient way to compile exception handling to Wasm, and existing compilers typically resorted to convoluted ways of implementing them by escaping to the host language, e.g., JavaScript. This was neither portable nor efficient. Wasm 3.0 hence provides native exception handling within Wasm. Exceptions are defined by declaring exception tags with associated payload data. As one would expect, an exception can be thrown, and selectively be caught by a surrounding handler, based on its tag. Exception handlers are a new form of block instruction that includes a dispatch list of tag/label pairs or catch-all labels to define where to jump when an exception occurs.\n• Relaxed vector instructions. Wasm 2.0 added a large set of vector (SIMD) instructions, but due to differences in hardware, some of these instructions have to do extra work on some platforms to achieve the specified semantics. In order to squeeze out maximum performance, Wasm 3.0 introduces “relaxed” variants of these instructions that are allowed to have implementation-dependent behavior in certain edge cases. This behavior must be selected from a pre-specified set of legal choices.\n• Deterministic profile. To make up for the added semantic fuzziness of relaxed vector instructions, and in order to support settings that demand or need deterministic execution semantics (such as blockchains, or replayable systems), the Wasm standard now specifies a deterministic default behavior for every instruction with otherwise non-deterministic results — currently, this includes floating-point operators and their generated NaN values and the aforementioned relaxed vector instructions. Between platforms choosing to implement this deterministic execution profile, Wasm thereby is fully deterministic, reproducible, and portable.\n• Custom annotation syntax. Finally, the Wasm text format has been enriched with generic syntax for placing annotations in Wasm source code. Analogous to custom sections in the binary format, these annotations are not assigned any meaning by the Wasm standard itself, and can be chosen to be ignored by implementations. However, they provide a way to represent the information stored in custom sections in human-readable and writable form, and concrete annotations can be specified by downstream standards.\n\nIn addition to these core features, embeddings of Wasm into JavaScript benefit from a new extension to the JS API:\n• JS string builtins. JavaScript string values can already be passed to Wasm as externrefs. Functions from this new primitive library can be imported into a Wasm module to directly access and manipulate such external string values inside Wasm.\n\nWith these new features, Wasm has much better support for compiling high-level programming languages. Enabled by this, we have seen various new languages popping up to target Wasm, such as Java, OCaml, Scala, Kotlin, Scheme, or Dart, all of which use the new GC feature.\n\nOn top of all these goodies, Wasm 3.0 also is the first version of the standard that has been produced with the new SpecTec tool chain. We believe that this makes for an even more reliable specification.\n\nWasm 3.0 is already shipping in most major web browsers, and support in stand-alone engines like Wasmtime is on track to completion as well. The Wasm feature status page tracks support across engines.",
      "author": null,
      "published_date": null,
      "meta_description": "",
      "word_count": 1155,
      "scraping_method": "goose3"
    },
    {
      "url": "https://riscv.org/ecosystem-news/2025/09/orange-pi-rv2-40-risc-v-sbc-friendly-gateway-to-iot-and-ai-projects/",
      "title": "Orange Pi RV2 $40 RISC-V SBC: Friendly Gateway to IoT and AI Projects",
      "content": "What if you could explore the innovative world of RISC-V computing without breaking the bank? The Orange Pi RV2 promises exactly that, a budget-friendly gateway into an architecture that’s reshaping the future of processors. With its 8-core RISC-V processor, a host of connectivity options, and a price tag starting at just $40, this single-board computer (SBC) is designed to appeal to developers, hobbyists, and industry professionals alike. But here’s the catch: while it excels in specialized tasks like IoT and lightweight AI, it’s not built to replace your desktop PC. This duality makes the RV2 both intriguing and polarizing, raising the question: can affordability and innovation truly coexist in the world of SBCs?\n\nIn this overview, Interfacing Linux explores what makes the Orange Pi RV2 a standout in its niche, from its energy-efficient design to its versatile hardware capabilities. You’ll discover how its dual NVMe slots, GPIO interface, and AI-optimized processor open doors to industrial automation and IoT projects, while also uncovering its limitations in software support and desktop performance. Whether you’re a tinkerer eager to experiment with RISC-V or a professional seeking a cost-effective solution for specialized applications, the RV2 offers plenty to unpack. By the end, you might just find yourself rethinking what’s possible with a $40 SBC.",
      "author": null,
      "published_date": null,
      "meta_description": "",
      "word_count": 210,
      "scraping_method": "goose3"
    },
    {
      "url": "https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues",
      "title": "A postmortem of three recent issues",
      "content": "Between August and early September, three infrastructure bugs intermittently degraded Claude's response quality. We've now resolved these issues and want to explain what happened.\n\nIn early August, a number of users began reporting degraded responses from Claude. These initial reports were difficult to distinguish from normal variation in user feedback. By late August, the increasing frequency and persistence of these reports prompted us to open an investigation that led us to uncover three separate infrastructure bugs.\n\nTo state it plainly: We never reduce model quality due to demand, time of day, or server load. The problems our users reported were due to infrastructure bugs alone.\n\nWe recognize users expect consistent quality from Claude, and we maintain an extremely high bar for ensuring infrastructure changes don't affect model outputs. In these recent incidents, we didn't meet that bar. The following postmortem explains what went wrong, why detection and resolution took longer than we would have wanted, and what we're changing to prevent similar future incidents.\n\nWe don't typically share this level of technical detail about our infrastructure, but the scope and complexity of these issues justified a more comprehensive explanation.\n\nHow we serve Claude at scale\n\nWe serve Claude to millions of users via our first-party API, Amazon Bedrock, and Google Cloud's Vertex AI. We deploy Claude across multiple hardware platforms, namely AWS Trainium, NVIDIA GPUs, and Google TPUs. This approach provides the capacity and geographic distribution necessary to serve users worldwide.\n\nEach hardware platform has different characteristics and requires specific optimizations. Despite these variations, we have strict equivalence standards for model implementations. Our aim is that users should get the same quality responses regardless of which platform serves their request. This complexity means that any infrastructure change requires careful validation across all platforms and configurations.\n\nThe overlapping nature of these bugs made diagnosis particularly challenging. The first bug was introduced on August 5, affecting approximately 0.8% of requests made to Sonnet 4. Two more bugs arose from deployments on August 25 and 26.\n\nAlthough initial impacts were limited, a load balancing change on August 29 started to increase affected traffic. This caused many more users to experience issues while others continued to see normal performance, creating confusing and contradictory reports.\n\nBelow we describe the three bugs that caused the degradation, when they occurred, and how we resolved them:\n\nOn August 5, some Sonnet 4 requests were misrouted to servers configured for the upcoming 1M token context window. This bug initially affected 0.8% of requests. On August 29, a routine load balancing change unintentionally increased the number of short-context requests routed to the 1M context servers. At the worst impacted hour on August 31, 16% of Sonnet 4 requests were affected.\n\nApproximately 30% of Claude Code users who made requests during this period had at least one message routed to the wrong server type, resulting in degraded responses. On Amazon Bedrock, misrouted traffic peaked at 0.18% of all Sonnet 4 requests from August 12. Incorrect routing affected less than 0.0004% of requests on Google Cloud's Vertex AI between August 27 and September 16.\n\nHowever, some users were affected more severely, as our routing is \"sticky\". This meant that once a request was served by the incorrect server, subsequent follow-ups were likely to be served by the same incorrect server.\n\nResolution: We fixed the routing logic to ensure short- and long-context requests were directed to the correct server pools. We deployed the fix on September 4. A rollout to our first-party platforms and Google Cloud’s Vertex was completed by September 16. The fix is in the process of being rolled out on Bedrock.\n\nOn August 25, we deployed a misconfiguration to the Claude API TPU servers that caused an error during token generation. An issue caused by a runtime performance optimization occasionally assigned a high probability to tokens that should rarely be produced given the context, for example producing Thai or Chinese characters in response to English prompts, or producing obvious syntax errors in code. A small subset of users that asked a question in English might have seen \"สวัสดี\" in the middle of the response, for example.\n\nThis corruption affected requests made to Opus 4.1 and Opus 4 on August 25-28, and requests to Sonnet 4 August 25–September 2. Third-party platforms were not affected by this issue.\n\nResolution: We identified the issue and rolled back the change on September 2. We've added detection tests for unexpected character outputs to our deployment process.\n\nOn August 25, we deployed code to improve how Claude selects tokens during text generation. This change inadvertently triggered a latent bug in the XLA:TPU[1] compiler, which has been confirmed to affect requests to Claude Haiku 3.5.\n\nWe also believe this could have impacted a subset of Sonnet 4 and Opus 3 on the Claude API. Third-party platforms were not affected by this issue.\n\nResolution: We first observed the bug affecting Haiku 3.5 and rolled it back on September 4. We later noticed user reports of problems with Opus 3 that were compatible with this bug, and rolled it back on September 12. After extensive investigation we were unable to reproduce this bug on Sonnet 4 but decided to also roll it back out of an abundance of caution.\n\nSimultaneously, we have (a) been working with the XLA:TPU team on a fix for the compiler bug and (b) rolled out a fix to use exact top-k with enhanced precision. For details, see the deep dive below.\n\nA closer look at the XLA compiler bug\n\nTo illustrate the complexity of these issues, here's how the XLA compiler bug manifested and why it proved particularly challenging to diagnose.\n\nWhen Claude generates text, it calculates probabilities for each possible next word, then randomly chooses a sample from this probability distribution. We use \"top-p sampling\" to avoid nonsensical outputs—only considering words whose cumulative probability reaches a threshold (typically 0.99 or 0.999). On TPUs, our models run across multiple chips, with probability calculations happening in different locations. To sort these probabilities, we need to coordinate data between chips, which is complex.[2]\n\nIn December 2024, we discovered our TPU implementation would occasionally drop the most probable token when temperature was zero. We deployed a workaround to fix this case.\n\nThe root cause involved mixed precision arithmetic. Our models compute next-token probabilities in bf16 (16-bit floating point). However, the vector processor is fp32-native, so the TPU compiler (XLA) can optimize runtime by converting some operations to fp32 (32-bit). This optimization pass is guarded by the flag which defaults to true.\n\nThis caused a mismatch: operations that should have agreed on the highest probability token were running at different precision levels. The precision mismatch meant they didn't agree on which token had the highest probability. This caused the highest probability token to sometimes disappear from consideration entirely.\n\nOn August 26, we deployed a rewrite of our sampling code to fix the precision issues and improve how we handled probabilities at the limit that reach the top-p threshold. But in fixing these problems, we exposed a trickier one.\n\nOur fix removed the December workaround because we believed we'd solved the root cause. This led to a deeper bug in the approximate top-k operation—a performance optimization that quickly finds the highest probability tokens.[3] This approximation sometimes returned completely wrong results, but only for certain batch sizes and model configurations. The December workaround had been inadvertently masking this problem.\n\nThe bug's behavior was frustratingly inconsistent. It changed depending on unrelated factors such as what operations ran before or after it, and whether debugging tools were enabled. The same prompt might work perfectly on one request and fail on the next.\n\nWhile investigating, we also discovered that the exact top-k operation no longer had the prohibitive performance penalty it once did. We switched from approximate to exact top-k and standardized some additional operations on fp32 precision.[4] Model quality is non-negotiable, so we accepted the minor efficiency impact.\n\nOur validation process ordinarily relies on benchmarks alongside safety evaluations and performance metrics. Engineering teams perform spot checks and deploy to small \"canary\" groups first.\n\nThese issues exposed critical gaps that we should have identified earlier. The evaluations we ran simply didn't capture the degradation users were reporting, in part because Claude often recovers well from isolated mistakes. Our own privacy practices also created challenges in investigating reports. Our internal privacy and security controls limit how and when engineers can access user interactions with Claude, in particular when those interactions are not reported to us as feedback. This protects user privacy but prevents engineers from examining the problematic interactions needed to identify or reproduce bugs.\n\nEach bug produced different symptoms on different platforms at different rates. This created a confusing mix of reports that didn't point to any single cause. It looked like random, inconsistent degradation.\n\nMore fundamentally, we relied too heavily on noisy evaluations. Although we were aware of an increase in reports online, we lacked a clear way to connect these to each of our recent changes. When negative reports spiked on August 29, we didn't immediately make the connection to an otherwise standard load balancing change.\n\nAs we continue to improve our infrastructure, we're also improving the way we evaluate and prevent bugs like those discussed above across all platforms where we serve Claude. Here's what we're changing:\n• More sensitive evaluations: To help discover the root cause of any given issue, we’ve developed evaluations that can more reliably differentiate between working and broken implementations. We’ll keep improving these evaluations to keep a closer eye on model quality.\n• Quality evaluations in more places: Although we run regular evaluations on our systems, we will run them continuously on true production systems to catch issues such as the context window load balancing error.\n• Faster debugging tooling: We'll develop infrastructure and tooling to better debug community-sourced feedback without sacrificing user privacy. Additionally, some bespoke tools developed here will be used to reduce the remediation time in future similar incidents, if those should occur.\n\nEvals and monitoring are important. But these incidents have shown that we also need continuous signal from users when responses from Claude aren't up to the usual standard. Reports of specific changes observed, examples of unexpected behavior encountered, and patterns across different use cases all helped us isolate the issues.\n\nIt remains particularly helpful for users to continue to send us their feedback directly. You can use the command in Claude Code or you can use the \"thumbs down\" button in the Claude apps to do so. Developers and researchers often create new and interesting ways to evaluate model quality that complement our internal testing. If you'd like to share yours, reach out to feedback@anthropic.com.\n\nWe remain grateful to our community for these contributions.",
      "author": null,
      "published_date": null,
      "meta_description": "This is a technical report on three bugs that intermittently degraded responses from Claude. Below we explain what happened, why it took time to fix, and what we're changing.",
      "word_count": 1785,
      "scraping_method": "goose3"
    }
  ],
  "audio_files": [
    "output/audio/hackercast_20250917_220024.mp3"
  ],
  "stats": {
    "stories_fetched": 6,
    "articles_scraped": 4,
    "total_words": 3449,
    "audio_files_generated": 1
  }
}