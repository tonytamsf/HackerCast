{
  "timestamp": "20250930_105346",
  "run_date": "2025-09-30T10:53:46.709889",
  "config": {
    "environment": "development",
    "max_stories": 20,
    "tts_voice": "en-US-Neural2-D"
  },
  "stories": [
    {
      "id": 45426490,
      "title": "Kagi News",
      "url": "https://blog.kagi.com/kagi-news",
      "score": 441,
      "by": "grappler",
      "time": 1759244940,
      "descendants": 163,
      "type": "story",
      "created_at": "2025-09-30T08:09:00"
    },
    {
      "id": 45427482,
      "title": "Launch HN: Airweave (YC X25) – Let agents search any app",
      "url": "https://github.com/airweave-ai/airweave",
      "score": 71,
      "by": "lennertjansen",
      "time": 1759249269,
      "descendants": 2,
      "type": "story",
      "created_at": "2025-09-30T09:21:09"
    },
    {
      "id": 45427634,
      "title": "A $196 fine-tuned 7B model outperforms OpenAI o3 on document extraction",
      "url": "https://arxiv.org/abs/2509.22906",
      "score": 62,
      "by": "henriquegodoy",
      "time": 1759249887,
      "descendants": 14,
      "type": "story",
      "created_at": "2025-09-30T09:31:27"
    },
    {
      "id": 45427197,
      "title": "Leaked Apple M5 9 core Geekbench scores",
      "url": "https://browser.geekbench.com/v6/cpu/14173685",
      "score": 78,
      "by": "aurareturn",
      "time": 1759248036,
      "descendants": 36,
      "type": "story",
      "created_at": "2025-09-30T09:00:36"
    },
    {
      "id": 45427059,
      "title": "Visualizations of Random Attractors Found Using Lyapunov Exponents",
      "url": "https://paulbourke.net/fractals/lyapunov/",
      "score": 46,
      "by": "cs702",
      "time": 1759247414,
      "descendants": 7,
      "type": "story",
      "created_at": "2025-09-30T08:50:14"
    },
    {
      "id": 45425746,
      "title": "Frank Chimero: I think we're in the lemon stage of the internet",
      "url": "https://frankchimero.com/blog/2025/selling-lemons/",
      "score": 96,
      "by": "gregwolanski",
      "time": 1759241668,
      "descendants": 38,
      "type": "story",
      "created_at": "2025-09-30T07:14:28"
    },
    {
      "id": 45428482,
      "title": "Boeing Has Started Working on a 737 MAX Replacement",
      "url": "https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df",
      "score": 8,
      "by": "bookofjoe",
      "time": 1759253494,
      "descendants": 6,
      "type": "story",
      "created_at": "2025-09-30T10:31:34"
    },
    {
      "id": 45425298,
      "title": "dgsh – Directed Graph Shell",
      "url": "https://www2.dmst.aueb.gr/dds/sw/dgsh/",
      "score": 67,
      "by": "pabs3",
      "time": 1759239562,
      "descendants": 27,
      "type": "story",
      "created_at": "2025-09-30T06:39:22"
    },
    {
      "id": 45395132,
      "title": "Computer Vision: Algorithms and Applications, 2nd ed",
      "url": "https://szeliski.org/Book/",
      "score": 37,
      "by": "ibobev",
      "time": 1758976049,
      "descendants": 4,
      "type": "story",
      "created_at": "2025-09-27T05:27:29"
    },
    {
      "id": 45425714,
      "title": "Deml: The Directed Acyclic Graph Elevation Markup Language",
      "url": "https://github.com/Mcmartelle/deml",
      "score": 45,
      "by": "todsacerdoti",
      "time": 1759241557,
      "descendants": 8,
      "type": "story",
      "created_at": "2025-09-30T07:12:37"
    },
    {
      "id": 45422653,
      "title": "Google CTF 2025 – webz : Exploiting zlib's Huffman Code Table",
      "url": "https://velog.io/@0range1337/CTF-Google-CTF-2025-webz-Exploiting-zlibs-Huffman-Code-Table-English",
      "score": 72,
      "by": "rot22",
      "time": 1759215011,
      "descendants": 11,
      "type": "story",
      "created_at": "2025-09-29T23:50:11"
    },
    {
      "id": 45395156,
      "title": "A Gentle Introduction to CUDA PTX",
      "url": "https://philipfabianek.com/posts/cuda-ptx-introduction/",
      "score": 12,
      "by": "ashvardanian",
      "time": 1758976241,
      "descendants": 0,
      "type": "story",
      "created_at": "2025-09-27T05:30:41"
    },
    {
      "id": 45423917,
      "title": "Comprehension debt: A ticking time bomb of LLM-generated code",
      "url": "https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/",
      "score": 433,
      "by": "todsacerdoti",
      "time": 1759228659,
      "descendants": 274,
      "type": "story",
      "created_at": "2025-09-30T03:37:39"
    },
    {
      "id": 45426099,
      "title": "BrowserPod: In-browser full-stack environments for IDEs and Agents via WASM",
      "url": "https://labs.leaningtech.com/blog/browserpod-annoucement",
      "score": 27,
      "by": "apignotti",
      "time": 1759243338,
      "descendants": 8,
      "type": "story",
      "created_at": "2025-09-30T07:42:18"
    },
    {
      "id": 45427982,
      "title": "Introducing Sora 2 [video]",
      "url": "https://www.youtube.com/watch?v=gzneGhpXwjU",
      "score": 35,
      "by": "skilled",
      "time": 1759251301,
      "descendants": 9,
      "type": "story",
      "created_at": "2025-09-30T09:55:01"
    },
    {
      "id": 45428052,
      "title": "Not only am I losing my livelihood to AI – now it's stealing my em dashes too",
      "url": "https://www.theguardian.com/lifeandstyle/2025/oct/01/artificial-intelligence-em-dashes-ai-stealing-my-livelihood",
      "score": 27,
      "by": "Freak_NL",
      "time": 1759251551,
      "descendants": 16,
      "type": "story",
      "created_at": "2025-09-30T09:59:11"
    },
    {
      "id": 45428020,
      "title": "Genomic analyses of hair from Ludwig van Beethoven",
      "url": "https://www.cell.com/current-biology/fulltext/S0960-9822(23)00181-1",
      "score": 3,
      "by": "petercooper",
      "time": 1759251427,
      "descendants": 0,
      "type": "story",
      "created_at": "2025-09-30T09:57:07"
    },
    {
      "id": 45423004,
      "title": "Bcachefs removed from the mainline kernel",
      "url": "https://lwn.net/Articles/1040120/",
      "score": 196,
      "by": "Bogdanp",
      "time": 1759218736,
      "descendants": 133,
      "type": "story",
      "created_at": "2025-09-30T00:52:16"
    },
    {
      "id": 45426673,
      "title": "AI will happily design the wrong thing for you",
      "url": "https://www.antonsten.com/articles/ai-will-happily-design-the-wrong-thing-for-you/",
      "score": 73,
      "by": "zdw",
      "time": 1759245637,
      "descendants": 9,
      "type": "story",
      "created_at": "2025-09-30T08:20:37"
    }
  ],
  "scraped_content": [
    {
      "url": "https://blog.kagi.com/kagi-news",
      "title": "Kagi News",
      "content": "Introducing Kagi News\n30 Sep, 2025\nA comprehensive daily press review with global news. Fully private, with sources openly curated by our community.\nNews is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be.\nWe can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.\nOur approach: Signal over noise\nKagi News operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then distill this massive information into one comprehensive daily briefing, while clearly citing sources.\nWe strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.\nDesign principles that put readers first\nOne daily update: We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.\nFive-minute complete understanding: Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.\nDiversity over echo chambers: Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.\nPrivacy by design: Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.\nCommunity-driven sources: Our news sources are open source and community-curated through our public GitHub repository. Anyone can propose additions, flag problems, or suggest improvements.\nCustomizable: In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.\nNews in your language: You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using Kagi Translate. The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.\nTechnical implementation that respects publishers\nWe don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.\nReady to experience news differently?\nIf you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:\nWeb\niOS\nAndroid",
      "author": "Vladimir Prelovac",
      "published_date": null,
      "meta_description": "*A comprehensive daily press review with global news.",
      "word_count": 532,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://github.com/airweave-ai/airweave",
      "title": "Launch HN: Airweave (YC X25) – Let agents search any app",
      "content": "Airweave is a tool that lets agents search any app. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.\n\nThe search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.\n\nMake sure docker and docker-compose are installed, then...\n\nThat's it! Access the dashboard at http://localhost:8080\n\nWe welcome contributions! Please check CONTRIBUTING.md for details.\n\nAirweave is released under the MIT license.\n• Discord - Get help and discuss features",
      "author": null,
      "published_date": null,
      "meta_description": "Airweave lets agents search any app. Contribute to airweave-ai/airweave development by creating an account on GitHub.",
      "word_count": 106,
      "scraping_method": "goose3"
    },
    {
      "url": "https://arxiv.org/abs/2509.22906",
      "title": "A $196 fine-tuned 7B model outperforms OpenAI o3 on document extraction",
      "content": "Computer Science > Computation and Language\narXiv:2509.22906 (cs)\n[Submitted on 26 Sep 2025]\nTitle:Extract-0: A Specialized Language Model for Document Information Extraction\nAuthors:Henrique Godoy View a PDF of the paper titled Extract-0: A Specialized Language Model for Document Information Extraction, by Henrique Godoy\nView PDF\nHTML (experimental)\nAbstract:This paper presents Extract-0, a 7-billion parameter language model specifically optimized for document information extraction that achieves performance exceeding models with parameter counts several orders of magnitude larger. Through a novel combination of synthetic data generation, supervised fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of 0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology employs a memory-preserving synthetic data generation pipeline that produces 280,128 training examples from diverse document sources, followed by parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M out of 7.66B parameters). The reinforcement learning phase introduces a novel semantic similarity-based reward function that handles the inherent ambiguity in information extraction tasks. This research demonstrates that task-specific optimization can yield models that surpass general-purpose systems while requiring substantially fewer computational resource.\nSubjects:\nComputation and Language (cs.CL); Artificial Intelligence (cs.AI)\nCite as:\narXiv:2509.22906 [cs.CL]\n(or\narXiv:2509.22906v1 [cs.CL] for this version)\nhttps://doi.org/10.48550/arXiv.2509.22906\nFocus to learn more\narXiv-issued DOI via DataCite (pending registration)\nSubmission history From: Henrique Godoy [view email]\n[v1]\nFri, 26 Sep 2025 20:34:43 UTC (1,102 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Extract-0: A Specialized Language Model for Document Information Extraction, by Henrique GodoyView PDFHTML (experimental)TeX SourceOther Formats\nview license\nCurrent browse context: cs.CL\n< prev\n|\nnext >\nnew\n|\nrecent\n| 2025-09\nChange to browse by:\ncs\ncs.AI\nReferences & Citations\nNASA ADSGoogle Scholar\nSemantic Scholar\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n×\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv (What is alphaXiv?)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub Toggle\nDagsHub (What is DagsHub?)\nGotitPub Toggle\nGotit.pub (What is GotitPub?)\nHuggingface Toggle\nHugging Face (What is Huggingface?)\nLinks to Code Toggle\nPapers with Code (What is Papers with Code?)\nScienceCast Toggle\nScienceCast (What is ScienceCast?)\nDemos\nDemos\nReplicate Toggle\nReplicate (What is Replicate?)\nSpaces Toggle\nHugging Face Spaces (What is Spaces?)\nSpaces Toggle\nTXYZ.AI (What is TXYZ.AI?)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower (What are Influence Flowers?)\nCore recommender toggle\nCORE Recommender (What is CORE?)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? |\nDisable MathJax (What is MathJax?)",
      "author": null,
      "published_date": null,
      "meta_description": "Abstract page for arXiv paper 2509.22906: Extract-0: A Specialized Language Model for Document Information Extraction",
      "word_count": 559,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://browser.geekbench.com/v6/cpu/14173685",
      "title": "Leaked Apple M5 9 core Geekbench scores",
      "content": "iPad17,3\n4133\nSingle-Core Score\n15437\nMulti-Core Score\nGeekbench 6.5.0 for iOS AArch64\nResult Information\nUpload Date\nSeptember 30 2025 12:36 PM\nViews\n5919\nSystem Information\nSystem Information\nOperating System\niOS 26.0\nModel\niPad17,3\nModel IDiPad17,3\nMotherboard\nJ820AP\nCPU Information\nName\nARM\nTopology\n1 Processor, 9 Cores\nIdentifier\nARM\nBase Frequency\n4.42 GHz\nCluster 13 Cores\nCluster 26 Cores\nL1 Instruction Cache128 KB x 1\nL1 Data Cache64.0 KB x 1\nL2 Cache6.00 MB x 1\nInstruction Setsneon aes sha1 sha2 neon-fp16 neon-dotprod i8mm sme-i8i32 sme-f32f32 sme2\nMemory Information\nSize\n11.20 GB\nSingle-Core Performance\nSingle-Core Score\n4133\nFile Compression\n3552\n510.1 MB/sec\nNavigation\n3659\n22.0 routes/sec\nHTML5 Browser\n4260\n87.2 pages/sec\nPDF Renderer\n3734\n86.1 Mpixels/sec\nPhoto Library\n3719\n50.5 images/sec\nClang\n4649\n22.9 Klines/sec\nText Processing\n3822\n306.1 pages/sec\nAsset Compression\n3547\n109.9 MB/sec\nObject Detection\n6032\n180.5 images/sec\nBackground Blur\n4104\n17.0 images/sec\nHorizon Detection\n4139\n128.8 Mpixels/sec\nObject Remover\n5276\n405.6 Mpixels/sec\nHDR\n4678\n137.3 Mpixels/sec\nPhoto Filter\n5061\n50.2 images/sec\nRay Tracer\n3302\n3.20 Mpixels/sec\nStructure from Motion\n3836\n121.4 Kpixels/sec\nMulti-Core Performance\nMulti-Core Score\n15437\nFile Compression\n12308\n1.73 GB/sec\nNavigation\n17065\n102.8 routes/sec\nHTML5 Browser\n17958\n367.6 pages/sec\nPDF Renderer\n15774\n363.8 Mpixels/sec\nPhoto Library\n18268\n247.9 images/sec\nClang\n23236\n114.4 Klines/sec\nText Processing\n4956\n396.9 pages/sec\nAsset Compression\n18577\n575.6 MB/sec\nObject Detection\n14896\n445.7 images/sec\nBackground Blur\n13114\n54.3 images/sec\nHorizon Detection\n19111\n594.7 Mpixels/sec\nObject Remover\n15968\n1.23 Gpixels/sec\nHDR\n18909\n554.9 Mpixels/sec\nPhoto Filter\n15246\n151.3 images/sec\nRay Tracer\n18888\n18.3 Mpixels/sec\nStructure from Motion\n16186\n512.5 Kpixels/sec\nCompare\nSet Baseline",
      "author": null,
      "published_date": null,
      "meta_description": "Benchmark results for an iPad17,3 with an ARM processor.\n",
      "word_count": 260,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://paulbourke.net/fractals/lyapunov/",
      "title": "Visualizations of Random Attractors Found Using Lyapunov Exponents",
      "content": "Written by Paul Bourke\n\n October 2001 Contribution by Philip Ham: attractor.basic\n\n and Python implementation by Johan Bichel Lindegaard. This document is \"littered\" with a selection of attractors found using the techniques described. In order for a system to exhibit chaotic behaviour it must be non linear. Representing chaotic systems on a screen or on paper leads one to considering a two dimensional system, an equation in two variables. One possible two dimensional non-linear system, the one used here, is the quadratic map defined as follows. The standard measure for determining whether or not a system is chaotic is the Lyapunov exponent, normally represented by the lambda symbol. Consider two close points at step n, x and x +dx . At the next time step they will have diverged, namely to x and x +dx . It is this average rate of divergence (or convergence) that the Lyapunov exponent captures. Another way to think about the Lyapunov exponent is as the rate at which information about the initial conditions is lost. There are as many Lyapunov exponents as dimensions of the phase space. Considering a region (circle, sphere, hypersphere, etc) in phase space then at a later time all trajectories in this region form an n-dimensional elliptical region. The Lyapunov exponent can be calculated for each dimension. When talking about a single exponent one is normally referring to the largest, this convention will be assumed from now onwards. If the Lyapunov exponent is positive then the system is chaotic and unstable. Nearby points will diverge irrespective of how close they are. Although there is no order the system is still deterministic! The magnitude of the Lyapunov exponent is a measure of the sensitivity to initial conditions, the primary characteristic of a chaotic system. If the Lyapunov exponent is less than zero then the system attracts to a fixed point or stable periodic orbit. These systems are non conservative (dissipative). The absolute value of the exponent indicates the degree of stability. If the Lyapunov exponent is zero then the system is neutrally stable, such systems are conservative and in a steady state mode. To create the chaotic attractors shown on this page each parameter a and b in the quadratic equation above is chosen at random between some bounds (+- 2 say). The system so specified is generated by iterating for some suitably large number of time steps (eg; 100000) steps during which time the image is created and the Lyapunov exponent computed. Note that the first few (1000) timesteps are ignored to allow the system to settle into its \"natural\" behaviour. If the Lyapunov exponent indicates chaos then the image is saved and the program moves on to the next random parameter set. There are a number of ways the series may behave.\n• It may converge to a single point, called a fixed point. These can be detected by comparing the distances between successive points. For numerical reasons this is safer than relying on the Lyapunov exponent which may be infinite (logarithm of 0)\n• It may diverge to infinity, for the range (+- 2) used here for each parameter this is the most likely event. These are also easy to detect and discard, indeed they need to be in order to avoid numerical errors.\n• It will form a periodic orbit, these are identified by their negative Lyapunov exponent.\n• It will exhibit chaos, filling in some region of the plane. These are the solutions that \"look good\" and the ones we wish to identify with the Lyapunov exponent. It should be noted that there may be visually appealing structures that are not chaotic attractors. That is, the resulting image is different for different initial conditions and there is no single basin of attraction. It's interesting how we \"see\" 3 dimensional structures in these essentially 2 dimensional systems. The software used to create these images is given here: gen.c. On average 98% of the random selections of (a , b ) result in an infinite series. This is so common because of the range (-2 <= a,b <= 2) for each of the parameters a and b, the number of infinite cases will reduce greatly with a smaller range. About 1% were point attractors, and about 0.5% were periodic basins of attraction. \n\n Image courtesy of Robert McGregor, Space Coast of Florida. Launch trail perhaps 30 minutes after the shuttle launch (June 2007) dispersing from a column into a smoke ring due to some unusual air currents in the upper atmosphere. Das, A., Das, Pritha, Roy, A\n\n Applicability of Lyapunov Exponent in EEG data analysis. Complexity International, draft manuscript. Peitgen, H., Jurgens, H., Saupe, D\n\n Lyapunov exponents and chaotic attractors in Chaos and fractals - new frontiers of science. Springer, new York, 1992.",
      "author": null,
      "published_date": null,
      "meta_description": "",
      "word_count": 793,
      "scraping_method": "goose3"
    },
    {
      "url": "https://frankchimero.com/blog/2025/selling-lemons/",
      "title": "Frank Chimero: I think we're in the lemon stage of the internet",
      "content": "I’ve been researching a new talk the last few weeks and along the way stumbled across a concept that’s been rattling around in my head. I am writing to share, because I find it a satisfying description for the tech flop era.\nThe idea is called “a market for lemons.” The phrase comes from a 1970 paper by George Akerlof that explains how information asymmetry between buyers and sellers can undermine a marketplace. Akerlof asks us to imagine ourselves buying a used car. Some cars on the lot are reliable, well-maintained gems. Others cars are lemons, the kinds of cars that can make it off the lot but are disasters waiting to happen. The sellers know which cars are which, but you, as a buyer, can’t tell the difference. That information asymmetry affects the average price in the market and eventually impacts the overall market dynamics.\nThe thinking goes like this: if a buyer can’t distinguish between good and bad, everything gets priced somewhere in the middle. If you’re selling junk, this is fantastic news—you’ll probably get paid more than your lemon is worth. If you’re selling a quality used car, this price is insultingly low. As a result, people with good cars leave the market to sell their stuff elsewhere, which pushes the overall quality and price down even further, until eventually all that’s left on the market are lemons.\nI think we’re in the lemon stage of the internet.\nI thought about this last week while shopping online for a sleep mask. Brands like MZOO, YFONG, WAOAW popped up, and these seemed less like companies and more like vowel smoke ejected from a factory flue hole, then slotted into a distribution platform. The long tail of generic brands on e-commerce platforms is a textbook lemons market: good products get drowned out by these alphabet soup products, who use their higher margins to buy sponsored placement in search results. Both buyers and sellers eventually lose (and perhaps the platforms win, as long as they don’t wear out their reputation).\nFor shoppers, buying online now feels like rolling the dice on the quality of the product. For sellers, the gamble is that their survival relies more on gaming the system than actually improving the product.\nI think the post-pandemic experience has been a collective realization that the value that drew us to certain digital products and marketplaces is gone. Much of this reduction in value gets pinned to ZIRP, but there’s another critical factor—the natural flight of value creators. As platforms matured, the users and sellers who generated real value were squeezed out by players focused on capturing value rather than creating it.\nOnce you identify a lemon market, you start to see it all over the place.\nOnline dating. A lemon market where participants have no familiarity with one another participate in strategic self-presentation. High-quality partners (emotionally available, looking for genuine connection) can’t effectively distinguish themselves from those just seeking validation and eventually leave.\nSearch results. A lemon market where platforms profit from sponsored placement, misaligning incentives with user needs. The first page is a minefield: sponsored listings posing as organic results, SEO content farms, affiliate aggregators. You add “reddit” to work around this, but even that has less success these days.\nSocial media. Your feed is now professional content creators, low-effort podcast video clips, algorithmic filler reaction videos, stand-up chaff, and animals. Good ideas don’t happen frequently enough to satisfy the pace of the algorithm, so many have pivoted to newsletters or stopped posting.\nWhat makes the Market for Lemons concept so appealing (and what differentiates it in my mind from enshittification is that everyone can be acting reasonably, pursuing their own interests, and things still get worse for everyone. No one has to be evil or stupid: the platform does what’s profitable, sellers do what works, buyers try to make smart decisions, and yet the whole system degrades into something nobody actually wants.\nI was first introduced to the Market of Lemons by Dan Luu in an essay titled, Why is it so hard to buy things that work well?. Luu applies the market of lemons as a metaphor, and specifically identifies hiring as a market of lemons, because of the information asymmetry for both companies and individuals.\nCompanies have always struggled to tell the difference between great individual contributors and mediocre ones. Lacking a clear way to separate the two, they lump everyone together and rely on proxy games to evaluate skill. Candidates, for their part, walk into interviews without crucial information: whether the company is quietly dysfunctional, whether the manager they liked during interviews is about to quit, or whether the open role itself is little more than a vestige of an abandoned strategy that’s likely to be cut once the other foot drops. The usual signals of strength or weakness don’t signify much at all when it comes to hiring. Layer on the automated scale of the application process—candidates firing off applications by the hundreds, companies screening by the thousands—and the result is a highly inefficient market that wastes everyone’s time. Meaningful signals get drowned out, everyone gets lumped together, rational players opt out to the extent they are able, and the market slides steadily downward.\nThere have been countless attempts to make hiring more rational and efficient—the stuff of startup pitch deck lore. But I’m not sure hiring can ever be much more efficient, because neither side has reason to show themselves as they really are, warts and all. Idealistically, both would come straight; pragmatically, it is a game of chicken. Candidates polish résumés and present curated versions of their abilities, listing outcomes and impact statistics with dubious accuracy and provenance. Companies do the same, putting culture and mission front and center while hiding systematic dysfunctions and looming existential risks. When neither side is forthcoming, you’re left with proxies: a famous logo on a resume, a polished culture deck. Gaming the meta of the system supersedes the actual development or evaluation of skill. And, much to my disappointment, gaming the meta may, in fact, be an essential aspect of most jobs.\nAt this point, it should be obvious how the market for lemons applies to ill-considered AI-generated content. I’ll let you sketch out that argument yourself since it’s fairly straightforward, and this thing is already long enough.\nInstead, let’s zag and revisit my point earlier about system-gaming becoming the most viable playbook instead of focusing on the product. As a consumer and as a designer, I hope this is a temporary state before a massive recalibration. The primacy of meta-activities—optimizing for algorithms, visibility theater, consumer entrapment, externalization of costs, performative internal alignment, horse-trading amongst a set of DOA ideas—is poison. It is a road to nowhere worth going.\nThis reflects a business culture obsessed with outcomes while treating outputs as speed bumps. But outputs (code, design, the products themselves) are the load-bearing work—the actual prerequisites for the outcomes desired. Focusing on outcomes while ignoring outputs means hiding in abstractions and absolving oneself of accountability. If any output is acceptable to hit your targets, what awful things emerge at scale? What horrors happen when success detaches completely from the necessity of being good—having both skill and ethics?\nThe safest, smartest path is also the most mundane: keep the main thing the main thing. Outcomes matter, but output literally comes first. Outputs are the business to everyone outside it—what customers see, buy, and use. You can’t stay safe in abstractions forever. Eventually, you must attend to the reality of what’s in front of you, because that’s where work gets done and where assumptions get validated or falsified (because reality has a surprising amount of detail).\nIn other words, the meta ruins things for everyone. To hide in abstractions is to dodge the reality of your choices. These tactics may get you profit, but you sacrifice benefit. The climb may feel like progress, but at the end you’ll find yourself at the top of a mountain of lemons, perhaps not of your own making, but almost certainly of your own doing.",
      "author": "Frank Chimero",
      "published_date": null,
      "meta_description": "Frank Chimero’s Personal Website",
      "word_count": 1345,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://www2.dmst.aueb.gr/dds/sw/dgsh/",
      "title": "dgsh – Directed Graph Shell",
      "content": "The dgsh suite has been tested under Debian and Ubuntu Linux, FreeBSD, and Mac OS X. A Cygwin port is underway.\n\nAn installation of GraphViz will allow you to visualize the dgsh graphs that you specify in your programs.\n\nBy default, the program and its documentation are installed under /usr/local . You can modify this by setting the PREFIX variable in the `config` step, for example:\n\nTo compile and run dgsh you will need to have the following packages installed in your system:\n\nBy default, the program and its documentation are installed under /usr/local . You can modify this by setting the PREFIX variable in the `config` step, for example:\n\nTo compile and run dgsh you will need to have the following commands installed on your system:\n\nThese are the manual pages for dgsh, the associated helper programs and the API in formats suitable for browsing and printing. The commands are listed in the order of usefulness in everyday scenarios.\n\nReport file type, length, and compression performance for data received from the standard input. The data never touches the disk. Demonstrates the use of an output multipipe to source many commands from one followed by an input multipipe to sink to one command the output of many and the use of dgsh-tee that is used both to propagate the same input to many commands and collect output from many commands orderly in a way that is transparent to users.\n\nProcess the Git history, and list the authors and days of the week ordered by the number of their commits. Demonstrates streams and piping through a function.\n\nProcess a directory containing C source code, and produce a summary of various metrics. Demonstrates nesting, commands without input.\n\nList the names of duplicate files in the specified directory. Demonstrates the combination of streams with a relational join.\n\nHighlight the words that are misspelled in the command's first argument. Demonstrates stream processing with multipipes and the avoidance of pass-through constructs to avoid deadlocks.\n\nRead text from the standard input and list words containing a two-letter palindrome, words containing four consonants, and words longer than 12 characters.\n\nCreates a report for a fixed-size web log file read from the standard input. Demonstrates the combined use of multipipe blocks, writeval and readval to store and retrieve values, and functions in the scatter block. Used to measure throughput increase achieved through parallelism.\n\nRead text from the standard input and create files containing word, character, digram, and trigram frequencies.\n\nGiven as an argument a directory containing object files, show which symbols are declared with global visibility, but should have been declared with file-local (static) visibility instead. Demonstrates the use of dgsh-capable comm (1) to combine data from two sources.\n\nGiven two directory hierarchies A and B passed as input arguments (where these represent a project at different parts of its lifetime) copy the files of hierarchy A to a new directory, passed as a third argument, corresponding to the structure of directories in B. Demonstrates the use of join to process results from two inputs and the use of gather to order asynchronously produced results.\n\nProcess the Git history, and create two PNG diagrams depicting committer activity over time. The most active committers appear at the center vertical of the diagram. Demonstrates image processing, mixining of synchronous and asynchronous processing in a scatter block, and the use of an dgsh-compliant join command.\n\nCount number of times each word appears in the specified input file(s) Demonstrates parallel execution mirroring the Hadoop WordCount example via the dgsh-parallel command. In contrast to GNU parallel, the block generated by dgsh-parallel has N input and output streams, which can be combined by any dgsh-compatible tool, such as dgsh-merge-sum or sort -m.\n\nGiven the specification of two publication venues, read a compressed DBLP computer science bibliography from the standard input (e.g. piped from curl -s http://dblp.uni-trier.de/xml/dblp.xml.gz or from a locally cached copy) and output the number of papers published in each of the two venues as well as the number of authors who have published only in the first venue, the number who have published only in the second one, and authors who have published in both. The venues are specified through the script's first two command-line arguments as a DBLP key prefix, e.g. journals/acta/, conf/icse/, journals/software/, conf/iwpc/, or conf/msr/. Demonstrates the use of dgsh-wrap -e to have sed(1) create two output streams and the use of tee to copy a pair of streams into four ones.\n\nCreate two graphs: 1) a broadened pulse and the real part of its 2D Fourier transform, and 2) a simulated air wave and the amplitude of its 2D Fourier transform. Demonstrates using the tools of the Madagascar shared research environment for computational data analysis in geophysics and related fields. Also demonstrates the use of two scatter blocks in the same script, and the used of named streams.\n\nNuclear magnetic resonance in-phase/anti-phase channel conversion and processing in heteronuclear single quantum coherence spectroscopy. Demonstrate processing of NMR data using the NMRPipe family of programs.\n\nCalculate the iterative FFT for n = 8 in parallel. Demonstrates combined use of permute and multipipe blocks.\n\nReorder columns in a CSV document. Demonstrates the combined use of tee, cut, and paste.\n\nWindows-like DIR command for the current directory. Nothing that couldn't be done with . Demonstrates use of wrapped commands with no input (df, echo).",
      "author": null,
      "published_date": null,
      "meta_description": "",
      "word_count": 892,
      "scraping_method": "goose3"
    },
    {
      "url": "https://szeliski.org/Book/",
      "title": "Computer Vision: Algorithms and Applications, 2nd ed",
      "content": "Welcome to the website (https://szeliski.org/Book) for the second edition of my computer vision textbook, which is now available for purchase at Amazon, Springer, and other booksellers.\n\nTo download an electronic version of the book, please fill in your information on this page. You are welcome to download the PDF website for personal use, but not to repost it on any other website; please post a link to this URL instead.\n\nNote that while the content of this electronic version and the hardcopy versions are the same, the page layout is different, since the electronic version is optimized for online reading. The PDF should be enabled for commenting in your viewer. Also, hyper-links to sections, equations, and references are enabled. To get back to where you were, use the Previous View (Alt-Left-Arrow) command in Acrobat.\n\nThe current download count is 179610 (since 1/23/2022).\n\nThis book is largely based on the computer vision courses that I have co-taught at the University of Washington (2020, 2008, 2005, 2001) with Steve Seitz and Harpreet Sawhney and at Stanford (2003) with David Fleet.\n\nIf you're curious about the process that went into writing my book, I did an interview with Computer Vision News (March 2022).\n\nYou can still download the first edition or potentially purchase it online. The first edition is also available in Chinese and Japanese (translated by Prof. Toru Tamaki).\n\nAdditional good sources for related courses (sorted roughly by most recent first) include:\n• Bill Freeman, Antonio Torralba, and Phillip Isola's 6.8300/6.8301: Advances in Computer Vision class at MIT (Spring 2023)\n• Alyosha Efros' CS194-26/294-26: Intro to Computer Vision and Computational Photography class at Berkeley (Fall 2024)\n• Justin Johnson's EECS 498.008 / 598.008: Deep Learning for Computer Vision class at the University of Michigan (Winter 2022), which is an outstanding introduction to deep learning and visual recognition\n• Luiz Velho's Fundamentals and Trends in Vision and Image Processing class at IMPA (Spring 2021)\n• Andrew Owens' EECS 504: Foundations of Computer Vision class at the University of Michigan (Winter 2020)\n\nIf you would like your course listed here, please contact me.",
      "author": null,
      "published_date": null,
      "meta_description": "",
      "word_count": 349,
      "scraping_method": "goose3"
    },
    {
      "url": "https://github.com/Mcmartelle/deml",
      "title": "Deml: The Directed Acyclic Graph Elevation Markup Language",
      "content": "Languages designed to represent all types of graph data structures, such as Graphviz's DOT Language and Mermaid JS's flowchart syntax, don't take advantage of the properties specific to DAGs (Directed Acyclic Graphs).\n\nDAGs act like rivers. Water doesn't flow upstream (tides and floods being exceptions). Sections of a river at the same elevation can't be the inputs or outputs of each other, like the nodes C, D, and E in the image below. Their input is B. C outputs to F, while D and E output to G.\n\nDEML's goal is to use this ordering as part of the file syntax to make it easier for humans to parse. In DEML we represent an elevation marker with on a new line. The order of elevation clusters is significant, but the order of nodes between two elevation markers is not significant.\n\nNodes are defined by the first word on a line. The defined node can point to its outputs with and to its inputs with . Inputs and outputs are separated by .\n\nDagrs is a library for running multiple tasks with dependencies defined in a DAG. In DEML, shell commands can be assigned to a node with . DEML files can be run via dag-rs with the comand .\n\nTo compare the difference in readability, here is the Dagrs YAML example in both YAML and DEML\n\nTo convert DEML files to Mermaid Diagram files (.mmd) use the command . The mermaid file can be used to generate an image at mermaid.live\n• Put my idea for an elevation based DAG representation into the wild\n\nI was thinking about how it's annoying in languages like C when function declaration order matters. Then I wondered if there could be a case when it would be a nice feature for declaration order to matter and I thought of DAGs.\n\nLicensed under either of\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.",
      "author": null,
      "published_date": null,
      "meta_description": "The Directed Acyclic Graph Elevation Markup Language - Mcmartelle/deml",
      "word_count": 344,
      "scraping_method": "goose3"
    },
    {
      "url": "https://velog.io/@0range1337/CTF-Google-CTF-2025-webz-Exploiting-zlibs-Huffman-Code-Table-English",
      "title": "Google CTF 2025 – webz : Exploiting zlib's Huffman Code Table",
      "content": "is a zlib exploitation challenge from Google CTF 2025. The Google-zlib implementation provided in the challenge is not upstream; it’s a version with an arbitrary patch applied. Whereas typical open‑source exploit challenges ship a patch that clearly introduces a vulnerability, ’s Google-zlib patch appears—at first glance—to be a normal optimization.\n\nIn practice, the vulnerability in this Google-zlib can be found quickly via fuzzing. However, in this write‑up we’ll derive the precise root cause through source analysis.\n\nThe Google-zlib codebase isn’t large, but it is quite tricky. Because it implements compression algorithms, manipulates data at the bit level, and contains optimizations that sacrifice readability, analysis can be difficult.\n\nFirst, let’s look at the provided . It’s simply a wrapper around Google-zlib. It receives raw Deflate-compressed data from the user and decompresses it using Google-zlib’s . Therefore, we must identify vulnerabilities in the code that implements : , , and .\n\nThe Google-zlib source shipped with the challenge contains a . From that patch we can see the code has been modified as above.\n\nThe patch removes some checks in and greatly increases the values of and . From the comments, the patch increases the sizes of Huffman tables and removes related checks to achieve a memory/speed tradeoff optimization. At this point we don’t yet know exactly what issues this introduces, but it’s clear the vulnerability will be related to Huffman tables and Huffman coding.\n\nBefore analyzing the code, let’s review the Deflate compression algorithm. Deflate uses Huffman coding and the LZ77 algorithm to compress data.\n\nThe principle of the LZ77 algorithm is very simple: repeated data is replaced by (length, distance) pairs.\n\nThe length is how many bytes to copy, and the distance is how far back from the current position the source data lies.\n\nHuffman coding is a bit more involved. The basic idea is to replace original data with compressed bit sequences. While the minimum unit of data is typically a byte (8 bits), replacing values with shorter bit sequences reduces size.\n\nIn this example there are only two symbols, A and B, which can be encoded with 1-bit Huffman codes (0 and 1). If there are more than two symbols, you obviously cannot compress them all with 1-bit codes.\n\nA Huffman code cannot be the prefix of another Huffman code. For example, if 111 is a code, then 11 cannot be a code; since codes have variable length, a prefix collision like 1110 would be ambiguous—unclear whether it’s 111 + 0 or 11 + 10.\n\nAlso, the minimum and maximum code lengths vary depending on the number of distinct data values. Huffman coding assigns shorter codes (e.g., 2 bits) to high-frequency values (A, B, C) and longer codes (e.g., 3 bits) to low-frequency values (D, E) to compress effectively.\n\nAdditionally, consider this: if Deflate generates efficient Huffman codes tailored to the input, then the decoder needs the corresponding Huffman table to decode. Therefore, Deflate uses either fixed Huffman tables or dynamic Huffman tables depending on the situation.\n\nLet’s elaborate on “including the Huffman table in the final compressed data.” In the standard implementation, the Huffman table can be represented using only code lengths.\n\nRather than storing the entire codes as above, you can store just the code lengths:\n\nSince actual Huffman codes have lengths in the range 3–15 bits, storing only lengths reduces the size of the embedded Huffman tables.\n\nThis works for a simple reason. A Huffman table is an array indexed by the original symbol. Assign the first 2-bit code 00 to A; then B gets 01, C gets 10, and so on. Using only lengths and order, all codes are recoverable. In other words, if Deflate assigns codes in order, Inflate can reconstruct them from just the lengths.\n\nis a virtual finite-state machine. It treats the compressed data stream ( ) as opcodes and executes like a VM. Since sets , it transitions to , and then hits the .\n\nBefore the main loop, let’s note some macros and variables used by Inflate. Members of the structure don’t benefit from register optimization, so these macros copy them into locals for faster operations.\n\nUnlike byte-oriented data, compressed data is processed at bit granularity because of packing and Huffman coding. The Inflate implementation uses macros like these to fill a bit buffer ( ) and manipulate it bitwise.\n\nThe basic logic is: use to pull bits from ( ) into ( ), decreasing ( ) accordingly. Then extract as many bits as needed with , and drop consumed bits with .\n\nUsing this bit-level handling, the code decodes the compressed data and appends the decoded bytes to ( ), decreasing ( ) by the number of bytes written.\n\nBack in , compressed data is processed in blocks, starting at . After ensuring at least 3 bits in the buffer ( ), it reads 1 bit with to set , which indicates whether this block is the last one. It then drops that bit and uses the next two bits to select the block type.\n\nBlocks have the following forms:\n\nThe compressed stream consists of one or more blocks, and decodes each according to the code above.\n\nLet’s look at how a dynamic Huffman table is built. As noted earlier, a dynamic codes block includes a Code Huffman table, and compressed Literal/Length and Distance tables. The code above reads the lengths for those three tables.\n\nThe Literal/Length table contains codes for literal bytes (A, B, …) and for LZ77 lengths; the Distance table holds codes for LZ77 distances. Using these two tables, the decoder performs Huffman and LZ77 decoding. So what is the Code Huffman table? The Literal/Length and Distance tables are stored compressed in the stream—again via Huffman coding. The Code Huffman table is the dynamic Huffman table used to decode the Huffman tables (lengths) themselves.\n\nFirst, we read the Code Huffman table lengths. We loop times and read 3 bits each time into . These 3-bit values are code lengths—the Huffman table is represented by lengths, not the full bit patterns, as discussed earlier. Thus, records the Code Huffman table’s code lengths in the permutation.\n\nHere, reduces the size of the encoded Code Huffman table. The Code table decodes original values 0–18. Storing lengths for all 19 values would be inefficient.\n\nTypically, codes are used more frequently in the same order as . If we stored lengths in plain 0–18 order, we’d need to write zeros for many unused values (e.g., 0–15) before the frequently used 16, 17, 18. By ordering them as above, we can store just the lengths for the frequently used codes and leave the rest implicit. The code reflects this: it reads lengths, and sets the remaining entries to zero.\n\nWe then set to point into , and call to build the Huffman table. The resulting table is written at ( ). We’ll cover shortly. For now, note the parameters: (build the Code table), (length array), (number of symbols, 0–18), (output pointer for the constructed table), (table index bit width—initially 7, but may be adjusted by ), and (a temporary array for sorting).\n\nOnce the Code table is built, we decode the compressed lengths of the Literal/Length and Distance tables. We read bits and use the Code table to decode entries, retrieving a struct from the table.\n\nValues 0–18 decoded via the Code table are not literal decoded bytes. Based on the code, they behave as follows:\n\nHere “original value” refers to the value decoded by Huffman coding, not necessarily the final decompressed byte. Some values (0–15) correspond to actual lengths, others (16–18) are special symbols.\n\nWe’ll explain this struct in the Huffman table construction section. Depending on its members, various actions occur to ultimately decode each value.\n\nAs before, we call to build the final and tables for Literal/Length and Distance respectively.\n\nWe now enter the decoding process. Again, we fetch a from the table and take actions based on its fields to decode the original value.\n\nA quick check shows that when , we switch to and append (the literal byte) to ( ). Also, is the number of bits consumed to decode that symbol; i.e., it’s the code length, and the decoder uses to consume bits. This is standard Huffman decoding. But there are other decoding forms depending on —we’ll explain this in the table construction section.\n\nBack to the code snippet above. If and are large enough, calls for high-speed decoding. The in-function Huffman decoding is slower because it transitions through VM-like states; operates with full buffers and fewer checks. Therefore, requires sufficiently large input/output buffers to be safe.\n• : pointer to the number of index bits for the table (may be adjusted)\n\nNow, the struct. The Huffman table is an array of structs; determines how to decode, is the code length, and holds the value. As the comment indicates, these fields can play different roles depending on .\n\nLet’s step through table construction. First we count, for each code length, how many symbols use that length.\n\nWe then determine the minimum and maximum code lengths from and set , the table’s index bit width. initially comes from the argument but may be adjusted based on min/max. That’s why is a pointer: any adjustments made in must also be visible to the caller.\n\nThe Huffman table is a simple 1D array, indexed by bits: table[huffman_code] = decoded_value (actually a code struct to decode it) . Thus, is really the number of index bits—i.e., the size of the primary table. If , the table has entries up to .\n\nIf , set to avoid wasting space. If , set ; otherwise you couldn’t store any codes at all.\n\nBut if , how do we store codes longer than ? Using multi-level tables. As we’ve seen, the field can indicate a second-level lookup. For example, suppose there are ten 8‑bit codes and one 9‑bit code. You don’t want to double the table size (from 256 to 512 entries) just for one symbol. So the primary table has 256 entries; all 8‑bit codes and the prefixes of any longer codes are stored there. For longer codes, entries in the primary table point to sub-tables that hold the remaining bits.\n\nWe’ll see the exact mechanics below.\n\nOnce is decided, we build the array to sort symbols by code length and symbol order into . The array is needed to reconstruct the full Huffman codes from the lengths.\n\nTo reconstruct codes from lengths, group symbols with the same length and assign codes in order:\n\nis the array that encodes this ordering; the build loop will walk to assign codes.\n\nDepending on , we set , , and . These support the Base/Extra decoding mode described below.\n\nDynamic Huffman table lengths in the stream are usually stored by position (index), since that index is the original value—e.g., index 65 corresponds to ‘A’. This is efficient for literals (0–255). But what about LZ77 length/distance values? Deflate specifies length range 3–258 and distance range 1–32,768, making a direct per‑value table impractical. So lengths and distances use Base/Extra coding.\n\nindicates where Base/Extra decoding begins. For Literal/Length, 0–255 are literals and 256 is End of Block; from 257 upward (LZ77 lengths), Base/Extra applies—so . The Code table doesn’t use Base/Extra at all, so (greater than the largest code index). Distance uses Base/Extra for all symbols, so .\n\nand select the arrays used for Base/Extra depending on whether we’re building the length or distance table.\n\nThis is the main construction loop. We iterate through , creating a entry for each symbol. If , it’s a normal entry: , . As we saw in , decoding such an entry emits .\n\nIf , we create an entry using the Base/Extra scheme: holds the number of extra bits, holds the base.\n\nTo understand Base/Extra, look at the length-decoding routine in . First, (the base). Then, based on , if it’s not a literal/end/invalid, we go to length decoding.\n\nextracts the number of extra bits. We then read that many bits and add them to the base to get the final length.\n\nFor example, suppose we want to decode length 20. Its code length entry would be at .\n\nThe length routine then computes . If the stream provides as the extra bits, we decode .\n\nThe key idea of Base/Extra: values like 20 (and the range ) are all represented by the same Huffman symbol (index 269); the exact value is determined by reading the extra bits. The table groups ranges by base and uses extra bits to resolve within the range.\n\nAfter creating a entry, we write it into the table at multiple positions. is used for sub-tables (multi-level); it’s 0 in the primary table.\n\nThe loop writes into . Before explaining why, let’s note something important:\n\nThe compressed bits for (0b0110) match those for (0b110). Even though the code set is prefix-free when read left-to-right, Deflate uses a bitstream where the trailing bits act as prefixes due to LSB-first packing. To handle this, we reverse the bit order when building indices:\n\nSo the correct index order is 0,2,1,3,7 rather than 0,1,2,6,7.\n\nBack to the loop. holds the (bit-reversed) Huffman code in progress. We don’t just store at ; we fill out all positions differing only in the unused high bits of the primary table.\n\nis (table size), and is (or for sub-tables). So the effect is:\n\nWe’re enumerating the higher bits that are irrelevant to this code length. This allows constant-time decoding:\n\nWhen decoding (0b0100), we can immediately index with and decode ‘A’ without checking code lengths; then drop 2 bits and continue.\n\nThis implements the same idea: index with fixed and decode immediately. The loop plus achieve this optimization.\n\nMove to the next symbol and update the working code length.\n\nLet’s illustrate with the earlier example:\n\nCodes of length ≤2 fit in the primary table.\n\nWe set (the number of lower bits to ignore when indexing sub-tables) and advance to the end of the current table—this is where the sub-table will live.\n\nNow the sub-table is ready: points to it, and causes future indices to ignore the lower bits.\n\nOn subsequent iterations, entries for the longer codes are placed into the sub-table:\n\nNote , so the sub-table stores only the remaining bits.\n\nWe also write into the primary table an entry that points to the sub-table. The final multi-level table looks like:\n\nDecoding a 3‑bit code like ‘E’ works like this: first-level lookup at yields , so we consume 2 bits and jump to the sub-table ( ). Then we use the next bit (1) to index the sub-table, yielding ; we consume 1 bit and emit ‘E’.\n\nFinally, if the code set is incomplete, the remaining entry is filled with an invalid code marker, then is updated and the function returns.\n\nIt loops until the preconditions fail. At the start of each iteration, if fewer than 15 bits are available in , it preloads 16 bits. This reduces overhead in the inner loop.\n\nThe logic mirrors : look up a in . If it’s a literal, emit it and continue; if it’s a length, decode the length and then decode the distance next, preloading more bits first.\n\nDistance decoding follows. After that, the LZ77 copy routine (not shown here) copies bytes from the window; the code is messy because it optimizes for various cases.\n\nAfter the LZ77 copy, the code handles second-level table lookups and invalid codes.\n\nWe analyzed the principal parts of , the decoder for . Do you see the bug? Everything looks well designed.\n\nThere’s a subtle issue in the Huffman table construction. A Huffman table can be incomplete. For example, if is 8 and the maximum code length is 10, there will be no entries for length‑9 codes; i.e., some table entries remain unset. Are such NULL entries handled correctly during decoding?\n\nNo. As we’ve seen, incomplete entries should be filled with (invalid).\n\nAs a result, any NULL entries get treated as if they were structures with . Or, they may retain stale values from a previous block.\n\nTo achieve high speed, omits many checks; it can therefore cause memory corruption when encountering incomplete Huffman tables. Let’s explore how.\n\nThe first memory bug identified was an integer overflow, but it wasn’t exploitable. The second was a stream overflow, which we ultimately exploited. We’ll describe both.\n\nLet’s see what happens when a zero‑initialized table entry ( ) is used in decoding.\n\nIn the literal path, a with consumes zero bits and decodes a null byte. Since no bits are consumed, would loop forever decoding that same entry.\n\nHowever, the loop is bounded by , so no overflow occurs here.\n\nWhat about the length path? Because of the checks, the code falls into the second-level table lookup. With zero bits consumed, it indexes the 0th entry again.\n\nDistance decoding behaves similarly, but worse: the second-level lookup jumps back into the distance decode path. The “0th table entry” behavior is dangerous, because the second-level lookup is designed to read a sub-table (with smaller ), but instead it’s indexing the primary table. keeps at least 16 bits in and assumes no codes exceed 15 bits, so it omits checks. The erroneous “0th entry” lookup breaks this assumption.\n\nAs noted, the distance path jumps back into distance decoding after the second-level lookup, so the primary table (not sub-table) is indexed next, consuming too many bits. Ultimately, the counter underflows—an integer overflow.\n\nAt the end of , the code adjusts / by the number of unused bits. Thus, the integer overflow in corrupts and , affecting subsequent decoding.\n\nWe reproduce the case described above. The Distance table has with a maximum code length of 12, so there are unfilled entries with . By crafting the stream to force a multilevel lookup, we can trigger the vulnerability.\n\nThese are key. The first is a valid 15‑bit length code. The second is an invalid 12‑bit distance code.\n\nThe valid distance‑5 code is . Instead, forces a second‑level lookup that reads an uninitialized entry.\n\nHowever, this particular memory corruption was not exploitable. If we first decompress dummy data in block one, and then in a second block trigger the bug with a Literal/Length table that only contains EOB, we can corrupt / without crashing. But since these are input stream variables (not output buffer variables), we couldn’t achieve an overflow or OOB write on the decompressed output buffer.\n\nSo what should we target to exploit uninitialized table entries? The most promising avenue in zlib is to abuse the copy routines. The stored-block copy and the LZ77 copy are powerful overwrite primitives—if we can disable the checks that constrain them.\n\nIn other words, we need to corrupt , not . Let’s inspect ’s LZ77 decode.\n\nThe LZ77 decode in has almost no checks. The only guard is , behind .\n\nHow can it still be safe?\n\nThe maximum copy length (LZ77 length) is → 258. Earlier we noted that is entered only when , and the loop exits as soon as that’s no longer true. Thus, can safely omit length checks because it guarantees there are at least 258 bytes of space.\n\nIn , tables are written into , which is not cleared between blocks. The tables’ boundaries aren’t fixed either; advances dynamically, so different blocks can lay out different tables at different offsets.\n\nTherefore, stale entries from a previous block can persist in uninitialized slots of later tables—even of different types.\n\nIf a Distance-table entry from a previous block remains in an uninitialized slot of the subsequent Literal/Length table, we’re in trouble. Deflate limits lengths to 258, but distances can be much larger. If a stale Distance entry is misinterpreted as a Length entry in , its length can exceed 258, breaking the invariant that made safe.\n\nUltimately, when the LZ77 decode interprets a stale Distance entry as a Length, suffers an integer overflow. Unlike , reflects the remaining size of the output buffer, so this immediately leads to a buffer overflow.\n\nThe decompressed bytes are written into the global in . To actually corrupt memory, we must write more than bytes and overflow into the following , overwriting its fields.\n\nThe PoC uses six blocks. Let’s walk through them.\n\nBlock 1: writes dummy bytes to the output buffer so that later copies with large distances won’t misbehave.\n\nBlock 2: prepares the ground for the bug by filling the table area with Distance symbols. Those entries will persist in uninitialized slots later.\n\nBlock 3: creates an incomplete Huffman table and references the uninitialized entry to perform LZ77 decoding. This actually triggers the bug and causes integer overflow in . From this point on, boundary checks for the decompression buffer malfunction, enabling buffer overflow.\n\nAs noted, to overwrite , we must first fill the 8192‑byte output buffer. We use LZ77 and a stored block to push ~8120 bytes of padding.\n\nThe final block performs the actual overflow to overwrite , letting us set its members arbitrarily.\n\nFirst, by partially overwriting the pointer in or setting it arbitrarily, we get arbitrary read.\n\nAdditionally, since and call / , control‑flow hijacking is easy.\n\nThis is the final exploit. It successfully retrieves the flag.",
      "author": null,
      "published_date": null,
      "meta_description": "webz is a zlib exploitation challenge from Google CTF 2025. The Google-zlib implementation provided in the challenge is not upstream; it’s a version w",
      "word_count": 3529,
      "scraping_method": "goose3"
    },
    {
      "url": "https://philipfabianek.com/posts/cuda-ptx-introduction/",
      "title": "A Gentle Introduction to CUDA PTX",
      "content": "Introduction\nAs a CUDA developer, you might not interact with Parallel Thread Execution (PTX) every day, but it is the fundamental layer between your CUDA code and the hardware. Understanding it is essential for deep performance analysis and for accessing the latest hardware features, sometimes long before they are exposed in C++. For example, the wgmma↗ instructions, which perform warpgroup-level matrix operations and are used in some of the most performant GEMM kernels, are available only through PTX instructions.\nThis post serves as a gentle introduction to PTX and its place in the CUDA ecosystem. We will set up a simple playground environment and walk through a complete kernel in PTX. My goal is not only to give you the foundation to use PTX but to also share my mental model of how PTX fits into the CUDA landscape.\nPTX and the CUDA ecosystem\nEvery processor has an instruction set architecture (ISA), which is the specific set of commands the hardware can execute. NVIDIA GPUs are no different, their native, hardware-specific ISA is called SASS (streaming assembly). However, the SASS for one GPU generation can be incompatible with another, meaning a program compiled for an older GPU might not run on a newer one. In other words, there is no forward compatibility. This is one of the problems that PTX solves. PTX is an ISA for a virtual machine: an abstract GPU that represents the common features of all NVIDIA hardware. When you compile your CUDA code, a tool called ptxas↗ translates your hardware-agnostic PTX into the specific SASS for your target GPU. This two-stage design is a common pattern in modern compilers. The LLVM (Low Level Virtual Machine) project↗ is a well-known example of this architecture.\nWe can utilize the PTX forward compatibility by using just-in-time (JIT) compilation. You can choose to embed the PTX code directly into your final executable (I will cover this later in the post). When your application runs on a new GPU for which it doesn’t have pre-compiled SASS, the NVIDIA driver on the system acts as a JIT compiler. It’s important to note that this provides forward compatibility only. For example, PTX generated for compute_70 can run on any future GPU (8.x, 9.x, etc.), but it cannot be run on an older 6.x GPU. This is different from the SASS binary itself, which has much stricter rules and is generally only compatible with GPUs of the same major version number. Tools like Triton↗ rely on this. They generate PTX and leave the final, hardware-specific compilation to the driver. By default, nvcc includes both PTX and SASS in your executable, giving you both immediate performance and future compatibility.\nThe PTX playground\nThe best way to learn PTX is to see it in action. To do that, we need a simple environment that lets us write PTX code and see it run. I have created precisely that in this repository↗.\nThe repository contains two main files:\nadd_kernel.ptx↗: This is the text file that contains the raw PTX instructions for our kernel.\nmain.cu↗: This is a C++ program that runs on the host to load, run, and verify the result of our PTX kernel.\nOur C++ host code needs a way to load and run the .ptx file at runtime. This requires the CUDA Driver API↗, a lower-level interface than the more common Runtime API. This is why the code in main.cu uses functions like cuLaunchKernel instead of the familiar <<<...>>> syntax. While a full explanation of the Driver API is outside the scope of this post, the main.cu file handles all the necessary setup for you, allowing us to focus entirely on the PTX code.\nTo compile the program, use the following command:\nnvcc main.cu -o ptx_runner -lcuda\n(notice the -lcuda which ensures the CUDA Driver API library is included)\nThen, run the executable:\n./ptx_runner\nIf everything works correctly, you should see a success message as the output.\nKernel walkthrough\nNow that we have a working environment, we can dive into the PTX kernel itself. Our goal is to implement a classic kernel that performs an element-wise addition of two vectors, a and b, and stores the result in a third vector, c. Before we look at the assembly, let’s first look at the equivalent logic in standard CUDA C++:\n__global__ void add_kernel(const float* a, const float* b, float* c, int n) {\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\nif (idx < n) {\nc[idx] = a[idx] + b[idx];\n}\n}\nHere is the complete add_kernel.ptx file that accomplishes this exact same task. Don’t worry if you don’t understand what is happening in this file, by the end of this post that will change.\n.version 7.0\n.target sm_70\n.address_size 64\n.visible .entry add_kernel (\n// Kernel parameters passed from the host\n.param .u64 in_a,\n.param .u64 in_b,\n.param .u64 out_c,\n.param .u32 n\n)\n{\n// Setup registers\n.reg .b64\n%rd<8>;\n// %rd for 64-bit addresses and pointers\n.reg .b32\n%r<2>;\n// %r for signed 32-bit integers\n.reg .u32\n%u<5>;\n// %u for unsigned 32-bit integers\n.reg .f32\n%f<4>;\n// %f for 32-bit floating point values\n.reg .pred\n%p<2>;\n// %p for predicates (booleans)\n// Load kernel parameters into registers\nld.param.u64\n%rd1, [in_a];\n// %rd1 = base pointer for 'a'\nld.param.u64\n%rd2, [in_b];\n// %rd2 = base pointer for 'b'\nld.param.u64\n%rd3, [out_c];\n// %rd3 = base pointer for 'c'\nld.param.u32\n%u1, [n];\n// %u1 = size 'n'\n// Move special register values to general registers to use them\nmov.u32\n%u2, %ctaid.x;\n// %u2 = blockIdx.x\nmov.u32\n%u3, %ntid.x;\n// %u3 = blockDim.x\nmov.u32\n%u4, %tid.x;\n// %u4 = threadIdx.x\n// Calculate the global thread ID by\n// idx = blockIdx.x * blockDim.x + threadIdx.x\nmad.lo.s32\n%r1, %u2, %u3, %u4;\n// %r1 = idx\n// Terminate if idx >= n\nsetp.ge.s32\n%p1, %r1, %u1;\n// Set predicate %p1 if idx >= n\n@%p1 bra\nDONE;\n// Branch to DONE if %p1 is true\n// Compute how many bytes to offset for the current index,\n// use 4 because we're dealing with 32-bit (4-byte) floats\nmul.wide.s32\n%rd4, %r1, 4;\n// %rd4 = byte offset\n// Calculate addresses for a[idx], b[idx], and c[idx]\n// by using the base address + offset\nadd.s64\n%rd5, %rd1, %rd4;\n// %rd5 = address of a[idx]\nadd.s64\n%rd6, %rd2, %rd4;\n// %rd6 = address of b[idx]\nadd.s64\n%rd7, %rd3, %rd4;\n// %rd7 = address of c[idx]\n// Load vector values at the target position\nld.global.f32\n%f1, [%rd5];\n// %f1 = a[idx]\nld.global.f32\n%f2, [%rd6];\n// %f2 = b[idx]\n// Perform the addition and store the result\nadd.f32\n%f3, %f1, %f2;\n// %f3 = a[idx] + b[idx]\nst.global.f32\n[%rd7], %f3;\n// c[idx] = %f3\nDONE:\nret;\n}\n(You can also compare this with how nvcc compiles the C++ version of our kernel into PTX on the Compiler Explorer (Godbolt)↗)\nPreamble and signature\nThe first thing to notice is that the .ptx file contains only the code for our GPU kernel. The host code you saw in main.cu is compiled separately by nvcc and then linked together with the GPU binary at the end.\nLet’s look at the first part of the file, which defines the kernel’s properties and the arguments it expects:\n.version 7.0\n.target sm_70\n.address_size 64\n.visible .entry add_kernel (\n// Kernel parameters passed from the host\n.param .u64 in_a,\n.param .u64 in_b,\n.param .u64 out_c,\n.param .u32 n\n)\nThe file begins with a preamble that sets up the compilation environment using several directives. A directive is a command for the PTX assembler that starts with a dot (.). The .version directive specifies the PTX language version we are using. The .target directive declares the minimum GPU architecture required, which in our case is sm_70. Thanks to PTX’s forward compatibility, this means our kernel will run correctly on any NVIDIA GPU from the Volta generation (sm_70) or newer. Finally, .address_size confirms that we are working with 64-bit pointers.\nFollowing this is the kernel’s signature. The .visible and .entry keywords declare that this is a kernel that can be seen and launched by the host. The name of our kernel, add_kernel, is what we reference from our C++ code. When nvcc compiles a C++ kernel, it normally generates a “mangled” name that encodes the function’s arguments, resulting in a long, unreadable string like _Z10add_kernelPKfS0_Pfi. Since we are writing the PTX by hand, we can choose a simple name for clarity.\nInside the parentheses are the kernel’s parameters, declared with the .param directive. These must match the order and type of the arguments we pass from the host. We use .u64 for the pointers, as they are 64-bit addresses, and .u32 for the size of the vectors.\nRegister declarations\nAfter the kernel signature, we define the virtual registers that will serve as our working variables.\nRegisters are a small amount of extremely fast, on-chip storage. For a processor to perform any computation, such as adding two numbers, the data must first be loaded from main memory into these registers. After that, the processor does the work and stores the results from the registers back to memory.\nOur kernel declares a pool of registers using the .reg directive:\n// Setup registers\n.reg .b64\n%rd<8>;\n// %rd for 64-bit addresses and pointers\n.reg .b32\n%r<2>;\n// %r for signed 32-bit integers\n.reg .u32\n%u<5>;\n// %u for unsigned 32-bit integers\n.reg .f32\n%f<4>;\n// %f for 32-bit floating point values\n.reg .pred\n%p<2>;\n// %p for predicates (booleans)\nThe prefixes like %rd, %r, and %u are not rules but a common convention that makes the code much easier to read by signaling the intended use of a register.\nThe <N> syntax declares a count of N registers, which are indexed from 0 to N-1. For example, .reg .pred %p<2> declares two predicate registers: %p0 and %p1.\nYou might notice that we have declared more registers than we actually use, and we don’t always start our indexing at 0. This is perfectly fine. The final executable binary will only allocate physical hardware registers for the virtual registers that are part of the program’s logic.\nData movement instructions\nAfter the register declarations, we get to the main body of our kernel, which consists of a sequence of instructions. In PTX, an instruction is an operation that the GPU will execute, like loading data or performing addition. They all follow a similar pattern:\nopcode{.modifier} destination_operand, source_operand_A, source_operand_B, ...;\nThe opcode is the name of the operation (e.g. ld for load). This is often followed by one or more .modifiers that specify the data type and other options. The destination operand is almost always a register where the result will be stored, and it is followed by one or more source operands. Keep in mind the output is always before the input in these instructions.\nLet’s start by looking at the core data movement instructions in our kernel: ld, st, and mov.\nld↗ (Load): Reading from memory\nThe ld instruction is used to load data from a memory location into a register. If you look at the official PTX documentation, the full syntax for ld is quite complex, with many optional modifiers for controlling caching, memory synchronization, and more:\nld{.weak}{.ss}{...}.type d, [a]{...};\nWhile this looks intimidating, the vast majority of these modifiers are for advanced usage. For our purposes, we can simplify this down to a form that we will actually be using:\nld{.ss}.type d, [a];\nHere’s how you can read this:\nld: The opcode.\n{.ss}: The curly braces {} mean this part is an optional modifier. The .ss stands for “state space” and is where you would put .global, .shared, or .param to tell the instruction where to load from.\n.type: This is a mandatory modifier where you specify the data type, like .f32 or .u64.\nd: The destination operand, which must be a register.\n[a]: The source operand. The square brackets [] mean this is a memory address. The a inside is a register that holds the address. This syntax means “dereference the pointer in register a.”\nNow, let’s look at the ld instructions from our kernel. The first ones load the kernel parameters passed from the host into our general-purpose registers:\n// Load kernel parameters into registers\nld.param.u64\n%rd1, [in_a];\n// %rd1 = base pointer for 'a'\nld.param.u64\n%rd2, [in_b];\n// %rd2 = base pointer for 'b'\nld.param.u64\n%rd3, [out_c];\n// %rd3 = base pointer for 'c'\nld.param.u32\n%u1, [n];\n// %u1 = size 'n'\nHere you can see the pattern in action. For the first instruction, we are loading from the .param state space, the data type is a .u64, the destination is the register %rd1, and the source address is the one associated with the parameter in_a.\nst↗ (Store): Writing to memory\nThe st instruction is the mirror image of ld. It is used to store data from a register to a memory location. Its simplified syntax is very similar, but the order of the operands is reversed:\nst{.ss}.type [a], b;\nHere, the memory address [a] is the destination, and the register b is the source. Our kernel uses st at the very end to write the final result back to global memory:\n// Perform the addition and store the result\nadd.f32\n%f3, %f1, %f2;\n// %f3 = a[idx] + b[idx]\nst.global.f32\n[%rd7], %f3;\n// c[idx] = %f3\nThis instruction stores the 32-bit float (.f32) value from the source register %f3 into the destination address [%rd7] in global (.global) memory.\nmov↗ (Move): Working with registers\nFinally, the mov instruction is used to move data between registers or to get the address of a variable. It has a much simpler syntax:\nmov.type d, a;\nOur kernel uses mov to copy the values from special registers (see below) into the general-purpose registers that our other instructions can use.\n// Move special register values to general registers to use them\nmov.u32\n%u2, %ctaid.x;\n// %u2 = blockIdx.x\nmov.u32\n%u3, %ntid.x;\n// %u3 = blockDim.x\nmov.u32\n%u4, %tid.x;\n// %u4 = threadIdx.x\nSpecial registers↗ (in this example %ctaid.x, %ntid.x, and %tid.x) are predefined, read-only registers that the GPU uses to give our kernel information about its environment. They live in a special state space called .sreg. As a programmer, you don’t declare them, you simply read from them to get essential values like the thread’s ID or the dimensions of the grid.\nIn PTX, these special registers have direct analogs to the built-in variables you would use in a CUDA C++ kernel:\nPTX Special RegisterCUDA C++ Built-in Variable%tid.xthreadIdx.x%ntid.xblockDim.x%ctaid.xblockIdx.x%nctaid.xgridDim.x\nMost arithmetic instructions cannot use these special registers directly. This is why we first use mov to copy these values into our general-purpose registers (%u2, %u3, and %u4), where we can then use them for our address calculations.\nComputation and control flow\nNow that we have loaded our parameters and identified our thread, we can move on to the core logic of the kernel. This involves calculating the thread’s index, checking if it’s within the bounds of our vectors, and finally, performing the addition.\nThe first step is to calculate the global index for the current thread, which corresponds to the following C++ line:\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\nPTX has a dedicated instruction for this common pattern:\n// Calculate the global thread ID by\n// idx = blockIdx.x * blockDim.x + threadIdx.x\nmad.lo.s32\n%r1, %u2, %u3, %u4;\n// %r1 = idx\nThe mad↗ instruction performs a multiply-add operation. It multiplies the first two source operands (%u2 and %u3) and adds the third (%u4), storing the result in the destination (%r1). When two 32-bit numbers are multiplied, the full result can be up to 64 bits long. Since we know our thread index will not exceed the 32-bit limit, the .lo modifier tells the instruction to take only the lower 32 bits of the full multiplication result before performing the addition. The .s32 modifier tells the instruction to treat the operands as 32-bit signed integers.\nNext, we need to implement the boundary check from our C++ code:\nif (idx < n)\nIn assembly, this is typically done by checking for the opposite condition and skipping the work if it’s met:\n// Terminate if idx >= n\nsetp.ge.s32\n%p1, %r1, %u1;\n// Set predicate %p1 if idx >= n\n@%p1 bra\nDONE;\n// Branch to DONE if %p1 is true\nFirst, the setp↗ (set predicate) instruction performs a comparison. Here, it compares %r1 (the index) with %u1 (the vector size). The .ge modifier means “greater than or equal to.” If idx >= n, the 1-bit predicate register %p1 is set to true.\nThe next instruction, bra↗ (branch), is a jump to a label. The @%p1 at the beginning is a predicate guard, meaning the instruction only executes if %p1 is true. Together, these two lines mean: “If idx is greater than or equal to n, jump to the DONE label at the end of the kernel.” This is how we skip the main logic for out-of-bounds threads.\nBefore we can load our data, we need to calculate the exact memory address for a[idx], b[idx], and c[idx]. This requires converting our logical index into a physical byte offset:\n// Compute how many bytes to offset for the current index,\n// use 4 because we're dealing with 32-bit (4-byte) floats\nmul.wide.s32\n%rd4, %r1, 4;\n// %rd4 = byte offset\nThis instruction multiplies our index in %r1 by 4, because each float takes up 4 bytes in memory. We use mul.wide.s32↗, which takes two 32-bit inputs but produces a full 64-bit result, preventing any potential overflow. The result, our byte offset, is stored in %rd4.\nThen, we simply use the integer add↗ instruction to add this 64-bit offset to our 64-bit base pointers (%rd1, %rd2, and %rd3) to get the final addresses for the elements we need to access:\n// Calculate addresses for a[idx], b[idx], and c[idx]\n// by using the base address + offset\nadd.s64\n%rd5, %rd1, %rd4;\n// %rd5 = address of a[idx]\nadd.s64\n%rd6, %rd2, %rd4;\n// %rd6 = address of b[idx]\nadd.s64\n%rd7, %rd3, %rd4;\n// %rd7 = address of c[idx]\nFinally, with all our addresses calculated and data loaded, we can perform the core task of the kernel:\n// Load vector values at the target position\nld.global.f32\n%f1, [%rd5];\n// %f1 = a[idx]\nld.global.f32\n%f2, [%rd6];\n// %f2 = b[idx]\n// Perform the addition and store the result\nadd.f32\n%f3, %f1, %f2;\n// %f3 = a[idx] + b[idx]\nThis is the simplest part. The float add.f32↗ instruction takes the two float values we loaded into registers %f1 and %f2 and adds them, storing the final result in %f3.\nThe last two lines of our kernel handle the exit path:\nDONE:\nret;\nDONE is a label. It is not an instruction, but a bookmark in the code. Its only purpose is to serve as a target for the bra (branch) instruction we saw earlier. Threads that are out-of-bounds will jump directly to this line.\nThe ret↗ instruction is the final command. It returns from the kernel, ending the execution for the current thread. All threads, whether they performed the addition or branched to DONE, will eventually execute this instruction.\nThis completes the walkthrough of all the instructions in our kernel.\nConclusion\nCongratulations! If you reached this point, you should now have a solid foundation and a practical mental model of how PTX works, from the virtual machine concepts down to the individual instructions.\nWhile the workflow we used (using a full .ptx file as a kernel) is the best way to learn the fundamentals, in practice, the most common way you will use PTX is through inline assembly directly inside your CUDA __global__ functions. You can learn more about inline PTX here↗. This technique is used for specific optimizations, allowing you to inject a few specific PTX instructions to perform a task that C++ cannot express. This is exactly how you would use instructions like the wgmma operations we mentioned in the introduction.\nYou should now possess the knowledge to navigate the dense official PTX documentation↗. It is best to use it as a reference manual. When you need to know the exact syntax of a specific instruction or explore a new hardware feature, you can read the documentation and understand the instructions.\nAppendix A: Controlling the fatbin with nvcc\nTo have precise control over what gets embedded in your final executable (called fatbin), you need to understand the terminology used by nvcc. The target for the PTX virtual machine is called a compute capability, specified with flags like compute_70. The target for the final, hardware-specific SASS is the streaming multiprocessor (SM) architecture, specified with sm_70.\nThe -arch flag used with sm_XX (e.g. -arch sm_86) results in both PTX and SASS for that specific architecture being included. If you use -arch compute_86, then only PTX will be included. If you want to specify exactly what goes into the binary, you can use the more powerful -gencode flag. For instance, you could ship an application with pre-compiled SASS (and no PTX) for both Ampere (sm_86) and Hopper (sm_90) by using this command:\nnvcc program.cu \\\n-gencode arch=compute_86,code=sm_86 \\\n-gencode arch=compute_90,code=sm_90\nBy including multiple SASS files you can prevent JIT compilation which can for example eliminate startup latency.\nTo see what’s inside your final executable, you can use the cuobjdump↗ utility. Running cuobjdump -ptx <executable> will extract and print the embedded PTX, while cuobjdump -sass <executable> will disassemble and show you the native SASS machine code for each architecture it contains. While you can inspect these to make performance adjustments, more commonly you generate a report using Nsight Compute (NCU)↗ and inspect the PTX and SASS there. The advantage is that NCU can link specific lines from your kernel code to their compiled PTX and SASS representations, making performance analysis easier. For interactive exploration, a fantastic online tool is the previously mentioned Compiler Explorer (Godbolt)↗, which can show you the PTX or SASS generated by nvcc in real-time as you type CUDA C++ code.\nAppendix B: The full compilation pipeline and NVVM IR\nAs we’ve seen, CUDA C++ is compiled to PTX, and this PTX is then assembled into SASS machine code for a specific GPU. However, there is one more step in this process: the NVVM IR↗. This is another internal representation that sits between your C++ code and the final PTX. NVVM IR is NVIDIA’s specialized version of the popular LLVM IR, and it’s converted to PTX using an NVIDIA library called libnvvm.\nSo what is the purpose of this extra layer? By building their compiler on LLVM, NVIDIA makes it much easier for other programming languages to target their GPUs. If someone wants to write a compiler for a new language that runs on NVIDIA hardware, they don’t need to become experts in generating PTX. Instead, they can target the much higher-level NVVM IR, allowing them to utilize the entire mature LLVM infrastructure. This is exactly how tools like Triton↗ and the Rust GPU compiler↗ work.\nThis translation from CUDA C++ to NVVM IR is handled by a Clang-based C++ frontend used by nvcc. However, nvcc treats this NVVM IR as a purely internal representation. This makes PTX the first stage in the compilation pipeline where we can reliably inspect the output, making it an essential tool for performance analysis.",
      "author": "Philip Fabianek",
      "published_date": null,
      "meta_description": "A gentle introduction to the PTX ISA. This post explains the entire CUDA compilation pipeline from C++ to SASS, provides a PTX playground and fully explains a hand-written PTX kernel.",
      "word_count": 3877,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/",
      "title": "Comprehension debt: A ticking time bomb of LLM-generated code",
      "content": "An effect that’s being more and more widely reported is the increase in time it’s taking developers to modify or fix code that was generated by Large Language Models.\nIf you’ve worked on legacy systems that were written by other people, perhaps decades ago, you’ll recognise this phenomenon. Before we can safely change code, we first need to understand it – understand what it does, and also oftentimes why it does it the way it does. In that sense, this is nothing new.\nWhat is new is the scale of the problem being created as lightning-speed code generators spew reams of unread code into millions of projects.\nTeams that care about quality will take the time to review and understand (and more often than not, rework) LLM-generated code before it makes it into the repo. This slows things down, to the extent that any time saved using the LLM coding assistant is often canceled out by the downstream effort.\nBut some teams have opted for a different approach. They’re the ones checking in code nobody’s read, and that’s only been cursorily tested – if it’s been tested at all. And, evidently, there’s a lot of them.\nWhen teams produce code faster than they can understand it, it creates what I’ve been calling “comprehension debt”. If the software gets used, then the odds are high that at some point that generated code will need to change. The “A.I.” boosters will say “We can just get the tool to do that”. And that might work maybe 70% of the time.\nBut those of us who’ve experimented a lot with using LLMs for code generation and modification know that there will be times when the tool just won’t be able to do it.\n“Doom loops”, when we go round and round in circles trying to get an LLM, or a bunch of different LLMs, to fix a problem that it just doesn’t seem to be able to, are an everyday experience using this technology. Anyone claiming it doesn’t happen to them has either been extremely lucky, or is fibbing.\nIt’s pretty much guaranteed that there will be many times when we have to edit the code ourselves. The “comprehension debt” is the extra time it’s going to take us to understand it first.\nAnd we’re sitting on a rapidly growing mountain of it.\nShare this:\nClick to share on X (Opens in new window)\nX\nClick to share on Facebook (Opens in new window)\nFacebook\nClick to share on LinkedIn (Opens in new window)\nLinkedIn\nClick to share on Reddit (Opens in new window)\nReddit\nClick to email a link to a friend (Opens in new window)\nEmail\nLike Loading...\nRelated\nAuthor: codemanship\nFounder of Codemanship Ltd and code craft coach and trainer\nView all posts by codemanship",
      "author": null,
      "published_date": null,
      "meta_description": "An effect that's being more and more widely reported is the increase in time it's taking developers to modify or fix code that was generated by Large Language Models. If you've worked on legacy systems that were written by other people, perhaps decades ago, you'll recognise this phenomenon. Before we can safely change code, we…",
      "word_count": 465,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://labs.leaningtech.com/blog/browserpod-annoucement",
      "title": "BrowserPod: In-browser full-stack environments for IDEs and Agents via WASM",
      "content": "X\nFacebook\nY Combinator\nLinkedIn\nReddit\nEmail\nWhatsApp\nWe’re excited to introduce BrowserPod a WebAssembly-based, in-browser container technology that runs full-stack development environments across multiple languages.\nLearn more\nJoin the Discord server\nBrowserPod is a generalised, more powerful alternative to WebContainers, with advanced networking capabilities and flexible multi-runtime support. Containers, called Pods, run completely client-side. At their core there is a flexible WebAssembly-based engine that can execute multiple programming languages.\nEach Pod can:\nRun multiple processes or services in parallel, with real concurrency powered by WebWorkers\nAccess a scalable block-based filesystem with privacy-preserving browser-local persistence.\nExpose virtualized HTTP / REST services to the internet via Portals\nPods are fast booting, since no provision of server-side resources is required. Moreover, multiple Pods can run in each browser tab, enabling complex deployments.\nBrowserPod is conceptually similar to WebContainers, but is designed from the ground up to be language-agnostic, to support inbound networking, and to be integrated within the Leaning Technologies ecosystem.\nBrowserPod will be released in late November, with an initial focus on Node.js environments and a well defined path to support additional stacks, with Python and Ruby as immediate priorities.\nFurther capabilities will become available later by integrating BrowserPod with CheerpX, our x86-to-WebAssembly virtualization engine. In particular, we plan to support React Native environments in 2026.\nWhat is BrowserPod for?\nBrowserPod is designed to run complete development environments in the browser, without installing local helper applications or dedicated server-side resources.\nA typical use case of BrowserPod, exemplified by this demo, would be a browser-based IDE that can run a preview server, for example via npm run dev. The preview server runs fully in the browser, with each update to files being reflected in the virtualized environment. As files are updated the normal Hot Module Replacement triggers, updating the preview.\nThe application preview is not just available in the same browser session, but is exposed to the internet via a Portal, a seamless solution to allow direct public access to any HTTP service running inside a Pod.\nPortals make it possible to achieve real cross-device testing of applications and even pre-release sharing of test URLs with external users, including early adopters, stakeholders and clients.\nBrowserPod’s initial focus will be on Node.js environments, which are supported via a complete build of Node.js, compiled to WebAssembly and virtualized in the browser. BrowserPod will support multiple versioned runtimes, with Node.js 22 being included in the first release.\nThis high-fidelity approach ensures that applications developed in BrowserPod containers will behave the same when migrated to production, making it possible to build real-world applications in Pods, not just prototypes.\nBrowserPod is a great fit for web-based IDEs, educational environments, interactive documentation websites and AI coding Agents.\nHow does BrowserPod work?\nBrowserPod builds on our years long experience in delivering high performance in-browser virtualization via WebVM. WebVM is powered by CheerpX, our x86-to-WebAssembly virtualization engine, and it has been the most popular tool we have released so far, with 15k+ stars on GitHub.\nTo make BrowserPod possible we have rearchitectured CheerpX to separate its two main components: the x86-to-WebAssembly JIT compiler engine, and the Linux system call emulation layer. This emulation layer, that we are calling CheerpOS internally, is now shared across CheerpX and BrowserPod and, down the line, it will become a foundation layer across all our products.\nCheerpOS is effectively a WebAssembly kernel that allows unmodified C/C++ code for Linux to run in the browser. In the context of BrowserPod it is used to provide a unified view of filesystem and access to networking across the multiple processes running in a Pod.\nOn top of this kernel layer, we compile the C++ source code of Node.js, with minimal changes, to a combination of WebAssembly and JavaScript. The use of JavaScript is specific to Node.js and provides the required shortcut to run JavaScript payloads natively in the browser itself, which is critical for high performance execution of Node.js environments.\nOther stacks, such as Python and Ruby on Rails, will instead run as pure WebAssembly applications on top of the CheerpOS kernel.\nLicensing\nBrowserPod will come with a generous free license with attribution, available for non-commercial users and technical evaluations.\nA transparent pay-as-you-go model will be available for any use and purpose, including companies working on AI codegen tools. Pricing will be announced at release time and it will be very affordable to maximize the adoption of this technology, with discounts available for educational and non-profit use.\nAn Enterprise license will also be available for self-hosting and commercial support.\nGeneral Availability\nThe initial release of BrowserPod will become generally available in late November - early December 2025, with support for Node.js 22.\nOver the course of the following year, we have planned several additional releases, including support for multiple Node.js versions, support for Python and Ruby on Rails, and eventually support for React Native environments.\nTo receive up-to-date information on BrowserPod, make sure to register on our website. We plan to extend an early adopter program to a selected subset of registered users and organizations.\nFor more information on all our products and technologies, please join our Discord. Most members of the development team are active there and ready to answer any question you might have. You can also follow us on X and LinkedIn for updates.\nConclusions\nBrowserPod is currently in the final stages of development, and we are thrilled to see what the community will build on top of this technology when it is released in late November.\nLeaning Technologies mission statement is “Run anything on the browser”, and BrowserPod is an important milestone along this journey. It will also not be the last and we have great ambitions for our ecosystem as we migrate to the unified CheerpOS foundational layer. Stay tuned!\nJoin us on Discord",
      "author": null,
      "published_date": null,
      "meta_description": "BrowserPod is a WebAssembly-based, client-side container technology that brings full-stack development environments to the browser. Pods run multiple processes with real concurrency, have a persistent block filesystem, and public HTTP endpoints via Portals. Launching with Node.js 22 and with short-term support for Python and Ruby planned, BrowserPod is perfect for web IDEs, interactive docs, and AI coding agents.\n",
      "word_count": 961,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://www.theguardian.com/lifeandstyle/2025/oct/01/artificial-intelligence-em-dashes-ai-stealing-my-livelihood",
      "title": "Not only am I losing my livelihood to AI – now it's stealing my em dashes too",
      "content": "‘Oh, em dash—what will we do without you?’ Illustration: Victoria Hart/Guardian DesignView image in fullscreen‘Oh, em dash—what will we do without you?’ Illustration: Victoria Hart/Guardian DesignMy petty gripe: not only am I losing my livelihood to AI – now it’s stealing my em dashes tooJames ShackellThe humble em dash is being used as a tell that something is written by a large language model. But it’s James Shackell’s favourite piece of punctuation, and he’s not ready to lose it\nRead more petty gripes\nMy editor’s email started off friendly enough, but then came the hammer blow: “We need you to remove all the em dashes. People assume that means it’s written by AI.” I looked back at the piece I’d just written. There were dashes all over it—and for good bloody reason. Em dashes—often used to connect explanatory phrases, and so named because they’re the width of your average lowercase ‘m’—are probably my favourite bit of punctuation. I’ve been option + shift + hyphening them for years.A person’s writing often reflects how their brain works, and mine (when it works) tends to work in fits and starts. My thoughts don’t arrive in perfectly rendered prose, so I don’t write them down that way. And here I was being told the humble em dash—friend to poorly paid internet hacks everywhere—was now considered a sign not of genuine intelligence, but the other sort. The artificial sort. To the extent that I have to go through and manually remove them one by one, like nits. The absolute cheek. Not only am I losing my livelihood to AI—I’m losing grammar too.My petty gripe: bands naming themselves as puns on other bands – will it never end?Read moreWhy couldn’t machines have embraced the semicolon? No one gives a fig about those.The reason AI is so hung up on em dashes–as you might expect–is because humans were first. Large language models were trained on vast swathes of actual writing, including mine, I suppose, with the result that em dashes got baked into algorithms from the beginning. It’s flattering, in a way. “Humans used them so often that AIs learned them as a default natural flow,” Brent Csutoras notes on Medium. “It’s like asking a bird not to chirp.”Well, my chirping days are over. It seems I have two choices now—keep using em dashes with a sort of stubborn, curmudgeonly pride until all my clients stop exchanging money for words, or start writing incredibly long run-on sentences, like this, with commas all over the place … and maybe ellipses too; ideas connected by semicolons. Staccato flow. Full stops everywhere. Nope, that sucks too. Sounds like someone flicking between radio stations.Oh, em dash—what will we do without you?Explore more on these topicsArtificial intelligence (AI)Petty gripesChatGPTcommentShareReuse this content",
      "author": null,
      "published_date": null,
      "meta_description": "The humble em dash is being used as a tell that something is written by a large language model. But it’s <strong>James Shackell</strong>’s favourite piece of punctuation, and he’s not ready to lose it",
      "word_count": 459,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://lwn.net/Articles/1040120/",
      "title": "Bcachefs removed from the mainline kernel",
      "content": "LWN.net\nNews from the source\nContentWeekly EditionArchivesSearchKernelSecurityEvents calendarUnread commentsLWN FAQWrite for us\n|\n|\nSubscribe /\nLog in /\nNew account\nBcachefs removed from the mainline kernel\n[Posted September 30, 2025 by corbet]\nAfter marking bcachefs \"externally maintained\" in 6.17, Linus Torvalds has\nremoved\nit entirely for 6.18.\n\"It's now a DKMS module, making the in-kernel\ncode stale, so remove it to avoid any version confusion.\"\nRisks\nPosted Sep 30, 2025 10:30 UTC (Tue)\nby patrakov (subscriber, #97174)\n[Link] (1 responses)\nBefore Bcachefs getting mainlined, one had to get the whole kernel source tree from the Bcachefs author. This happened because this filesystem relied on changes in other subsystems, and was not just a self-contained add-on. Upon upstreaming, the required changes were applied to these subsystems, making Bcachefs self-contained, and then it was split out as a module that can be compiled via DKMS.\nHere I see the risk of the mainline kernel reverting the changes in other subsystems, because they might no longer have an in-tree user. Am I paranoid? What's the Bcachefs project's position on this risk if it is material - are they going to adapt, or go back to telling the user to compile their tree, as opposed to compiling just their module against an existing kernel?\nRisks\nPosted Sep 30, 2025 16:45 UTC (Tue)\nby georgh-cat (guest, #158897)\n[Link]\nThat's exactly my thought as well, and the kind of conversation that is impossible in phoronix (but it should)\nDecision process\nPosted Sep 30, 2025 12:44 UTC (Tue)\nby daeler (subscriber, #130460)\n[Link] (3 responses)\nJust curious, no judgement: How is this decision made? Can Linus just decide that he removes something from the kernel?\nDecision process\nPosted Sep 30, 2025 12:56 UTC (Tue)\nby corbet (editor, #1)\n[Link]\nYes, Linus can make that kind of decision.\nHe doesn't just do it on his own, though; there was a long series of public and private discussions that led up to this one.\nDecision process\nPosted Sep 30, 2025 12:57 UTC (Tue)\nby pizza (subscriber, #46)\n[Link] (1 responses)\n> Can Linus just decide that he removes something from the kernel?\nLinus (just like everyone else with the Linux sources, can add or remove anything he wants.\nOf course, nobody else is forced to get \"Linux\" from him directly.\nIndeed, the vastly overwhelming majority of Linux users get their kernel from someone else, with features, drivers, and fixes not present in, or removed outright from, Linus's Linux.\nDecision process\nPosted Sep 30, 2025 13:37 UTC (Tue)\nby Lionel_Debroux (subscriber, #30014)\n[Link]\nNote that things go the other way round as well: security fixes available in Linus' Linux but missing from other kernels because the backporting process didn't occur for some reason (difficulty, interest, etc.).\nThe older a third-party kernel version is, the more it is likely to be missing both backports for security fixes, and vulnerabilities in code introduced in newer versions but not backported to the given third-party kernel (some vendors perform large amounts of backports to their franken-kernels).\nI think this was the right thing\nPosted Sep 30, 2025 15:41 UTC (Tue)\nby DemiMarie (subscriber, #164188)\n[Link] (2 responses)\nI don’t think that experimental filesystems belong upstream.\nThe need to get fixes out super quickly to recover user’s data does not mix well with the upstream kernel’s release cycle.\nOnce a filesystem has settled down and emergency fixes are less likely, upstreaming makes a lot more sense.\nThe only exception I can think of is if recovery is handled almost entirely in userspace, which is not tied to the kernel release cycle.\nI think this was the right thing\nPosted Sep 30, 2025 15:45 UTC (Tue)\nby intelfx (subscriber, #130118)\n[Link]\nThe problem is that according to Linus' et al. philosophy (which subsequently dictates their actions), everything belongs upstream and out-of-tree modules kinda sorta \"do not exist\". No changes to kernel internals are accepted (or, conversely, refused) to accommodate out-of-tree modules, and the mentioned people are kind of very vocal in asserting that.\nSo if you're developing something that \"does not belong upstream\", the second you need an in-kernel API modified to suit your code (or, conversely, the second an in-kernel API is modified in such a way that harms your code) — you are SOL.\nI think this was the right thing\nPosted Sep 30, 2025 17:29 UTC (Tue)\nby roryi (subscriber, #25099)\n[Link]\nIt was very clearly marked as being experimental, though. I find it hard to understand who could possibly be using an experimental filesystem in a bleeding-edge kernel for important data without backups. And, honestly, it feels like there's potentially a more serious problem here than any of the much-aired interpersonal issues.\nHas someone been actively recommending use of bcachefs to naive end users? Is there such a thing as a meme filesystem - and if so, has bcachefs somehow become one? Is there a rogue forum poster / youtuber / tiktoker out there tricking people into such risky behaviour without realising the implications?\nCopyright © 2025, Eklektix, Inc.\nComments and public postings are copyrighted by their creators.\nLinux\nis a registered trademark of Linus Torvalds",
      "author": null,
      "published_date": null,
      "meta_description": null,
      "word_count": 855,
      "scraping_method": "beautifulsoup"
    },
    {
      "url": "https://www.antonsten.com/articles/ai-will-happily-design-the-wrong-thing-for-you/",
      "title": "AI will happily design the wrong thing for you",
      "content": "I need to clear something up about my book, “Products People Actually Want.” When I write about how “anyone can build anything” now, some people assume I’m anti-AI. That I think these tools are ruining design or product development.\nThat’s not it at all.\nAI tools are incredible leverage. They make me think faster, broader, and help me produce better work. But like any creative partner or colleague, this doesn’t happen out of the gate. I’ve spent hours—days—training my AI on how we write at Summer Health so the tone is right. I’ve taught it our business model. I tweak its suggestions to better match the experience we want to build.\nThe problem isn’t that AI exists. The problem is how most people use it.\nThe real issue with “anyone can build anything”\nMy book focuses on a specific problem: people building things without knowing if anyone actually needs them. When the barrier to building drops to zero, we get flooded with products that work fine but solve problems that don’t exist.\nBut there’s a second issue too. While AI can help you get started quickly, it won’t build something ready for mass market. Users’ expectations for polish and detail are much higher than what AI produces by default. You can spot AI-generated work from a mile away because it lacks the intentional decisions that make products feel right.\nThe combination is brutal: people building the wrong things, and building them poorly.\nJust last week, we saw a perfect example. Food influencer Molly Baz discovered that Shopify was selling a website template featuring what she called “a sicko AI version of me.” The image—a woman in a red sweatshirt eating an onion ring in a butter-yellow kitchen—looked almost identical to her cookbook cover.\nThis isn’t really an AI problem, it’s a laziness problem that AI makes more tempting. What used to be someone using an image without permission now gets dressed up as “AI-generated content.” The ironic part? Creating something unique with AI tools is actually easier than trying to replicate someone else’s work. The tools are there. The capability is there. The only thing missing is the intention to actually create something new.\nHow I actually use AI\nI use AI tools extensively, but strategically:\nGranola for transcribing user research sessions\nVisual Electric for images that are impossible to find in stock libraries (like families that aren’t white middle class)\nChatGPT as a sparring partner and for writing better copy\nCursor for building websites and prototypes\nA custom copywriter agent trained on our brand voice\nBut here’s what I don’t do: I don’t use AI to replace thinking. I saw a tool recently that listens to user interviews and suggests follow-up questions. Tools like this make designers lazier, not better. You need genuine curiosity. You need to understand what you’re looking for before you can synthesize anything meaningful.\nWhat AI is great at is helping you process large sets of information and pull out themes. But if you don’t understand your users and what they’re struggling with first, it’s impossible to prompt any AI tool for a real solution.\nHere’s the thing about AI tools: if you don’t know what your customers want, if you as a designer don’t have a view on how to package it, AI will happily make all of that up for you. That just doesn’t mean it’s the right thing. AI fills in the blanks confidently, but those blanks are exactly where the real design work should happen.\nThe skills that actually matter\nThe divide for engineers is easier to see: tools can help them code much faster, but it’s worthless if they can’t understand the generated code.\nFor designers, the key skill going forward will be taste and curation. Understanding what you want to prompt before diving in. Knowing good work from generic work. Being able to refine and iterate until something feels intentional rather than automated.\nI think designers who resist AI entirely will find themselves without jobs in the next five years. When you can use AI to speed up tedious tasks, what’s the reason for doing them manually?\nBut designers who treat AI like a magic button will struggle too. The ones who thrive will use AI as leverage—to think better, work faster, and explore more possibilities than they could alone.\nAI amplifies everything\nHere’s how I see it: AI is leverage. It amplifies whatever you bring to it.\nIf you understand your users deeply, AI helps you explore more solutions. If you have good taste, AI helps you iterate faster. If you can communicate clearly, AI helps you refine that communication.\nBut if you don’t understand the problem you’re solving, AI just helps you build the wrong thing more efficiently. If you have poor judgment, AI amplifies that too.\nThe future belongs to people who combine human insight with AI capability. Not people who think they can skip the human part.\nMy book isn’t the antidote to AI. It’s about developing the judgment to use any tool—AI included—in service of building things people actually want. The better you understand users and business fundamentals, the better your AI-assisted work becomes.\nAI didn’t create the problem of people building useless products. It just made it easier to build more of them, faster. The solution isn’t to avoid the tools. It’s to get better at the human parts of the job that the tools can’t do for you.\nYet.\nMy book Products People Actually Want is out now.\nDid you enjoy this article?\nShare on LinkedIn\nShare on Bluesky\nCopy link\nJoin 3,000+ designers, developers, and product people who get my best ideas about design each month.",
      "author": null,
      "published_date": null,
      "meta_description": "AI tools are incredible leverage, but they amplify whatever you bring to them. If you don't understand the problem you're solving, AI just helps you build the wrong thing more efficiently.",
      "word_count": 945,
      "scraping_method": "beautifulsoup"
    }
  ],
  "audio_files": [
    "output/audio/hackercast_20250930_105346.mp3"
  ],
  "stats": {
    "stories_fetched": 19,
    "articles_scraped": 16,
    "total_words": 16271,
    "audio_files_generated": 1
  }
}