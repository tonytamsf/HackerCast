Welcome to HackerCast, your daily digest of the top stories from Hacker News. Today is September 16, 2025, and we have 18 fascinating stories to share with you.

Story 1: Global Peace Index 2025
The Global Peace Index 2024 reveals that the world is at a crossroads. Without concerted effort, there is a risk of a surge in major conflicts. There are currently 56 conflicts, the most since World War II. They have become more international with 92 countries involved in conflicts outside their borders, the most since the GPI‚Äôs inception. The rising number of minor conflicts increases the likelihood of more major conflicts in the future. For example, in 2019, Ethiopia, Ukraine, and Gaza were all identified as minor conflicts.
‚Ä¢ 97 countries deteriorated in peacefulness, more than any year since the inception of the Global Peace Index in 2008.
‚Ä¢ Conflicts in Gaza and Ukraine were the primary drivers of the global fall in peacefulness, as battle deaths reached 162,000 in 2023.
‚Ä¢ 92 countries are currently involved in conflicts beyond their borders, more than at any time since the inception of the GPI.
‚Ä¢ First of its kind military scoring system suggests that US military capabilities are up to three times higher than China.
‚Ä¢ The global economic impact of violence increased to $19.1 trillion in 2023, representing 13.5% of global GDP. Exposure to conflict poses a significant supply chain risk for governments and businesses.
‚Ä¢ Militarisation recorded its largest yearly deterioration since the inception of the GPI, with 108 countries becoming more militarised.
‚Ä¢ 110 million people are either refugees or internally displaced due to violent conflict, with 16 countries now hosting more than half a million refugees.
‚Ä¢ North America saw the largest regional deterioration, driven by increases in violent crime and fear of violence.
‚Ä¢ Iceland, Ireland, Austria, New Zealand, and Singapore are the top 5 most peaceful countries in the world in 2024.

Last year recorded 162,000 conflict related deaths. This was the second highest toll in the past 30 years, with the conflicts in Ukraine and Gaza accounting for nearly three-quarters of deaths. Ukraine represented more than half, recording 83,000 conflict deaths, with estimates of at least 33,000 for Palestine up to April 2024. In the first four months of 2024, conflict related deaths globally amounted to 47,000. If the same rate continues for the rest of this year, it would be the highest number of conflict deaths since the Rwandan genocide in 1994.

Read more: Highest number of countries engaged in conflict since World War II

Next up...

Story 2: Apple releases iOS 15.8.5 security update for 10-year old iPhone 6s
About the security content of iOS 15.8.5 and iPadOS 15.8.5
This document describes the security content of iOS 15.8.5 and iPadOS 15.8.5.
About Apple security updates
For our customers' protection, Apple doesn't disclose, discuss, or confirm security issues until an investigation has occurred and patches or releases are available. Recent releases are listed on the Apple security releases page.
Apple security documents reference vulnerabilities by CVE-ID when possible.
For more information about security, see the Apple Product Security page.
iOS 15.8.5 and iPadOS 15.8.5
Released September 15, 2025
ImageIO
Available for: iPhone 6s (all models), iPhone 7 (all models), iPhone SE (1st generation), iPad Air 2, iPad mini (4th generation), and iPod touch (7th generation)
Impact: Processing a malicious image file may result in memory corruption. Apple is aware of a report that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals.
Description: An out-of-bounds write issue was addressed with improved bounds checking.
CVE-2025-43300: Apple
Information about products not manufactured by Apple, or independent websites not controlled or tested by Apple, is provided without recommendation or endorsement. Apple assumes no responsibility with regard to the selection, performance, or use of third-party websites or products. Apple makes no representations regarding third-party website accuracy or reliability. Contact the vendor for additional information.
Published Date:¬†September 15, 2025

Next up...

Story 3: Things you can do with a Software Defined Radio (2024)
Fifty Things you can do with a Software Defined Radio üìª

Last week, I went on an adventure through the electromagnetic spectrum!

It‚Äôs like an invisible world that always surrounds us, and allows us to do many amazing things: It‚Äôs how radio and TV are transmitted, it‚Äôs how we communicate using Wi-Fi or our phones. And there are many more things to discover there, from all over the world.

In this post, I‚Äôll show you fifty things you can find there ‚Äì all you need is this simple USB dongle and an antenna kit!

A couple of years ago, I heard about the ‚ÄúMake 50 of Something‚Äù technique in Vi Hart‚Äôs Fifty Fizzbuzzes. Since then, I‚Äôve already made fifty programs for the fantasy console TIC-80 in one weekend in 2021.

I found that a very exciting experience ‚Äì trying to make so many new things really pushed me to leave my comfort zone, to be creative, and not to get sucked into rabbit holes too deep.

I knew I definitely wanted to try the technique again. So, when I took a week of vacation, I decided to try to find 50 things to do with a Software Defined Radio!

What is an SDR?

A Software Defined Radio is essentially a radio that relies on a computer to do most of its data processing. It doesn‚Äôt rely on analog hardware too much ‚Äì instead, most of what is does is ‚Äúdefined in software‚Äù, hence the name.

Usually, SDRs can detect electromagnetic waves in a much wider range than a common FM radio, which makes it especially exciting! I got interested in SDRs after reading about Albert‚Äôs project to build one as a module for the Framework laptop!

I went into this week without much knowledge of the things I‚Äôd find. I‚Äôd read through a introductory course for aspiring amateur radio operators (more on that later), but I barely knew which way to point my antenna.

If you want to follow along, this section is intended to help you get started!

Most of the 50 things also have a little infobox at the beginning, explaining the frequencies, and some special knowledge needed to receive them.

I looked into the topic a bit, and a popular, cheap SDR right now is the RTL-SDR Blog V4, which has the form factor of a simple SUB dongle. You can get it for around $30, or as a kit with telescopic antennas for $50.

Everything I tried during this week was done using this USB dongle, the antenna kit, and a long piece of wire!

I tried to adjust my antenna to the desired frequencies as best as I could. I think for receiving, it‚Äôs not super important that your antenna is perfectly configured, though.

For most applications, I used the dipole antennas that came with the kit I purchased. Dipole antennas have two sides that stick out the same length. You generally wanna make the whole antenna half as long as the wave length you want to receive, and orient it vertically.

My rule of thumb was to divide 72 by the frequency in MHz, and take that as the length of each side of the dipole in meters. That‚Äôd make the whole antenna a bit shorter than half of the wavelength.

For example, this is what the configuration looked like for frequencies around 100 MHz:

And for higher frequencies, I used the tiny screw-on antennas from the kit:

For specific applications like receiving satellites, or receiving locators for airplanes, I used special configurations, but I‚Äôll discuss these as we go!

The software I liked best, and which I used for many things, was SDR++. It allows you to explore the frequency spectrum very smoothly, and has a modern user interface!

But I also used plenty of other software, on Linux in my case. I‚Äôll link to the software as needed below.

On Monday morning, I was excited to start this project! I sat down at my desk, and got to work!

This as an obvious first thing to do, as the signals are very strong! I was using the SDR++ software, and it felt very nice browsing around and discovering the stations around me! It reminded me of exploring the radio as a child.

I found a local station that gives 1-hour slots to civic groups, for example!

This is a special frequency range in Germany: Anyone is allowed to send there, using licensed devices. There are 6 channels.

I think someone was testing their device there when I listened in. :D I heard a ‚ÄúHellooo?‚Äù, then a ‚ÄúTest, test‚Äù, and then a ‚ÄúGeneral call to all stations‚Äù. Oh, and shortly after a short transmission on channel 3 in a Slavic-sounding language!

Freenet devices have a range of only a couple of kilometers, so these people must have been pretty close! :O

While browsing the aviation frequencies, I found this station that reports weather conditions in an endless loop. It seems to be the ‚ÄúAutomatic Terminal Information Service‚Äù of Hamburg airport!

Thanks to that, I found out that the current air pressure was 1011 hPa! :D

Listening to ‚Äúmessages not meant for the general public‚Äù is not allowed in Germany, so of course I didn‚Äôt do that. And if I had accidentally done that, I wouldn‚Äôt be allowed to tell you about it. üôÖ

That‚Äôs short for ‚ÄúAutomatic Dependent Surveillance ‚Äì Broadcast‚Äù. Aircraft send it automatically to be tracked.

For this, I built my first antenna! From wire and and an antenna connector called ‚ÄúSMA‚Äù.

And it worked! \o/ I decoded the signal using the software SDRangel. Fascinating! I saw some big & small airplanes, and even a helicopter!

How stereo audio is transmitted is really interesting, because it‚Äôs backwards-compatible to receivers that don‚Äôt support it:

Here, you see the demodulated audio frequency spectrum, as shown in SDRangel. Below 19k Hz, it‚Äôs just mono audio. Then, to mark a stereo station, there‚Äôs a constant ‚Äúpilot tone‚Äù at 19k Hz! (Outside of what most humans can hear.)

Then, if you double the frequency of the pilot tone, you can derive the sections where the difference of the left & right channel to the mono channel is transmitted!

Correction: I‚Äôve been told that instead of what I call ‚Äúleft‚Äù and ‚Äúright‚Äù in this diagram, the upper frequencies transmit the difference of the left and right channels! That way, the receiver can calculate the left and right channels from the mono signal (which is, esseutially, the sum of left and right).

If you triple the frequency of the pilot tone, you get to a range where FM stations transmit small amounts of digital metadata, like the name and genre of the station, and the current song! That‚Äôs a protocol called Radio Data System.

This system can also transmit road traffic information! There seemed to be a road closure at ‚Äú0x64BE‚Äù, as decoded by SDRangel.

The Federal Highway Research Institute publishes an Excel table, where I could look up that this is a town in Lower Saxony!

8: Listen to conversations on the 2-meter amateur radio band

This is a frequency range reserved for amateur radio operators ‚Äì for non-commercial use only. You may send on this band after getting a license.

What I found here is seemingly a conversation circle facilitated by a relay around 15 km away from here ‚Äì it takes input on a certain frequency, and outputs an amplified copy of it on another frequency! Klaus, Bernd, J√ºrgen and Horst were talking about antennas, relays, and Windows XP! üòÅ

The SDRangel software also has a demodulator for Digital Audio Broadcast! :O I continue to be amazed by it!

I think this was the first time I‚Äôve received digital radio via air! I saw so many stations, and I‚Äôve only checked a couple of channels.

The advantage of this digital channel is that there‚Äôs no noise. And I even saw a ‚Äúcover image‚Äù in one of the programs!

This is a frequency range for ‚ÄúPrivate Mobile Radio‚Äù. It‚Äôs another of these bands where anyone can transmit using a licensed device!

Not a lot of activity here. I heard ‚ÄúHello, hellooo!‚Äù, ‚ÄúCan you hear me?‚Äù and some short transmissions that sounded like a child! :D

There also seemed to be digital transmissions, but I didn‚Äôt know how to decode them yet.

The range of PMR446 devices is pretty low (a couple of hundred metres in cities), so again, the people must‚Äôve been close!

After the first day of SDR experiments, I was amazed how much invisible communication is going on around us in the electromagnetic spectrum at the same time!

I posted each of these things on Mastodon as I went, and asked people for suggestions for more things I could receive.

At 433 MHz, there‚Äôs a frequency band for ‚Äúindustrial, scientific and medical‚Äù applications. And wow, there was quite a lot of activity nearby!

Using the decoder rtl_433, I saw two sensors that output the current temperature, humidity, and air pressure!

There were also some ‚ÄúIBIS beacons‚Äù flying by, which are used in public transportation, so maybe it‚Äôs buses driving by?

Later, an ‚ÄúInterlogix Security‚Äù device also appeared, reporting ‚Äúclosed switch states‚Äù :O

Ships send out their status using AIS (Automatic Identification System). And again, I received a lot of them here in Hamburg! :O

I was especially excited to receive data from the MS Stubnitz (a fisher boat that was turned into a culture center/techno club)! It reports its status as ‚Äúmoored‚Äù, and its speed as 0.1 knots! :D

Again, I used the software SDRangel. Apparently, it can also display a 3D map, but I haven‚Äôt figured out how to add 3D models‚Ä¶
‚Ä¢ Frequency: 876-959 MHz, I looked up the specific ranges for Germany on Wikipedia

I was curious whether you could tell if someone used their phone! So I borrowed a GSM phone, tuned to the correct frequencies, and made some test calls.

What surprised me most: You can kind of ‚Äúsee‚Äù the volume at which I was talking!?

In the recording, the three dense bands at the end were when I was humming into the phone at the other end. This only worked in the ‚Äúreceiving‚Äù direction.

I spent all Tuesday afternoon and evening learning about satellites. The program gpredict is really nice to find out when satellites will pass overhead! I learned a lot, including that one satellite I was trying to receive burned up last week! :D

I was super excited when I first received a signal from a NOAA satellite! üõ∞Ô∏è

But I didn‚Äôt manage to decode it properly yet. Maybe my reception was too noisy? I wanted to keep trying, but I had to move on.

In Germany, the police has switched to an encrypted digital protocol called TETRA.

Even though I‚Äôve seen some interesting talks at CCC events about weaknesses in the decryption, all I wanted to do for now is looking at the pretty signals in SDR++. :3

Again, this is communication not meant for the general public.

I didn‚Äôt listen to someone dispatching taxis to specific addresses, and you also shouldn‚Äôt do that either. üöï

Some of the most fun I had was just browsing frequencies and seeing what I can find! Sometimes, I encountered signals I can‚Äôt identify.

For example, at 865-868 MHz, there was a family of slow, continuous, digital signals that made a nice melody when listened to in single-sideband demodulation!

And at 177-180 MHz, there were two very broadband transmissions. Might be TV? But I couldn‚Äôt find out what type. (It later turned out that I‚Äôd already listened to these signals ‚Äì it was digital radio, DAB+.)

As I was browsing around for things to receive, I saw on this tracking website that a radiosonde was just launched in Hamburg! SDRangel could decode its transmission! It had climbed to a height of 7 km, and it‚Äôs -17 ¬∞C there!

I knew that it would eventually burst and fall back to Earth, and that I could try to get to it and find it!

I decided to go on a field trip, using trains and my bike.

I was following the tracker. The balloon popped earlier than predicted, and I frantically changed travel plans!

Eventually, it landed in a forest. I hoped I could get to it! What made this adventure more tricky was that my mobile Internet contract ran out while I was on the go, and my battery was also almost empty.

But I made it to the forest, and entered it.

As I circled the site, I encountered a person in their 60s, with a stubbly beard and a blue wool hat. He was looking in the direction of the crash site, and was holding a smartphone, so I asked him whether he also was looking for the radiosonde.

He was! We looked for it together for half an hour, jumping over small rivers and crawling through the woods, while he gave me a lot of tips related to hunting sondes.

He told me that he had found around 40 of them so far!

Usually, the sondes keep broadcasting after landing, but this one wasn‚Äôt. So he quickly guessed that someone else could‚Äôve taken it. Or maybe it landed in the water and died?

Some pictures of the area we searched:

Eventually, we gave up, and walked back to our vehicles. He also is an amateur radio operator, and could answer a couple of questions related to building antennas!

And he was right: Someone had been faster than us! The status was changed. So in the end, I didn‚Äôt find the sonde. But something that might be even better ‚Äì a friend!

In the 2-meter amateur band, there are certain frequencies for the ‚ÄúAutomatic Packet Reporting System‚Äù. It‚Äôs a bit like IP ‚Äì packets have a ‚Äúfrom‚Äù and a ‚Äúto‚Äù. They can also broadcast their position, or weather data.

Some stations seem to announce themselves as repeaters, which probably help forward the packets to increase the range.

And two people seemed to be on a ‚Äúfieldday‚Äù, and broadcasted their location. :D

I started the day by building an antenna!

This was going to be a simple ‚Äúrandom wire‚Äù antenna, to allow me to get better reception in the lower frequencies, which I‚Äôve omitted so far (because I knew it would be much more fun with a better antenna)!

I measured out 21.6 m of wire (which for ‚ú®magic‚ú® reasons seem to be a good universal antenna length)‚Ä¶

‚Ä¶directly attached it to the center of another SMA connector‚Ä¶

‚Ä¶and draped it all around my room!

People on the Internet say that there are many problems with this ‚Äì that it would be better to have it outside, and that there‚Äôs an impedance mismatch between the receiver and the wire.

I could address those problems, but I wanna try how well this works first :)

I‚Äôd been learning it a little bit, so if I recorded it and slowed it down, I could understand it: They‚Äôre sending their callsigns. These are from Belgium, France, and Italy! \o/

I compared to my 2-meter dipole antenna, and the reception was definitely better ‚Äì I can pick up more transmissions, and with much less noise!

The German Weather Service broadcasts maritime information throughout the day on various shortwave frequencies.

They use a protocol called RTTY (radioteletype), and it took me a while to decode it. But I found a neat little program called ‚Äúfldigi‚Äù: You can pipe audio to it (single side band modulation), and then if you pick the correct settings (see screenshot), it happily transcribes the messages!

Here‚Äôs the station weather reports for the Baltic Sea and Northern Sea!

I found some other strange signals on the 30-meter band. The Signal Identification Wiki was really helpful for figuring out what they were: FT8!

FT8 is quite a new protocol, invented in 2017, and it seems to be super popular right now! It allows you to transmit short messages, and again, people are looking for people to talk to (CQ), saying how well they receive each other, or saying goodbye (73).

This is the WSJT-X software.

24: Detect whether your notebook is charging

As I was browsing the very low-frequency bands, I had a strange problem: Sometimes, that would work okayish, sometimes I could even make out voices!

But other times, it wouldn‚Äôt work at all, and everything would be loud, angry noise. Even in regions where I had better reception before!

Eventually, I found out how to solve that issue ‚Äì by unplugging my notebook charger. D‚Äôoh! :D

In the low frequencies, occasionally, you can hear a short chirp! :D These are caused by ionosondes, scientific instruments which measure the properties of the ionosphere by sweeping a wide frequency spectrum.

Another signal (which I accidentally got in the same screenshot) is a radar system ‚Äì in this case, according to the Signal Identification Wiki, it‚Äôs a ‚ÄúCODAR‚Äù system, used to measure the motion of water waves and currents along coasts! :O
‚Ä¢ Frequency: In all amateur bands, especially the ones below 30 MHz

How do you transmit speech over long distances? You can use ‚Äúamplitude modulation‚Äù, where you change the volume of the carrier frequency to model your audio.

As a side effect, the bands to the sides of the carrier will contain a signal, as well.

One trick is to transmit just those sidebands, which saves power! But you have to ‚Äúguess‚Äù the base frequency when listening. Depending on which part you transmit, this is called ‚Äúlower side band‚Äù or ‚Äúupper side band‚Äù.

SDR++ makes it very easy to play with this! :) Here‚Äôs someone from Serbia!

28: Listen to AM radio from the other side of the world

At night, low-frequency radio waves can travel further around the world, because they‚Äôre reflected by the layers of the ionosphere! There‚Äôs something magical about this.

I put my antenna outside, and I could hear a lot of broadcasting stations! On short-wave.info, you can look up where they are located.

Some stations in China are broadcasting with very high power! Some I could hear were over 7500 km away.

Originally, I had planned the project to run from Monday to Friday. When I still had 32 things to do in Friday morning, I knew I‚Äôd need to extend it. But I hadn‚Äôt run out of ideas yet:

After I‚Äôd looked into the low frequencies on Thursday, I went to a higher band again: The Citizens Band!

This is the third frequency band I‚Äôm aware of where anyone is allowed to transmit ‚Äì provided that you use a licensed device!

This is a band where my random wire antenna really came in handy. Without it, I would have had a hard time understanding anything. And even with it, transmissions are extremely noisy.

CB radio is used internationally, especially by truck drivers, it seems.

30: Assess the propagation of radio waves using beacons

The International Beacon Project runs a network of 18 stations, which take turns transmitting their callsigns at certain frequencies.

Using this system, you can quickly get a sense of how well radio waves are currently propagating to your location. Clever!

I picked up the beacon from southern Finland! You can see its callsign scrolling away in the video. It‚Äôs followed by four dashes send with decreasing power. I only heard the first one‚Ä¶

I would‚Äôve loved to receive DCF77, which powers the radio clocks in Germany! But no matter how hard I listened to 77.5 kHz, there was nothing there. I don‚Äôt think my dongle can do that.

So I used higher frequencies! Russia transmits its ‚ÄúRWM‚Äù time signal at 9996 kHz, which beeps every second, with a long beep for the full minute.

Not enough to tell the time, but enough to adjust your wrist watch, I guess!
‚Ä¢ Frequency: 3855, 7880, and 13882.5 kHz (see weatherfax.com for more)

The German Weather Service broadcasts weather maps throughout the day! You can decode them using fldigi‚Äôs ‚ÄúWEFAX-576‚Äù setting.

I caught this one only halfway through. According to the schedule, it‚Äôs the ‚ÄúSurface weather chart North Atlantic, Europe‚Äù!

If you squint really hard, you can make out the coast of Spain and the Mediterranean Sea on the right side!

I couldn‚Äôt stop trying to capture a weather satellite, it‚Äôs just too cool to receive an image from space!

That evening, an American satellite called NOAA-15 passed right over us, so I thought I‚Äôd try again. And this time, I got parts of an image! \o/

This is real-time data! At night, both transmitted images are infrared recordings.

I recorded the FM signal using SDR++, and then decoded the image using noaa-apt, which also added country outlines.

Here‚Äôs what the NOAA-15 weather satellite sounds like, by the way! tick-tock

While recording, I noticed something strange: The transmission didn‚Äôt happen at the frequency I had expected it to! And also, the frequency changed.

Then it hit me: Doppler effect! At the time of the recording, the frequency was around 4250 Hz higher than expected.

After looking up the formula, I calculated a relative speed of 9 km/s! (Which got close to its real speed, 7.5 km/s.)

These stations send encrypted messages using number sequences, possibly for espionage purposes!

So why not listen to one? There‚Äôs a surprisingly well-maintained database of them on a site call Priyom.

So I tuned into the next frequency that was listed, and: Bingo!

Allegedly, this was a station in Moscow. That day, it sent ‚Äú218, 218, 218‚Äù in a loop, followed by three long beeps, which is the format of a ‚Äúnull message‚Äù.

So no news for the Russian spies.

The week was really intense for me. Initially, I thought I‚Äôd do 10 things per day, but it turned out that that was too much. I had to learn so many new things.

Many things I tried don‚Äôt work on my first attempt. Finding LoRaWAN signals, decoding packet radio, finding something on PMR446, decoding the satellite ‚Äì those were all things that required a second (or third) attempt.

This project was exhausting, but also joyful ‚Äì having committed to it, I got in a nice flow state, where I could focus on it for hours.

Often, I thought: ‚ÄúOkay, this is it. I can‚Äôt possibly find more things.‚Äù But this is the power of the 50 Things technique: I have to keep looking, leave my comfort zone, be creative, try things I otherwise wouldn‚Äôt have tried!
‚Ä¢ Frequency: 14.230, 14.233, 21.340, 28.680, 145.625 MHz seem to be popular

Using a protocol called ‚ÄúSSTV‚Äù (slow-scan television), amateur radio operators send each other postcards! :D

I‚Äôve been browsing the usual frequencies, and tried to decode images using the software QSSTV on Linux. And I accidentally caught a piece of what seems to be a test image!

There‚Äôs a mysterious Russian station broadcasting at 4625 kHz. Sometimes, it sends encrypted voice messages.

But usually, all it does is send a honking sound every two seconds, to deter other stations from using the same frequency.

The purpose of the station is unclear, but most theories think it‚Äôs military communication.

This was a bit like trying to catch a rare insect! üêõ

LoRaWAN is a low-power, wide-area networking protocol, intended for ‚ÄúInternet of Things‚Äù applications.

You can see transmission in the lower half of the screenshot! It has a very cute structure: You can see eight ‚Äúdown-chirps‚Äù, followed by two ‚Äúup-chirps‚Äù. That‚Äôs the header, followed by the payload.

To look for the signal, I made a ‚Äúbaseband capture‚Äù in SDR++, and opened the recording in Sonic Visualizer.

Devices like smoke detectors or meters for water or heat are sending their readings via a protocol called Wireless M-Bus.

Again, I was surprised by how many devices seem to be around! Thanks for the tip, @envy :)

wmbusmeters is a really nice tool for decoding the messages.

The chips in my SDR stick are also being used in DVB-T dongles! So, can we watch TV? Unfortunately, no.

From what I pieced together, there‚Äôs a difference between using the stick in SDR mode (where it sends the full spectrum), and in TV mode (where it sends the decoded video).

In Germany, there‚Äôs now DVB-T2, which my hardware doesn‚Äôt support in TV mode. And in SDR mode, the bandwidth is too narrow for DVB-T2. But we can scroll over a channel and look at it! :3

Did a little walk to a big intersection, to see what ‚Äúdevice signals‚Äù I‚Äôd find there at 433 MHz.

I could confirm that the IBIS beacons are in fact being sent by buses! The included ‚Äúvehicle ID‚Äù even matches the white number that‚Äôs printed on it.

I also saw some messages from tire pressure monitoring systems in cars! They also include an ID, and usually, the brand of the car! The owners probably aren‚Äôt aware how easy it would be to track them‚Ä¶ (Thanks, @scy!)

Side note: I wonder why some signals in that band are warped like the one at 433.96 MHz here!

At first, I thought ‚ÄúAh, Doppler effect again, it‚Äôs coming from a moving car!‚Äù But if that‚Äôd be the case, that car would be moving at over 700 m/s‚Ä¶

Friends later suspected that this effect is due to weak batteries affecting the crystal in the sending devices, or temperature changes.

So I caught a satellite again! :D This time, it was school project, the Italian satellite ‚ÄúMax Valier‚Äù. It continuously sends Morse code on a beacon frequency.

Pretty weak signal, but here‚Äôs what I could hear:

Super happy about this! I got both the name of the satellite, as well as its callsign at the end, and what seems to be some kind of greeting? I later learned that is Morse code shorthand for ‚Äúand‚Äù, and that Manfred and Christa Fuchs were the founders of a company that helped launch the satellite!

This is another thing that‚Äôs not allowed in Germany, so you shouldn‚Äôt do it.

Pagers use a format called ‚ÄúPOCSAG‚Äù (Post Office Code Standardisation Advisory Group‚Ä¶), which you should not decode using multimon-ng.

Because you would find that the content is short and cryptic anyway. It would probably be repeated by several stations all around you, to make sure the whole region is covered.

Do not read the English Wikipedia page! It contains frequencies!

At this point, I was pretty tired. Focusing on this project for 6 days straight took a lot of energy, and I was always uncertain if I could actually complete all 50 things in that week! But I woke up with a fun idea:

44: Detect when a smartphone is turned on

I was curious whether I could see the NFC transceiver in my smartphone! And yeah, especially using my random wire antenna, this works really well!

My smartphone seems to emit at the NFC frequency a couple of times per second. And when unlocking the screen, it emits five very strong beeps on that frequency! I can see those from the other side of our apartment.

Surely, these signals are the same for every device, right? üò∂

Observe the five beeps here:

Piko and I played around with NFC a bit more, and we found out that when getting close to an NFC tag, a smartphone emits at 13.56 MHz continuously!

So, we started sending Morse code to each other between rooms, using a smartphone and a library book! :‚ÄôD

Seems that the shortest signal you can create is 0.7 s long, resulting in a meager communication speed of 3-4 words per minute‚Ä¶

There are ground stations that emit a signal that allow calculating your angle relative to it! If you receive two, you can determine your position. (Thanks, @fly_it!)

I heard the one close to Hamburg! And SDRangel has a decoder, of course! It calculated angles between 210¬∞ and 230¬∞, which is pretty close to the actual value of 224¬∞! I don‚Äôt think they are meant to be used from the ground.

The neat navigational map is from https://skyvector.com!

I spent ages trying to build my own decoder in GNU Radio. But I wasn‚Äôt familiar with it at all, and I eventually gave up. Still, that seems to be the software you wanna learn for tasks like these!

By the way, how the ground stations work is fascinating: In my case, it‚Äôs a ‚ÄúDoppler VOR‚Äù: It transmits a static frequency via amplitude modulation, and adds another signal that moves around in circles, so you get a Doppler frequency shift.

If you compare the two, you can calculate the angle!

47: See how low you can go in the frequency spectrum

This was a fun exploration: What‚Äôs the lowest-frequency broadcast I can receive?

The RTL-SDR Blog V4 stick I‚Äôm using has a neat feature ‚Äì a built-in ‚Äúupconverter‚Äù, which is enabled automatically when you try to listen to frequencies below what the chipset supports. This allows it to receive down to ~500 kHz!

The first stations that are comprehensible started at 1 MHz for me.

48: See how high you can go in the frequency spectrum

The chipset in my SDR stick go up to maximum frequency of 1766 MHz. It seems pretty quiet up there, probably because I lack proper antennas. I found these three lines in an amateur band, but they probably originate from the stick itself, or another device.

So the highest-frequency thing I‚Äôve received is ADS-B at 1090 MHz (see entry #5)! üéâ

We‚Äôve been over this. Not allowed in Germany. Don‚Äôt do it. ‚õî

But if you‚Äôre in the US, anyone can purchase a marine radio, and even use it to transmit! :D

Just now, I was wondering whether there are any Android apps for controlling SDRs.

And it turned out, the software I liked best that week, SDR++, had an Android version since a couple of weeks! \o/

So now I could go track down the source of some of these strange signals! :3

And with that, ‚Ä¶ ü•Å ‚Ä¶ I was officially done with my ‚Äú50 things to do with a software defined radio‚Äù! üéâ

This were seven very intense days, where I learned a lot of new things about radio waves and the many things they can be used for!

I was proud! I was tired! I was amazed that all those things I received are all around us, everywhere, all at once ‚Äì if you know where to look. :O

Here‚Äôs some things that I haven‚Äôt tried or that haven‚Äôt worked:
‚Ä¢ Receiving digital voice modes (SDRangel should be able to do it, but I couldn‚Äôt figure it out)
‚Ä¢ Receive something from the ISS
‚Ä¢ Use the GRAVES radar to detect meteors (couldn‚Äôt detect it)

Also, doing things with Wi-Fi/Bluetooth/Zigbee could be fun, but I‚Äôd need a more expensive receiver for those frequencies.

So, was this project in fact a gateway drug to getting an amateur radio license?

Yeah, probably. I‚Äôd love to transmit something and experiment more! :D

In Germany, a new license class will be introduced in summer 2024, that‚Äôll allow you to send on the 10-meter, 2-meter and 70-cm bands (the ‚ÄúN class‚Äù).

In fact, there‚Äôs a really good German online course that teaches you everything you need to know: 50ohm.de

Highly recommended, even if you‚Äôre not planning on getting a license.

Finally, thanks to Piko, Chris, and Cqoicebordel for proof-reading this blog post! <3

You can add your comment in the Fediverse! Alternatively, drop me a mail at mail@blinry.org. Also, you can support me on Patreon or subscribe to my newsletter

Next up...

Story 4: How to make the Framework Desktop run even quieter
Not so long ago, the compact, small form factor PC segment witnessed a significant refreshment with the launch of the Framework desktop PC. If you've missed it and are wondering why this mini-PC is considered so special, it's because it was the first desktop PC to utilise the AMD Ryzen AI Max APU, a processor previously exclusive to laptops.

The AMD Ryzen AI Max processor stands out for its exceptional speed and integrated graphics performance, frequently surprising users with its gaming capabilities, even on demanding titles. Users often highlight its powerful integrated GPU, which can leverage a massive memory pool (up to 96GB for AI tasks), allowing it to efficiently handle complex AI and deep learning workloads. The raw performance delivered by this chip makes it a worthwhile choice for intensive tasks and creative workflows.

As a collaborator and partner on the Framework Desktop mini-PC project, our first steps involved integrating our NF-A12x25 fan and a fan duct. This way, we could significantly reduce system noise levels while ensuring safe operating temperatures ‚Äì you can read more about this here. But can the Framework Desktop be made even quieter? We wanted to leave no stone unturned to find out, so we took it a step further by trying to integrate our signature Noctua fan grill design that debuted on the Seasonic Prime TX-1600 Noctua Edition power supply.

It must be noted that customer safety and EMC requirements for the mini PC, a standalone electrical item, differ from those for hardware components (such as the PSU) designed to be inside a PC case. The safety standard suggests that ventilation openings on case side panels need to be less than 5mm in diameter. To comply with safety regulations, we created an updated version of the original fan grille as implemented on the Seasonic Prime TX 1600 Noctua Edition power supply featuring more struts and a smaller opening size, ensuring full adherence to these standards. To complement the new grille design, we have also designed a custom, funnel shaped fan duct that makes maximum use of the outermost openings of the custom side panel.

In combination, the custom side panel and duct design provided a massive noise reduction compared to the stock configuration, particularly in lower fan speed ranges. We have measured around 7 dB(A) lower noise levels at around 50% fan speed, and up to 5 dB(A) lower at higher fan speeds, when compared at the same APU operating temperature.

While the custom side panel with our signature Noctua grill as well as the custom fan duct are not slated for mass production at this point, we are more than happy to share the 3D CAD files for everyone who is looking to make their Framework Desktop run even quieter.

Both the custom side panel and the customised fan ductare available to download at Printables.com for you to 3D-print at home:
‚Ä¢ Custom fan duct to make best use of the custom side panel

Fortunately, the quality of 3D printing technologies has advanced so much that you can end up with a nice, clean side panel, which will additionally optimise the sound profile, or bring your APU temperatures down significantly.

In addition to redesigning and testing the Noctua fan grill, we also evaluated various other scenarios. These included replacing the NF-A12x25 with its G2 variant and incorporating an additional 8cm fan for exhaust purposes. The findings of these tests may prove surprising. The additional NF-A8 PWM fan, which was added as an exhaust fan at the front of the case, yielded slightly lower temperatures, but at the expense of extra noise emission, so it‚Äôs not a setup that we would recommend from a performance-to-noise efficiency point of view.

While upgrading to an NF-A12x25 G2 does provide some acoustical benefits compared to the stock setup (around 1 to 1.5 dB(A) lower noise levels at the same temperatures), its maximum speed is limited to 1800 RPM, so it cannot match the performance headroom of the 2400 RPM HS-PWM version of the NF-A12x25 that is supplied with the Framework Desktop PC. This high-speed version of the ‚ÄûG1‚Äú fan is a safeguard that ensures the system can maintain full performance in worst-case conditions with high ambient temperatures. In other words, we would only recommend upgrading to the NF-A12x25 G2 if you seek to lower noise levels as much as possible and if you are willing to sacrifice the maximum performance headroom in worst-case scenarios that the G1 HS-PWM fan provides.

In summary, after a lot of simulation, experimenting and testing, we can conclude that not all tweaks to the Framework Desktop‚Äôs cooling setup make sense. However, if you have access to a 3D printer, swapping the stock side panel and fan duct for the custom designed ones can help to make your unit run significantly quieter.

Next up...

Story 5: Denmark close to wiping out cancer-causing HPV strains after vaccine roll-out
News from the lab
HPVSpotlight on HPV
Denmark close to wiping out leading cancer-causing HPV strains after vaccine roll-out
A nationwide study suggests infections with human papillomavirus (HPV) types 16 and 18 have been virtually eliminated since vaccination began in 2008 ‚Äì protecting even unvaccinated women.
2
September
2025
3
min read
by
Linda Geddes
Republish this article
Republish this article
Disclaimer
If you would like to republish this article, please follow these steps: use the HTML below; do not edit the text; include the author‚Äôs byline; credit VaccinesWork as the original source; and include the page view counter script.
<article>
<h1>
<span>Denmark close to wiping out leading cancer-causing HPV strains after vaccine roll-out</span>
</h1>
<div>
<div><p>A nationwide study suggests infections with human papillomavirus (HPV) types 16 and 18 have been virtually eliminated since vaccination began in 2008 ‚Äì protecting even unvaccinated women.</p></div>
</div>
<ul>
<li>
<b>2
September
2025</b>
</li>
<li>
<b class="me-2">by</b>
<span>
<a href="https://www.gavi.org/vaccineswork/authors/linda-geddes" hreflang="en">Linda Geddes</a>
</span>
</li>
</ul>
<div>
<div>
<div>
<div>
<div>
<p>¬†</p><p>Denmark has effectively eliminated infections with the two biggest cancer-causing strains of human papillomavirus (HPV) since the vaccine was introduced in 2008, data suggests.</p><p>The research, published in <a href="https://www.eurosurveillance.org/content/10.2807/1560-7917.ES.2025.30.27.2400820"><em>Eurosurveillance</em></a>, could have implications for how vaccinated populations are screened in the coming years ‚Äì particularly as people increasingly receive vaccines that protect against multiple high-risk types of HPV virus.</p><aside class="pquote"><blockquote><p>Before vaccination, the prevalence of HPV16/18 was between 15 and 17%, which has decreased in vaccinated women to less than one percent by 2021.</p></blockquote><p>- Researchers, <a href="https://www.eurosurveillance.org/content/10.2807/1560-7917.ES.2025.30.27.2400820"><em>Eurosurveillance</em></a></p></aside><h3>Deadly cancer</h3><p>After breast cancer, cervical cancer is the most common type of cancer among women aged 15 to 44 years in Europe, and human papillomavirus (HPV) is the leading cause.</p><p>At least 14 high-risk types of the virus have been identified, and before Denmark introduced the HPV vaccine in 2008, HPV types 16 and 18 accounted for around three quarters (74%) of cervical cancers in the country.</p><p>Initially, girls were offered a vaccine that protected against four types of HPV: 16, 18, plus the lower risk types 6 and 11. However, since 2017, Danish girls have been offered a vaccine that protects against nine types of HPV ‚Äì including those accounting for approximately 90% of cervical cancers.¬† ¬†¬†</p><div><h4>Have you read?</h4><ul><li><a href="https://www.gavi.org/vaccineswork/six-ways-hpv-vaccine-saving-lives">Six ways the HPV vaccine is saving lives</a></li><li><a href="https://www.gavi.org/vaccineswork/hpv-poses-ongoing-threat-older-women-study-finds">HPV poses ongoing threat for older women, study finds</a></li></ul></div><p>To better understand the impact that these vaccination programmes have had on HPV prevalence as vaccinated girls reach cervical screening age (23 to 64 years in Denmark), Dr Mette Hartmann Nonboe at Zealand University Hospital in Nyk√∏bing Falster and colleagues analysed up to three consecutive cervical cell samples collected from Danish women between 2017 and 2024, when they were 22 to 30 years of age.</p><p>‚ÄúIn 2017, one of the first birth cohorts of women in Denmark who were HPV-vaccinated as teenage girls in 2008 reached the screening age of 23 years,‚Äù Nonboe explained.</p><p>‚ÄúCompared with previous generations, these women are expected to have a considerably lower risk of cervical cancer, and it is pertinent to assess [their] future need for screening.‚Äù</p><h3>High-risk HPV elimination</h3><p>The research found that infection with the high-risk HPV types (HPV16/18) covered by the vaccine has been almost eliminated.¬†</p><p>‚ÄúBefore vaccination, the prevalence of HPV16/18 was between 15 and 17%, which has decreased in vaccinated women to less than one percent by 2021,‚Äù the researchers said.</p><p>In addition, prevalence of HPV types 16 and 18 in women who had not been vaccinated against HPV was five percent. This strongly suggests that the vaccine has reduced the circulation of these HPV types in general population, to the extent that even unvaccinated women are now less likely to be infected with them ‚Äì so called ‚Äúpopulation immunity‚Äù ‚Äì the researchers said.</p><aside class="pquote"><blockquote><p>In 2017, one of the first birth cohorts of women in Denmark who were HPV-vaccinated as teenage girls in 2008 reached the screening age of 23 years. Compared with previous generations, these women are expected to have a considerably lower risk of cervical cancer.</p></blockquote><p>- Mette Hartmann Nonboe, Researcher, <a href="https://www.eurosurveillance.org/content/10.2807/1560-7917.ES.2025.30.27.2400820"><em>Eurosurveillance</em></a></p></aside><p>Despite this good news, roughly one third of women screened during the study period still had infection with high-risk HPV types not covered by the original vaccines ‚Äì and new infections with these types were more frequent among vaccinated women, compared to unvaccinated ones.</p><p>This is expected to fall once girls who received the more recent ‚Äònine-valent‚Äô vaccine reach screening age. At this point, the screening guidelines should potentially be reconsidered, Nonboe and colleagues said.</p>
</div>
</div>
</div>
</div>
</div>
<p>
<a target="_blank" rel="noopener noreferrer" href="https://www.gavi.org/vaccineswork/denmark-close-wiping-out-leading-cancer-causing-hpv-strains-after-vaccine-roll-out">Original article</a>
</p><p>This article was originally published on
<a target="_blank" rel="noopener noreferrer" href="https://www.gavi.org/vaccineswork">VaccinesWork</a>
</p><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-M7H54VD');</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M7H54VD" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript></article>
Copy html
A plaster is applied to the injection site of a young woman after receiving a vaccination. Credit: Centers for Disease Control and Prevention (CDC)
Denmark has effectively eliminated infections with the two biggest cancer-causing strains of human papillomavirus (HPV) since the vaccine was introduced in 2008, data suggests.The research, published in Eurosurveillance, could have implications for how vaccinated populations are screened in the coming years ‚Äì particularly as people increasingly receive vaccines that protect against multiple high-risk types of HPV virus.Deadly cancerAfter breast cancer, cervical cancer is the most common type of cancer among women aged 15 to 44 years in Europe, and human papillomavirus (HPV) is the leading cause.At least 14 high-risk types of the virus have been identified, and before Denmark introduced the HPV vaccine in 2008, HPV types 16 and 18 accounted for around three quarters (74%) of cervical cancers in the country.Initially, girls were offered a vaccine that protected against four types of HPV: 16, 18, plus the lower risk types 6 and 11. However, since 2017, Danish girls have been offered a vaccine that protects against nine types of HPV ‚Äì including those accounting for approximately 90% of cervical cancers.¬† ¬†¬†Have you read?Six ways the HPV vaccine is saving livesHPV poses ongoing threat for older women, study findsTo better understand the impact that these vaccination programmes have had on HPV prevalence as vaccinated girls reach cervical screening age (23 to 64 years in Denmark), Dr Mette Hartmann Nonboe at Zealand University Hospital in Nyk√∏bing Falster and colleagues analysed up to three consecutive cervical cell samples collected from Danish women between 2017 and 2024, when they were 22 to 30 years of age.‚ÄúIn 2017, one of the first birth cohorts of women in Denmark who were HPV-vaccinated as teenage girls in 2008 reached the screening age of 23 years,‚Äù Nonboe explained.‚ÄúCompared with previous generations, these women are expected to have a considerably lower risk of cervical cancer, and it is pertinent to assess [their] future need for screening.‚ÄùHigh-risk HPV eliminationThe research found that infection with the high-risk HPV types (HPV16/18) covered by the vaccine has been almost eliminated.¬†‚ÄúBefore vaccination, the prevalence of HPV16/18 was between 15 and 17%, which has decreased in vaccinated women to less than one percent by 2021,‚Äù the researchers said.In addition, prevalence of HPV types 16 and 18 in women who had not been vaccinated against HPV was five percent. This strongly suggests that the vaccine has reduced the circulation of these HPV types in general population, to the extent that even unvaccinated women are now less likely to be infected with them ‚Äì so called ‚Äúpopulation immunity‚Äù ‚Äì the researchers said.Despite this good news, roughly one third of women screened during the study period still had infection with high-risk HPV types not covered by the original vaccines ‚Äì and new infections with these types were more frequent among vaccinated women, compared to unvaccinated ones.This is expected to fall once girls who received the more recent ‚Äònine-valent‚Äô vaccine reach screening age. At this point, the screening guidelines should potentially be reconsidered, Nonboe and colleagues said.
More from Linda Geddes
View all
Can MMR vaccines cause autism?
10
Sep
2025
9
min read
verified
MeaslesMumpsRubellaVaccine safety
Malaria education could reduce cases by over a fifth, study finds
8
Sep
2025
3
min read
verified
MalariaSpotlight on malaria
Denmark close to wiping out leading cancer-causing HPV strains after vaccine roll-out
2
Sep
2025
3
min read
verified
HPVSpotlight on HPV
Scientists make breakthrough towards ‚Äòuniversal‚Äô antiviral drugs
29
Aug
2025
3
min read
verified
Research summary
Recommended for you
Senegal rolls out ‚Äúsix-in-one‚Äù jab, making life easier for families and health workers
16
Sep
2025
5
min read
verified
Child dies from complications of measles years after infection ‚Äì SSPE explained
16
Sep
2025
3
min read
verified
A Nigerian father of 21 children decided to give vaccines a shot. His whole community benefited
15
Sep
2025
4
min read
verified
No girl left behind: HPV vaccines bring hope to Pakistan
15
Sep
2025
5
min read
verified
There‚Äôs a new outbreak of Ebola in Africa. Here‚Äôs what you need to know
15
Sep
2025
4
min read
verified
From data to decisions: why political economy analysis matters for reaching zero-dose children
12
Sep
2025
6
min read
verified

Next up...

Story 6: Doom crash after 2.5 years of real-world runtime confirmed on real hardware
DOOM crash after 2.5 years of real-world runtime confirmed on real hardware
A discussion group for the most cursed software ideas out there
Post Reply
Print view
1 post
‚Ä¢ Page 1 of 1
minki
Site Admin
Posts: 13 Joined: 2024-03-25
Location: /bin/bash
Contact:
Contact minki
DOOM crash after 2.5 years of real-world runtime confirmed on real hardware
Quote
Post
by minki ¬ª 2025-09-16
Two and a half years ago, I started my now longest real-world software experiment. I had read an article about how DOOMs engine works and noticed how a variable for tracking the demo kept being incremented even after the next demo started. This variable was compared with a second one storing its previous value. The issue here being, each incrementation would cause the variable to slowly get closer to an overflow, realistically this would never happen in a normal scenario, although it got me curious on just how long it would take until the game would crash due to this.
I did a few calculations, I don't remember the specifics of it sadly as it has been over two years since that point and I sadly did not document it back then (or I did, but on a partition I no longer have access to) but I remember having gotten roughly 2 1/2 years of possible runtime before an overflow. Obviously, I wanted to know if this would actually happen in the real game on real hardware.
So I set up DOOM on a small PDA, powered through a DIY 18650 based UPS which itself was connected to the USB port of my router for a constant 5V supply. I left the system running and mostly forgot about it.
... Until today when I noticed a pop-up appearing on the device, not long ago from posting this to the board. The game had crashed, only hours after the two and a half year mark, proving that the variable did indeed overflow and cause the expected hard crash of the game:
IMG_20250916_224553.jpg (81.99 KiB) Viewed 11946 times
~-~-~ MSD - Making your old devices useful again since 2022! ~-~-~
Top
Post Reply
Print view
1 post
‚Ä¢ Page 1 of 1
Return to ‚ÄúSoftware shenanigans‚Äù
Jump to
Cursed Hardware
Software shenanigans

Next up...

Story 7: A dumb introduction to z3
Recently I have come across a nice article: Many Hard Leetcode Problems are Easy Constraint Problems, and I figured, I really should learn how to use these things! What else do I really have to do? I have had use for solvers (or as they are commonly called: theorem provers) In a previous article, but then I tried to prove the things with good old algorithms. I looked at at the time, but found the whole concept a bit too opaque. Now however, it seemed a bit easier to get into.

To be clear, as of writing these words, I have only been looking at reading material for two days. I am in no way an expert, and I have not written anything more complex than a solver for the change counter problem (the first example in the article listed above). So I am writing this really knowing nothing about the underlying theory, theorem provers, or whatever the hell "unification" is. There is a good chance you know more about this than I do.

There are bindings in many popular languages. I will be using 's Rust bindings, because I am more comfortable in Rust than, say, Python or JavaScript. The examples I worked with to understand however, can be found in two nice documents:
‚Ä¢ First is in Python

Solvers are a class of .. tools? libraries? where you input some rules and constraints and have the tool just .. solve it for you. It is not going to be a faster or more optimized solution than a custom made algorithm, but it is much easier to change the rules on the fly.

There are many real world uses. They are often used for scheduling and resource allocation problems. Consider the common scenario of a school schedule: Mary cannot work on Tuesdays because she needs to take care of her father; John lives far so he cannot give classes before 10; Class 3-A is full of nerds so their math hours are double; city council regulates no outdoor activity after 12; Susan and Sarah hate each other so you should not have them teach the same class; etc. You can either have two teachers work on it for a week, or just pop it in a solver!

The MiniZinc homepage (another popular solver) has a couple of nice examples: a seating chart, rostering, vehicle routing, grid coloring.

On that note, you might wonder: why did I go with when MiniZinc has a more colorful homepage and is actually referenced by the article linked at the start of this article? The answer is because has bindings in Rust. That is pretty much it.

Documentation on and its API use a lot of jargon, which makes the whole thing really difficult to wade into without a previous background. I will explain things as I understand when I get to them, but two things really stand out.

The first is the word . You see this in the context of arrays and function declarations (we will get to those, I hope). But it has nothing to do with .. well .. sorting. is just the jargon word for types.

The second one is constants. They are not what a normal person would call constants: they are actually the knobs the solvers use to solve problems. There are two types of constants: free, which are what one would call variables; and interpreted, which is when you'd type an integer literal and clever type machinations turns it into a constant in the solver.

Note that also solvers do not work within the regular type system of the programming language. They have their own types (sorry, sorts), and operations that may or may not map nicely to the language's types and operations. Much of the actual code you are writing is about expressing things in the target solver's language. uses a language called "SMT-LIB2" (henceforth called ), apparently. And you can actually write your constraints immediately in said language and have the library consume it. Much of what the bindings is take your code and translate internally to this language before feeding it to the solver.

Let's start with what might be the simplest, dumbest equation. Solve for :

Yes, a child (literally) can solve this. But it is nice. Here is the Rust program to solve it.

This prints out the solution. equals three. Who would have guessed?

The Rust bindings have some nice ergonomics here. You can simply do and it would do all the bookkeeping behind closed doors to transform the (and the ) into an interpreted constant and have them inserted into the internal model.

The reason you have to pass in a string in is that this is the name given to the variable in . It does not have to be , it can be anything. Why do the bindings not autogenerate the name for you? Who knows.

If you print the solver (as in ), you will get the following output in the .

Note that the variable you declared is declared as a function. A free constant is basically a function that takes no input and gives an output (here of sort ). The solver finds which version of the function satisfies the assertions. This also explains the arrow in earlier. evaluates to 3.

In school, jumping from solving equations with a single variable to equations with two variables was a real jump on complexity. Everything was doubled! Here is a pair of equations we will try to solve next:

Here is the program. I am going to print the result of first, tho. I just made up those numbers!

This prints out the following:

Oh it is . Unsatisfiable. Bummer. This means this cannot be solved as defined.

Let's try changing the type to . The type does not have the same nice ergonomics as apparently, so the code will look slightly uglier. This is the new updated code.

Excellent! Using and printing the model as before gives us the following answer, presented as a nice rational number.

To actually extract the values programmatically, instead of debug printing , requires some song and dance with the API, but it is simple, really. This is what it would look like.

As I am sure you know from your high school math, some equations have multiple solutions. Here is a simple one.

The Rust bindings have a nice method for getting multiple solutions out of a solver, simply called . It works similarly to above, and takes the same parameters with the same output. Here is the complete program. (I am going back to because I am not cool enough for numbers.)

I am not clear really on how to get multiple solutions with the regular followed by method, but this one is easy enough to use. Also, some problems might have infinitely many solutions, so it is advisable to use with the iterator. To demonstrate, I will use the circle equation.

Here is the straightforward script followed by the printed out result. Note that the API creates a unique name for every invocation.

This goes without saying, but if I used the type in this example it would generate infinite solutions.

The Coin Change problem is a simple one: given a list of denominations and a total, find the smallest number of coins that add up to said total. Emphasis on smallest. Unlike previous problems, this is an optimization problem. We are looking for a solution that satisfies specific criteria instead of just a solution. Conveniently enough, provides an object which we can use to optimize.

Let us set up the parameters of the problem in plain language. The denominations we have are 1, 5, and 10. We need to give 37 money in the least amount of coins. This is simple enough that we can know the solution is three 10 coins, one 5 coin, and two 1 coins. Let's see if we can get the same result. As usual, code followed by output:

Oops. This cannot be right.

I do not really understand why the answer is so nonsensical here. The problem is that really spans the entire natural integers range, so it is accounting for negative amounts of coins. This still does not explain how the optimal solution given is 37 coins. (If you can explain, please let me know.)

The solution for this is to constrain the amount of coins to be non-negative. So let's do that. Add these assertions somewhere before , and Bob's your uncle.

That's more like it. Now let's try with different denominations. Something like 10. 9, and 1. Note that the optimal solution for 37 would be: one 10 coin, three 9 coins, and no 1 coins. The greedy solution would fail to catch that. Here is the output of printing the optimizer and the result.

Currently, the total 37 is hardcoded. But what if I want the answers for a number of different totals? Thankfully, you do not need to build the optimizer from scratch for every total. Instead, use the magical functions and . The first one essentially creates a bookmark in the stack of assertions. The second removes everything above said bookmark, and the bookmark. It is simple really. Here are the solutions from 30 to 39, because why not.

Here is the full . I will spare you the output.

Note that the and API is available for as well. At any rate, back to solving.

This is a significant jump in complexity, so bear with me. We are going to solve a Sudoku. So let's write the constraints first. We can use Rust's arrays or to organize our s and check their constraints. First, this is the puzzle we are solving:

I will forgo the steps to turn that into a . Instead the code below will get that info from a function.

Printing the solver after each step lets you debug whether you have your constraints correctly. The printout is over 200 lines long, so let's skip that. All we have to do next is to check the value of each cell in .

And this prints out the result. You can verify for yourself whether this is correct or not. Maybe try other puzzles. Or add more constraints. You can even try the Miracle Sudoku.

One thing of note here: which is how dumb the solver is. Note that if you print out the solver, there is no notion of rows and columns and squares. It does not know any Sudoku tricks like X-wings and what have you. All the data is organized on the Rust side of things, and what is given to the solver is "these two variables cannot be the same" over and over and over again. And it just .. tells you what the rest of them are.

Another thing that is not obvious at first glance, is that it does not check if there is a unique solution. The puzzle may be badly constructed and have multiple solutions, and it will happily give you one, or two, or how many you ask for. It does, however, check if it is unsolvable!

One of the famous examples of using solvers in production is .. layouting. You have a number of elements and you want to arrange them on a page, or a browser window, or whatever. So let's do a rudimentary version of that.

The page we are layouting has an arbitrary size of 190mm width by 270mm tall. We are to put three boxes on the page of varying sizes and rules. I am just spitballing the sizes here: first box is 105mm by 140mm; second is 85 by 135, third is 120 by 110. They should not overlap, and like .. that's it?

This prints out this neat solution:

Obviously, all these examples are rather simple. Figuring out how to model the problem in the form of boolean rules and constraints is almost all the challenge. There are some limitations, too: cannot solve equations of the sort ; it cannot call external functions to get values (though there are ways to work around that).

There is plenty of stuff I have not shown. s, which are nothing like programming language arrays and more mappings from one domain to another. , bit vectors, which allow bitwise operations on their values like and and bit shifting. (Which are the key to solving the Hanging Gardens Problem.) s and s and s and regexes and stuff I have not really looked into. Unfortunately, most resources on the web are a bit heavy on theory, and are not targeted to stupid coders like myself.

Next up...

Story 8: CubeSats are fascinating learning tools for space
These are CubeSats. Satellites that are going to space‚Äîor at least, the ones I have here are prototypes. But these have one thing in common: they're all powered by either a Raspberry Pi, or a microcontroller.
There are already Pis in space, like on Mark Rober's SatGus, on GASPACS, and the Astro Pis on the Space station. Another Pi is going up this weekend, which is why I'm posting this today. I'll get to that one, but I wanted to spend some time talking about two things that fascinate me: Raspberry Pis, and putting them space!
In this post, I'll cover:
What is a CubeSat
Who builds and launches CubeSats
How you can build your own CubeSat
Then for a bonus, in today's video, I interviewed two people helping students launch SilverSat into space (this weekend!), and a YouTuber who I've learned a lot from about track satellites (including CubeSats) from your own backyard!
The rest of this post contains a lightly-edited transcript of the video above.
So let's dive in.
What's a CubeSat?
What's a CubeSat? Well, it's in the name‚Äîit's a satellite that's a cube!
But they don't have to be a cube, these smallest ones are '1U', or 10 x 10 x 10 centimeters. You can also find 2U CubeSats, like the taller Build a CubeSat, which is 20 centimeters tall. (Well, technically the current prototype is 1.5U).
SatGus, Mark Rober's satellite taking space selfies, is a whopping 12U! They needed all that extra space to fit a phone, a mechanism to deploy the phone, a camera to take the selfie, a Raspberry Pi to control the phone, and redundant systems for everything. They've already taken thousands of selfies, and SatGus has me beat. My best Pi might get to 3.4 Gigahertz, but the Pi on SatGus is whizzing through space at almost 17,000 miles per hour. That's 7,570 meters per second for everyone else in the world.
But back to CubeSats. Having standards means you can build off existing work for the hard things, like a space-rated Aluminum frame, or the complex EPS, or Electrical Power System board.
Then you can add in custom parts, like a Pi to run experiments, a communications board with antennas and radios, cameras, sensors, and more!
And these cubesats have normal screw-on antennas, but the way these things are deployed, you only get 10x10x10 centimeters‚Äîyou can't have an antenna poking out the top. So they use cool things like flexible tape antennas that pop out once your CubeSat deploys.
What else makes CubeSats cool?
Well, how about price? In the old days, you had to have like $10 million to build a satellite, and $60+ million to launch it into space.
Today, you can build a space-ready CubeSat using a few thousand dollars of parts. Then you can launch it on a rideshare for... well, $85 grand. Which is a lot, but it's not $60 million-a-lot.
So most of us won't be launching one of these things into space, unless maybe you can get a grant. But that doesn't mean they're not useful to us.
Who builds CubeSats?
Like with many projects, I love these things for the challenge, the way they break some of my assumptions, like working with Raspberry Pis.
If you're building a device that's less than 2 kilograms, has 1.8W of maximum continuous power draw, and needs to be operated remotely‚Äîeven for just a month‚Äîyou're immediately going to change your assumptions about how you build things.
I would hack Home Assistant onto a mini PC to monitor some sensors if I was feeling lazy‚Äîbut that Mini PC would use an order of magnitude too much power for a CubeSat (much less the internal volume it would occupy).
On CubeSats, every millimeter, and every milliAmp has to be accounted for.
So to me, CubeSats are like Swiss watches of modern electronics. How many sensors can you fit in one? How much throughput can you get on a tiny radio with a small antenna? Can you get enough power out of tiny solar cells to keep the main flight computer working? How do you control thermals without air? How do you design it so it can recover from a complete power loss?
Every step of the way there are challenges; and that's before we even launch one! Someone who I think illustrates this best is Manuel, with his Build a CubeSat project. He's working on this Cubesat:
He did a weather balloon launch this year, and he's documenting everything on YouTube.
His first launch had many small problems. But also great learning, especially around redundancy and how to get the thing off the launch stand without problems.
And you're not only dealing with hardware, but also with software. And software that, at its core, has to be remotely accessed. And not only remote, but also wireless, meaning anyone else on earth within range can access it too.
So how do you keep it secure? That's something Tim from Ethos Labs is also dealing with with this, his T.E.M.P.E.S.T. CubeSat:
This thing is actually made to be not secure. It has intentional vulnerabilities, and he uses those to teach people different ways to make their CubeSats more secure.
You have complex hardware, running in limited space, with limited power and communications, and you want cram in as much functionality as possible.
Do you see where I'm going with this? That kind of problem is perfect for the microcontrollers and low-power SBCs that I love testing and playing with every day.
Except instead of me worrying about something consuming 10 watts, these guys are looking at a power budget of one watt. Or less!
These problems are hard. And not everyone has the patience for a completely custom project like Build a CubeSat, so there are also some small companies building kits to help you learn all these lessons with a little less stress.
Like what hardware do you need for a 100% self-contained CubeSat? And how do you get it certified for flight on a SpaceX rocket?
Your own CubeSat
Well, I'll quickly cover two products that are meant for like STEM classroom education, one from the lower end, and one that's based on a CubeSat that just flew this summer.
The first one is the MySat Kit, that you can buy from MySat in Ukraine. It comes with a board powered by an ESP32 with a camera, light sensors, an LED, gyroscope, accelerometer, barometer, clock, and a few other boards. And these are all off-the-shelf components you can buy replacements for or use 'em with other hardware, like a Raspberry Pi.
The way it's put together won't hold up on a rocket launch, but it's not meant for that. It's meant to show you how it's built, how you can communicate with it, and that sort of thing.
It took like an hour to build, and once I put it together I tried flashing the flight control firmware with my Mac... but I ran into some issues with Arduino IDE, and that's a me problem and not so much a MySat problem. Plus the team behind it has a whole war going on that they've been dealing with, so I'll be patient and try getting it going later.
The MySat goes from like $130 for a basic kit where you 3D print your own frame, or up to $300 for a full kit including deployable solar panels.
On the higher end, there's RASCube, and Edward Robinson, the 21 year old founder of Robinson Space, sent it over after he saw me posting about CubeSats online.
The RASCube comes from Australia, and Edward's mission is to teach students about space through hands-on building.
I just built this LS version of the cube last week; it's the little brother to their V2 design, which flew in space on a Falcon 9 rocket earlier this year.
Like MySat, you build the kit with an EPS board for power, a computer board with all the controls, and a radio board that ties in GPS and radio comms.
The RASCubes are a bit more expensive, coming in at around $430 each for the LB, and $600 each for the full aluminum V2s. But the price tag on that also covers full lesson plans and resources for teachers.
I love these things‚Äîall the people I've talked to on this journey are motivated by the same thing: learning about space, electronics, and integrating hardware in a new way, and sharing what they learn with others, especially students.
CubeSat T.E.M.P.E.S.T. and Build a CubeSat
Like take Build a Cubesat. For that project, everything is open source hardware, and every part of the journey is being documented on YouTube.
One thing I learned from the first flight test was how weird it is to have your Pi go from like overheating on the ground, to getting really cold as it goes higher, but then overheating again in the upper atmosphere because there's not enough air to dissipate heat!
You start to realize some of the crazy physical conditions you'll deal with on orbit.
Back down to earth, though, for CubeSat Tempest: the whole reason this exists is to help people learn why security is important, even for a tiny CubeSat. More importantly, Tim Fowler's course teaches people how to secure things like uplinks (see: the ground station pictured above) and flight control systems.
There are so many people like Tim, who work in their free time to try to teach about space, or engineering, or just small slices of things like security, using these tactile little cubes you can build and put next to your laptop on a desk.
It's crazy to think we're to a point where students can build these things, write flight control software, and even launch 'em into space!
And that brings me to SilverSat.
SilverSat
There's another CubeSat with a Raspberry Pi onboard, and it's launching NET Sunday, at 6:11 p.m. Eastern time, aboard a Falcon 9 rocket. What does NET mean? Well, as I found out when I visited Florida this summer, that means "No Earlier Than", and in spaceflight, many things delay launches.
The students who built SilverSat are no strangers to delays‚Äîthey were originally supposed to see their CubeSat launch earlier this year, but the cargo module they were on got damaged during transport, and that delayed them for months.
I got to talk to two of the adults guiding the students on their first space launch, and I discussed the history of the project (it started up in 2017), how they are supported by NASA's CubeSat Launch Initiative, the importance of amateur radio for CubeSats, and why they chose a Raspberry Pi Zero for their onboard computer.
That interview is tucked away in the last half of the video at the top of this post.
Tracking Satellites from your backyard
Also in that video, I spoke to Gabe from saveitforparts, and he mentioned it's not that difficult to listen in on satellites on orbit‚Äîincluding amateur CubeSats!
SilverSat will be broadcasting SSDV (Slow-Scan Digital Video) at set times, and the schedule for that should be posted on their website.
Check out the video embedded in this post (near the top), or Gabe's own channel for ideas for tracking satellites. It can be done with under $100 of equipment (usually just an SDR and a cheap antenna).
Infectious Enthusiasm for Learning (and Teaching)
I feel like a broken record, but one thing I love, talking to anyone in the CubeSat community is this sense of infectious enthusiasm. And I was going to cut this video out for time, but watching it back, I realized other people would probably enjoy Tim showing off some neat CubeSats in his personal collection as much as I did. So I put up some bonus content on my second channel, Level 2 Jeff; you can watch another 8 minutes of CubeSat hardware below:
Thank you to everyone who taught me about CubeSats for this video and blog post.
Further reading
I took down Starlink (but I haven't cancelled)
Starlink's current problem is capacity
Getting my amateur radio (ham) license
cubesat
space
raspberry pi
video
youtube
microcontroller
satellite
Add new comment
Comments
–∞ –ø—Ä–∏—á—ë–º —Ç—É—Ç —É–∫—Ä–∞–∏–Ω–∞?
Reply
The MySat is a project that is built by a small group in Ukraine.
Reply

Next up...

Story 9: Slow Social Media
Slow social media
16 Sep, 2025
People often assume that I hate social media. And they'd be forgiven for believing that, since I am overtly critical of current social media platforms and the effects they have on individuals and society; and deleted all of my social media accounts back in 2019.
However, the underlying concept of social media is something I resonate with: Stay connected with the people you care about.
It's just that the current form of social media is bastardised, and not social at all. Instead of improving relationships and fostering connection, they're advertisement-funded content mills which are explicitly designed and continually refined to keep you engaged, lonely, and unhappy. And once TikTok figured out that short-form video with a recommendation engine is digital crack, all other social media platforms quickly sprang into action to copy their secret sauce.
Meta basically turned Instagram and Facebook from 'connecting with friends' into 'doom-scrolling random content'. Even Pinterest is starting to look like TikTok! They followed user engagement, but not the underlying preferences of their users. I posit that any for-profit social media will eventually degrade into recommendation media over time.
I don't think most people using these platforms understand that they are the product. Instagram isn't built for you. It's built for marketers. It's built for celebrities to capitalise on their audiences. It's built for politicians and their cronies to sway sentiment. It's built to be as addictive as possible, and to capitalise on your insecurity and uncomfortability.
Imagine that, society and politics are on the rocks all so a fitness influencer can sell you their "Abs in 30 days" training program.
These platforms are the quintessential poster child for late-stage capitalism.
Okay, now that we've established what the problems with current platforms are‚Äîwhat would a non-evil social media platform look like?
I'd love to see everyone running a blog, and subscribing to the people they care about via RSS. But unfortunately this doesn't scale since it requires effort to put your thoughts down in writing longer than 255 characters. I have many friends who don't even know I have a blog, or what an RSS reader is.
So while everyone blogging may be the ideal we can aspire to, let's design a hypothetical social media platform that takes the good aspects of current social media, while creating pro-social incentives.
The platform should be about:
Keeping up with friends, family, and other acquaintances
Connection (but, you know, real connection)
Improving relationships
Thoughtful engagement
The platform should NOT be about:
Collecting followers
Self-promotion
Advertising and marketing
Short-form video and media entertainment
In my opinion, as soon as there is the ability for commercial interests to take hold, they will. The "follow" mechanism is a key part of that. I propose that instead of followers we should regress back to the "friend" or "connection" system where there is a symmetric relationship where both people have to agree to the connection. There is no good reason to have "followers" on a platform that is trying to improve relationships. "Following" is purely for egotistical or financial gain and breeds parasocial relationships.
I think there should also be a reasonable cap on the number of connections that can be made. Something like 300 friends sounds right. Any more than that and you're a collector, and not using the platform to foster connection.
This feature alone already removes 90% of the marketing interests in the platform. Do you want to make a connection, but are maxed out? You'll need to unfriend someone first.
The second necessary element would be a chronological feed with posts from your connections. This turns the platform from an engagement engine into a way to keep up with what everyone else is doing, but importantly, gives you a natural "end" to the feed when you start seeing posts you've already viewed. This way when you start scrolling there's an explicit stopping point.
Relatedly, pagination is more humane than infinite-scroll since it gives users a natural breathing point where they can decide whether they want to keep going. Infinite-scroll is such an obvious user-trap, and I view any website doing it as not having its user's best interests at heart.
And finally, there should be a reasonable cap on the number of times a user can post per day. Roughly 5 times per day feels like the upper threshold of what you can post while being intentional about what it is you're posting. This will keep the feed reasonably populated without one or two people completely overwhelming it.
The rest of the platform can be optimised to be as easy-to-use as possible. Something like a mixture between the old Instagram and Twitter, with comments and reactions. No reels or any other recommendation system to keep people engaged to death. And no analytics, since that would be optimising for reach and engagement instead of the stated goal of connection.
Do I expect a platform like this to succeed? Not by the traditional metrics of success. In the real world it would exist alongside the content mills, which are exciting by design and competing for attention. Could it work in niche groups, or amongst intentional people who are sick of the current platforms? Maybe.
Naturally, a project like this would have to be funded somehow, and unfortunately very few people are willing to pay $5 per month for software services, even if they use it every day. However, I suspect that a social media platform like this would be manageable enough that a small team could run it fairly cheaply and profitably if they're creative. Perhaps with nothing but donations.
Who will create this egalitarian social media? Not me, that's for sure. I already have my fair share of work moderating the Bear discovery feed, to the extent I've had to bring on a second moderator (hello Sheena!) to keep it clean of spam and other nasty things that free services on the internet attract.
That being said, I would love to see something like this. I'd love to be able to stay connected with friends and family abroad without having my attention sold to the highest bidder.
If anyone is working on something like this, I'd be happy to consult.

Next up...

Story 10: Shai-Hulud malware attack: Tinycolor and over 40 NPM packages compromised
The NPM ecosystem is facing another critical supply chain attack. The popular @ctrl/tinycolor package, which receives over 2 million weekly downloads, has been compromised along with more than 40 other packages across multiple maintainers. This attack demonstrates a concerning evolution in supply chain threats - the malware includes a self-propagating mechanism that automatically infects downstream packages, creating a cascading compromise across the ecosystem. The compromised versions have been removed from npm.

In this post, we'll dive deep into the payload's mechanics, including deobfuscated code snippets, API call traces, and diagrams to illustrate the attack chain. Our analysis reveals a Webpack-bundled script (bundle.js) that leverages Node.js modules for reconnaissance, harvesting, and propagation; targeting Linux/macOS devs with access to NPM/GitHub/cloud creds.

To help the community respond to this incident, StepSecurity hosted a Community Office Hour on September 16th at 1 PM PT. The recording is available here: https://www.youtube.com/watch?v=D9jXoT1rtaQ

The attack unfolds through a sophisticated multi-stage chain that leverages Node.js's process.env for opportunistic credential access and employs Webpack-bundled modules for modularity. At the core of this attack is a ~3.6MB minified bundle.js file, which executes asynchronously during npm install. This execution is likely triggered via a hijacked postinstall script embedded in the compromised package.json.

The malware includes a self-propagation mechanism through the NpmModule.updatePackage function. This function queries the NPM registry API to fetch up to 20 packages owned by the maintainer, then force-publishes patches to these packages. This creates a cascading compromise effect, recursively injecting the malicious bundle into dependent ecosystems across the NPM registry.

The malware repurposes open-source tools like TruffleHog to scan the filesystem for high-entropy secrets. It searches for patterns such as AWS keys using regular expressions like AKIA[0-9A-Z]{16}. Additionally, the malware dumps the entire process.env, capturing transient tokens such as GITHUB_TOKEN and AWS_ACCESS_KEY_ID.

For cloud-specific operations, the malware enumerates AWS Secrets Manager using SDK pagination and accesses Google Cloud Platform secrets via the @google-cloud/secret-manager API. The malware specifically targets the following credentials:

The malware establishes persistence by injecting a GitHub Actions workflow file (.github/workflows/shai-hulud-workflow.yml) via a base64-encoded bash script. This workflow triggers on push events and exfiltrates repository secrets using the expression ${{ toJSON(secrets) }} to a command and control endpoint. The malware creates branches by force-merging from the default branch (refs/heads/shai-hulud) using GitHub's /git/refs endpoint.

The malware aggregates harvested credentials into a JSON payload, which is pretty-printed for readability. It then uploads this data to a new public repository named via the GitHub /user/repos API.

The entire attack design assumes Linux or macOS execution environments, checking for os.platform() === 'linux' || 'darwin'. It deliberately skips Windows systems. For a visual breakdown, see the attack flow diagram below:

The compromise begins with a sophisticated minified JavaScript bundle injected into affected packages like @ctrl/tinycolor. This is not rudimentary malware but rather a sophisticated modular engine that uses Webpack chunks to organize OS utilities, cloud SDKs, and API wrappers.

The payload imports six core modules, each serving a specific function in the attack chain.

This module calls getSystemInfo() to build a comprehensive system profile containing platform, architecture, platformRaw, and archRaw information. It dumps the entire process.env, capturing sensitive environment variables including AWS_ACCESS_KEY_ID, GITHUB_TOKEN, and other credentials that may be present in the environment.

The AWS harvesting module validates credentials using the STS AssumeRoleWithWebIdentityCommand. It then enumerates secrets using the @aws-sdk/client-secrets-manager library.

The module handles errors such as DecryptionFailure or ResourceNotFoundException silently through decorateServiceException wrappers. It targets all AWS regions via endpoint resolution.

The GCP module uses @google-cloud/secret-manager to list secrets matching the pattern projects//secrets/. It implements pagination using nextPageToken and returns objects containing the secret name and decoded payload. The module fails silently on PERMISSION_DENIED errors without alerting the user.

This module spawns TruffleHog via child_process.exec('trufflehog filesystem / --json') to scan the entire filesystem. It parses the output for high-entropy matches, such as AWS keys found in ~/.aws/credentials.

The NPM propagation module parses NPM_TOKEN from either ~/.npmrc or environment variables. After validating the token via the /whoami endpoint, it queries /v1/search?text=maintainer:${username}&size=20 to retrieve packages owned by the maintainer.

This creates a cascading effect where an infected package leads to compromised maintainer credentials, which in turn infects all other packages maintained by that user.

The GitHub backdoor module authenticates via the /user endpoint, requiring repo and workflow scopes. After listing organizations, it injects malicious code via a bash script (Module 941).

Here is the line-by-line bash script deconstruction:

This workflow is executed as soon as the compromised package create a commit with it, which immediately exfiltrates all the secrets.

The malware builds a comprehensive JSON payload containing system information, environment variables, and data from all modules. It then creates a public repository via the GitHub /repos POST endpoint using the function . The repository is public by default to ensure easy access for the command and control infrastructure.

We are observing hundreds of such public repositories containing exfiltrated credentials. A GitHub search for "Shai-Hulud" repositories reveals the ongoing and widespread nature of this attack, with new repositories being created as more systems execute the compromised packages.

This exfiltration technique is similar to the Nx supply chain attack we analyzed previously, where attackers also used public GitHub repositories to exfiltrate stolen credentials. This pattern of using GitHub as an exfiltration endpoint appears to be a preferred method for supply chain attackers, as it blends in with normal developer activity and bypasses many traditional security controls.

These repositories contain sensitive information. The public nature of these repositories means that any attacker can access and potentially misuse these credentials, creating a secondary risk beyond the initial compromise.

The attack employs several evasion techniques including silent error handling (swallowed via catch {} blocks), no logging output, and disguising TruffleHog execution as a legitimate "security scan."

We analyzed the malicious payload using StepSecurity Harden-Runner in a GitHub Actions workflow. Harden-Runner successfully flagged the suspicious behavior as anomalous. The public insights from this test reveal how the payload works:
‚Ä¢ The compromised package made unauthorized API calls to during the npm install process
‚Ä¢ These API interactions were flagged as anomalous since legitimate package installations should not be making such external API calls

These runtime detections confirm the sophisticated nature of the attack, with the malware attempting credential harvesting, self-propagation to other packages, and data exfiltration - all during what appears to be a routine package installation.

The following indicators can help identify systems affected by this attack:

Use these GitHub search queries to identify potentially compromised repositories across your organization:

Replace with your GitHub organization name and use the following GitHub search query to discover all instance of in your GitHub environment.

To find malicious branches, you can use the following Bash script:
‚Ä¢ The malicious bundle.js file has a SHA-256 hash of:

The following packages have been confirmed as compromised:

If you use any of the affected packages, take these actions immediately:

The malware harvests credentials from multiple sources. Rotate ALL of the following:
‚Ä¢ Any credentials stored in AWS Secrets Manager or GCP Secret Manager

Since the malware specifically targets AWS Secrets Manager and GCP Secret Manager, you need to audit your cloud infrastructure for unauthorized access. The malware uses API calls to enumerate and exfiltrate secrets, so reviewing audit logs is critical to understanding the scope of compromise.

Start by examining your CloudTrail logs for any suspicious secret access patterns. Look specifically for BatchGetSecretValue, ListSecrets, and GetSecretValue API calls that occurred during the time window when the compromised package may have been installed. Also generate and review IAM credential reports to identify any unusual authentication patterns or newly created access keys.

For Google Cloud Platform, review your audit logs for any access to the Secret Manager service. The malware uses the @google-cloud/secret-manager library to enumerate secrets, so look for unusual patterns of secret access. Additionally, check for any unauthorized service account key creation, as these could be used for persistent access.
‚Ä¢ Check deploy keys and repository secrets for all projects
‚Ä¢ Set up alerts for any new npm publishes from your organization

The following steps are applicable only for StepSecurity enterprise customers. If you are not an existing enterprise customer, you can start our 14 day free trial by installing the StepSecurity GitHub App to complete the following recovery step.

The NPM Cooldown check automatically fails a pull request if it introduces an npm package version that was released within the organization‚Äôs configured cooldown period (default: 2 days). Once the cooldown period has passed, the check will clear automatically with no action required. The rationale is simple - most supply chain attacks are detected within the first 24 hours of a malicious package release, and the projects that get compromised are often the ones that rushed to adopt the version immediately. By introducing a short waiting period before allowing new dependencies, teams can reduce their exposure to fresh attacks while still keeping their dependencies up to date.



Here is an example showing how this check protected a project from using the compromised versions of packages involved in this incident:

We have added a new control specifically to detect pull requests that upgraded to these compromised packages. You can find the new control on the StepSecurity dashboard.

Use StepSecurity Harden-Runner to detect compromised dependencies in CI/CD

StepSecurity Harden-Runner adds runtime security monitoring to your GitHub Actions workflows, providing visibility into network calls, file system changes, and process executions during CI/CD runs. Harden-Runner detects the compromised nx packages when they are used in CI/CD. Here is a sample Harden-Runner insights page demonstrating this detection:

If you're already using Harden-Runner, we strongly recommend you review recent anomaly detections in your Harden-Runner dashboard. You can get started with Harden-Runner by following the guide at https://docs.stepsecurity.io/harden-runner.

The StepSecurity Threat Center provides comprehensive details about this @ctrl/tinycolor compromise and all 40+ affected packages. Access the Threat Center through your dashboard to view IOCs, remediation guidance, and real-time updates as new compromised packages are discovered. Threat alerts are automatically delivered to your SIEM via AWS S3 and webhook integrations, enabling immediate incident response when supply chain attacks occur. Our detection systems identified this attack within minutes of publication, providing early warning before widespread exploitation.

Use StepSecurity Artifact Monitor to detect software releases outside of authorized pipelines

StepSecurity Artifact Monitor provides real-time detection of unauthorized package releases by continuously monitoring your artifacts across package registries. This tool would have flagged this incident by detecting that the compromised versions were published outside of the project's authorized CI/CD pipeline. The monitor tracks release patterns, verifies provenance, and alerts teams when packages are published through unusual channels or from unexpected locations. By implementing Artifact Monitor, organizations can catch supply chain compromises within minutes rather than hours or days, significantly reducing the window of exposure to malicious packages.

Learn more about implementing Artifact Monitor in your security workflow at https://docs.stepsecurity.io/artifact-monitor.
‚Ä¢ The npm security team and package maintainers for their swift response to this incident.
‚Ä¢ @franky47, who promptly notified the community through a GitHub issue

The collaborative efforts of security researchers, maintainers, and community members continue to be essential in defending against supply chain attacks.

Next up...

Story 11: Waymo has received our pilot permit allowing for commercial operations at SFO
All systems go at SFO! Waymo has received our pilot permit allowing for commercial operations at San Francisco International Airport.

We‚Äôll partner with SFO to prepare our operations at the airport in phases, beginning with employee testing soon ahead of welcoming Bay Area riders. Pickups and dropoffs will initially start at SFO‚Äôs Kiss & Fly area ‚Äì a short AirTrain ride from the terminals ‚Äì with the intention to explore other locations at the airport in the future.

This is a major milestone that strengthens Waymo‚Äôs impact on the region and offers residents and visitors an innovative way to travel. With years of experience serving riders at Phoenix Sky Harbor (PHX) and operations beginning soon at San Jose Mineta International Airport (SJC), we‚Äôre accelerating our efforts to serve more airports in more cities as we scale.

‚ÄúAcross San Francisco, we are expanding safe, reliable, and modern transportation options‚Äîsupporting our city‚Äôs economic comeback, boosting our tourism industry, and connecting residents and visitors to everything our city has to offer,‚Äù said Mayor Lurie. ‚ÄúWe announced in March that we wanted visitors to be able to ride in a Waymo as soon as they arrived in San Francisco, and today, we are taking another important step to get there.‚Äù

‚ÄúBringing the Waymo experience to San Francisco International Airport is about more than just a ride‚Äîit‚Äôs about providing a safe, reliable, magical way for Bay Area residents and global visitors to connect with the places and people that matter most,‚Äù said Tekedra Mawakana, co-CEO, Waymo. ‚ÄúWe‚Äôre grateful for the partnership with SFO and the vision of Mayor Lurie in making this a reality.‚Äù

Next up...

Story 12: In Defense of C++
Dayvi Schuster
12 min read
Tuesday, September 9, 2025
In Defense of C++
Why C++ remains a powerful and relevant programming language in today's tech landscape.
The Reputation of C++
C++ has often and frequently been criticized for its complexity, steep learning curve, and most of all for its ability to allow the developers using it to not only shoot themselves in the foot, but to blow off their whole leg in the process. But do these criticisms hold up under scrutiny?
Well, in this blog post, I aim to tackle some of the most common criticisms of C++ and provide a balanced perspective on its strengths and weaknesses.
C++ is ‚ÄúComplex‚Äù
C++ is indeed a complex language, with a vast array of features and capabilities. For any one thing you wish to achieve in C++, there are about a dozen different ways to do it, each with its own trade-offs and implications. So, as a developer, how are you to know which approach is the best one for your specific use case? Surely you have to have a deep understanding of the language to make these decisions, right?
Not really‚Ä¶ I mean, don‚Äôt get me wrong, it helps, but it‚Äôs not a hard requirement. Premature optimization is the root of all evil, and in C++, you can write perfectly fine code without ever needing to worry about the more complex features of the language. You can write simple, readable, and maintainable code in C++ without ever needing to use templates, operator overloading, or any of the other more advanced features of the language.
There‚Äôs this idea that for everything you want to do in any programming language, you need to use the most efficient and correct approach possible. Python has this with their pythonic way of doing things, Java has this, C# has this, and Go has this. Heck, even something as simple as painting HTML onto a browser needs to be reinvented every couple of years and argued about ad nauseam. Here‚Äôs the thing, though, in most cases, there is no one right way to do something. The hallowed ‚Äúbest approach‚Äù is often just a matter of personal or team preference. The idea that if you just write your code in the ‚Äúbest‚Äù and correct way, you‚Äôll never need to worry about maintaining it is just plain wrong.
Don‚Äôt worry so much about using the ‚Äúbest‚Äù approach; worry more about writing code that is easy to read and understand. If you do that, you‚Äôll be fine.
C++ is ‚ÄúOutdated‚Äù
C++ is very old, in fact, it came out in 1985, to put it into perspective, that‚Äôs 4 years before the first version of Windows was released, and 6 years before the first version of Linux came out, or to drive the point even further home, back when the last 8-bit computer was released. So yes, C++ is quite old by any standard. But does that make it outdated?
Hell no it‚Äôs not like C++ has just been sitting around unchanged from its 1985 release. C++ has been actively developed and improved upon for over 40 years now, with new features and capabilities being added all the time. The most recent version of the C++ standard, C++20, was released in 2020 and introduced a number of new features and improvements to the language. C++23 has introduced significant enhancements, particularly in the standard library and constexpr capabilities. Notably, concepts, ranges, and coroutines have been expanded, bringing modern programming paradigms to C++ and making the language more powerful and expressive th
an ever before.
But Dave, what we mean by outdated is that other languages have surpassed C++ and provide a better developer experience.
Matter of personal taste, I guess, C++ is still one of the most widely used programming languages with a huge ecosystem of libraries and tools. It‚Äôs used in a wide range of applications, from game development to high-performance computing to embedded systems. Many of the most popular and widely used software applications in the world are written in C++.
I don‚Äôt think C++ is outdated by any stretch of the imagination; you have to bend the definition of outdated quite a bit to make that claim.
C++ is ‚ÄúUnsafe‚Äù
Ah, finally, we get to the big one, and yes, I will draw comparisons to Rust as it‚Äôs the ‚Äúmemory safe‚Äù language that a lot of people claim will or should replace C++.
In fact, let‚Äôs get the main point out of the way right now.
Rewrites of C++ codebases to Rust always yield more memory-safe results than before.
Countless companies have cited how they improved their security or the amount of reported bugs or memory leaks by simply rewriting their C++ codebases in Rust.
Now is that because of Rust? I‚Äôd argue in some small part, yes. However, I think the biggest factor is that any rewrite of an existing codebase is going to yield better results than the original codebase.
When you rewrite a codebase, you have the opportunity to rethink and redesign the architecture, fix bugs, and improve the overall quality of the code. You get to leverage all the lessons learned from the previous implementation, all the issues that were found and fixed, and you already know about. All the headaches that would be too much of a pain to fix in the existing codebase, you can just fix them in the new one.
Imagine if you will that you‚Äôve built a shed, it was a bit wobbly, and you didn‚Äôt really understand proper wood joinery when you first built it, so it has a few other issues, like structural integrity and a leaky roof. After a few years, you build a new one, and this time you know all the mistakes you made the first time around, so you build it better, stronger, and more weatherproof. In the process, you decide to replace the materials you‚Äôve previously used, say for example, instead of using maple, you opt for oak. Is it correct to say that the new shed is better only because you used oak instead of maple? Or is that a really small part of the overall improvement?
That‚Äôs how I feel when I see these companies claim that rewriting their C++ codebases in Rust has made them more memory safe. It‚Äôs not because of Rust, it‚Äôs because they took the time to rethink and redesign their codebase and implemented all the lessons learned from the previous implementation.
But that does not deny the fact that C++ is unsafe.
Yes, C++ can be unsafe if you don‚Äôt know what you‚Äôre doing. But here‚Äôs the thing: all programming languages are unsafe if you don‚Äôt know what you‚Äôre doing. You can write unsafe code in Rust, you can write unsafe code in Python, you can write unsafe code in JavaScript.
Memory safety is just one aspect of safety in programming languages; you can still write unsafe code in memory-safe programming languages. Just using Rust will not magically make your application safe; it will just make it a lot harder to have memory leaks or safety issues.
The term ‚Äúunsafe‚Äù is a bit too vague in this context, and I think it‚Äôs being used as a catch-all term, which to me reeks of marketing speak.
Can C++ be made safer?
Yes, C++ can be made safer; in fact, it can even be made memory safe. There are a number of libraries and tools available that can help make C++ code safer, such as smart pointers, static analysis tools, and memory sanitizers. Heck, if you wish, you can even add a garbage collector to C++ if you really want to(please don‚Äôt).
But the easiest and most straightforward way to make C++ safer is to simply learn about smart pointers and use them wherever necessary. Smart pointers are a way to manage memory in C++ without having to manually allocate and deallocate memory. They automatically handle the memory management for you, making it much harder to have memory leaks or dangling pointers. This is the main criticism of C++ in the first place.
C++ is Hard to Read
Then don‚Äôt write it that way. C++ is a multi-paradigm programming language; you can write procedural code, object-oriented code, functional code, or a mix of all three. You can write simple and readable code in C++ if you want to. You can also write complex and unreadable code in C++ if you want to. It‚Äôs all about personal or team preference.
Here‚Äôs a rule of thumb I like to follow for C++: make it look as much like C as you possibly can, and avoid using too many advanced features of the language unless you really need to. Use smart pointers, avoid raw pointers, and use the standard library wherever possible.
You can do a heck of a lot of programming by just using C++ as you would C and introducing complexity only when you really need to.
But doesn‚Äôt that defeat the whole purpose of C++? Why not just use C then?
C++ is a superset of C you can write C code in C++, and it will work just fine. C++ adds a lot of features and capabilities to C. If you were to start with C, then you are locked with C, and that‚Äôs fine for a lot of cases, don‚Äôt get me wrong, but C++ gives you the option to use more advanced features of the language when you need them. You can start with C and then gradually introduce C++ features as you need them. You don‚Äôt have to use all the features of C++ if you don‚Äôt want to.
Again, going back to my shed analogy, if you build a shed out of wood, you can always add a metal roof later if you need to. You don‚Äôt have to build the whole shed out of metal if you don‚Äôt want to.
C++ has a confusing ecosystem
C++ has a large ecosystem built over the span of 40 years or so, with a lot of different libraries and tools available. This can make it difficult to know which libraries and tools to use for a specific task. But this is not unique to C++; every programming language has this problem.
Again, the simple rule of thumb is to use the standard library wherever possible; it‚Äôs well-maintained and has a lot of useful features. For other tasks like networking or GUI development, there are a number of well-known libraries that are widely used and well-maintained. Do some research and find out which libraries are best suited for your specific use case.
Avoid boost like the plague. Boost is a large collection of libraries that are widely used in the C++ community. However, many of the libraries in boost are outdated and no longer maintained. They also tend to be quite complex and difficult to use. If you can avoid using boost, do so.
Unless you are writing a large and complex application that requires the specific features provided by Boost, you are better off using other libraries that are more modern and easier to use. Do not add the performance overhead and binary size bloat of Boost to your application unless you really need to.
C++ is not a good choice for beginners
Programming is not a good choice for beginners, woodworking is not a good choice for beginners, and car mechanics is not a good choice for beginners. Programming is hard; it takes time and effort to learn, as all things do. There is no general language that is really good for beginners; everything has its trade-offs.
Fact is, if you wanna get into something like systems programming or game development then starting with Python or JavaScript won‚Äôt really help you much. You will eventually need to learn C or C++.
If your goal is to become a web developer or data scientist, then start with Python or JavaScript.
If you just want a job in the programming industry, I don‚Äôt know, learn Java or C#, both great languages that get a lot of undeserved hate, but offer a lot of job opportunities.
Look, here‚Äôs the thing: if you‚Äôre just starting out in programming, yeah, it‚Äôs gonna be hard no matter what language you choose. I‚Äôd actually argue that starting with C or C++ is far better than starting with something that obscures a lot of the underlying concepts of programming, I‚Äôd argue further that by starting with Python or Javascript you are doing yourself a disservice in the long run and trading off the pain of learning something when your understanding of a topic is still fresh and malleable for the pain of learning something later when you have a lot more invested in your current understanding of programming.
But hey, that‚Äôs just my opinion.
C++ vs Rust: Friends or Rivals?
Rust has earned a lot of love in recent years, and for good reason. It takes memory safety seriously, and its borrow checker enforces discipline that C++ often leaves to the programmer. That said, Rust is still building its ecosystem, and the learning curve can feel just as steep ‚Äî just in different ways. C++ may not prevent you from shooting yourself in the foot, but it gives you decades of battle-tested tooling, compilers, and libraries that power everything from Chrome to Unreal Engine. In practice, many teams use Rust and C++ together rather than treating them as enemies. Rust shines in new projects where safety is the priority, while C++ continues to dominate legacy systems and performance-critical domains.
Is C++ Still Used in 2025?
The short answer: absolutely. Despite the constant chatter that it‚Äôs outdated, C++ remains one of the most widely used languages in the world. Major browsers like Chrome and Firefox are still written in it. Game engines like Unreal run on it. Automotive systems, financial trading platforms, and even AI frameworks lean heavily on C++ for performance and control. New standards (C++20, C++23) keep modernizing the language, ensuring it stays competitive with younger alternatives. If you peel back the layers of most large-scale systems we rely on daily, you‚Äôll almost always find C++ humming away under the hood.
Conclusion
C++ is a powerful and versatile programming language that has stood the test of time. While it does have its complexities and challenges, it remains a relevant and widely used language in today‚Äôs tech landscape.
With the right approach and mindset, C++ can be a joy to work with and can yield high-performance and efficient applications. So next time you hear someone criticize C++, take a moment to consider the strengths and capabilities of this venerable language before dismissing it outright.
Hope you enjoyed this blog post. If you did, please consider sharing it with your friends and colleagues. If you have any questions or comments, please feel free to reach out to me on Twitter.
Keep reading If you liked that one here's another: Zig Error Handling Or explore one of these tags
Browse by tag:
c
programming
software development
zig
sys programming
low level
frontend
webdev
opinion
jotai
react
react-query
javascript
astro
typescript
state
react hooks
fullstack
github
next.js
performance
career
open source
git
auth
context
csharp
dotnet
Wanna show support?
If you find my sporadic thoughts and ramblings helpful.
You can buy me a coffee if you feel like it.
It's not necessary but it's always appreciated. The content will always
stay free regardless.
Buy me a Coffee

Next up...

Story 13: Wait4X allows you to wait for a port or a service to enter the requested state
Wait4X helps you wait for services (databases, APIs, message queues, etc.) to be ready before your app or script continues. It's ideal for:
‚Ä¢ CI/CD pipelines: Ensure dependencies are up before tests run

After installing, jump to Quick Start to try it out!

üêπ Go Install (for Go users) You can install Wait4X directly from source using Go (requires Go 1.16+): This will place the binary in your or directory.

Get started in seconds! After installing, try these common checks:

For more, see Usage Examples or Detailed Usage.

Here are some of the most useful Wait4X commands. Click the links for more details!
‚Ä¢ TCP: Wait for a port to be available
‚Ä¢ HTTP: Wait for a web endpoint with status code and body check
‚Ä¢ Redis: Wait for Redis and check for a key
‚Ä¢ Reverse check (wait for port to be free):



See Detailed Usage for advanced options and more protocols.

Wait for an HTTP(S) endpoint to be ready, with flexible validation options.

Check for various DNS record types and values.
‚Ä¢ Check if a table exists: If you need to specify a schema for the table existence check, you can use the connection string parameter, for example:
‚Ä¢ Check for key with value (regex):
‚Ä¢ Wait for multiple Kafka brokers (cluster) to be ready:

Wait for a shell command to succeed or return a specific exit code.

See Advanced Features for timeout, retry, backoff, and parallel/reverse checking options.

Control how long Wait4X waits and how often it checks.
‚Ä¢ Waits up to 30 seconds before giving up.

Retry with increasing delays for more efficient waiting (useful for slow-starting services).
‚Ä¢ Enable exponential backoff: Doubles the wait time between retries, up to 30 seconds.

Wait for a service to become unavailable (e.g., port to be free, service to stop).
‚Ä¢ Wait for a port to become free:
‚Ä¢ Wait for a service to stop: Use for shutdown/cleanup workflows or to ensure a port is not in use.

Run a command after a successful check (great for CI/CD or startup scripts).
‚Ä¢ Chain multiple commands: Automate your workflow after dependencies are ready.

Wait for multiple services at once (all must be ready to continue).
‚Ä¢ Check several services in parallel: Use for microservices, integration tests, or complex startup dependencies.

See CLI Reference for all available flags and options.

For more detailed examples with complete code, see the examples/pkg directory. Each example is in its own directory with a runnable file.

Wait4X provides a flexible CLI with many commands and options. Here is a summary of the main commands and global flags. For the most up-to-date and detailed information, use the built-in help:

Each command supports its own set of flags. See examples above or run for details.

For a full list of commands and options, run:

We welcome contributions of all kinds! Whether you want to fix a bug, add a feature, improve documentation, or help others, you're in the right place.
‚Ä¢ Make your changes (add tests if possible)

For more details, see CONTRIBUTING.md (if available).
‚Ä¢ ‚≠ê Star the repo to support the project!

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.

The project logo is based on the "Waiting Man" character (Zhdun) and is used with attribution to the original creator.

Next up...

Story 14: AMDVLK (AMD Open Source Driver For Vulkan) project is discontinued
GPUOpen-Drivers
/
AMDVLK
Public
Notifications
You must be signed in to change notification settings
Fork
166
Star
1.9k
AMDVLK open-source project is discontinued
#416
jinjianrong
announced in
Announcements
AMDVLK open-source project is discontinued
#416
jinjianrong
Sep 15, 2025
¬∑
6 comments
¬∑
6 replies
Return to top
Discussion options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
edited
Uh oh!
There was an error while loading. Please reload this page.
jinjianrong
Sep 15, 2025
Maintainer
-
In a move to streamline development and strengthen our commitment to the open-source community, AMD is unifying its Linux Vulkan driver strategy and has decided to discontinue the AMDVLK open-source project, throwing our full support behind the RADV driver as the officially supported open-source Vulkan driver for Radeon‚Ñ¢ graphics adapters.
This consolidation allows us to focus our resources on a single, high-performance codebase that benefits from the incredible work of the entire open-source community. We invite developers and users alike to utilize the RADV driver and contribute to its future.
Learn more about RADV: Mesa RADV Documentation
Contribute to the code: Mesa GitLab Repository
We are excited about this focused path forward and are committed to the continued success of open-source Vulkan on Radeon.
Beta
Was this translation helpful?
Give feedback.
22
You must be logged in to vote
üëç
37
üëé
2
üéâ
7
‚ù§Ô∏è
3
üöÄ
42
üëÄ
16
All reactions
üëç
37
üëé
2
üéâ
7
‚ù§Ô∏è
3
üöÄ
42
üëÄ
16
Replies:
6 comments
¬∑
6 replies
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
Snektron
Sep 15, 2025
-
What does this mean for AMDPAL and in particular ROCm on Windows?
Beta
Was this translation helpful?
Give feedback.
4
You must be logged in to vote
All reactions
2 replies
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
Flakebi
Sep 15, 2025
-
The code base is still used for Windows, so I‚Äôd expect PAL and HIP on Windows to continue working as it is today.
Beta
Was this translation helpful?
Give feedback.
All reactions
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
Snektron
Sep 15, 2025
-
But will those repositories remain open source & under development?
Beta
Was this translation helpful?
Give feedback.
All reactions
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
R1chterScale
Sep 15, 2025
-
To clarify, does this mean that AMD will be allocating more engineering resources towards RADV?
Beta
Was this translation helpful?
Give feedback.
28
You must be logged in to vote
All reactions
0 replies
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
Leopard1907
Sep 15, 2025
-
Why pal also got archieved?
Beta
Was this translation helpful?
Give feedback.
4
You must be logged in to vote
All reactions
0 replies
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
Gen5551
Sep 15, 2025
-
Hi all. that is, AMDVLK is no more? and then which Vulkan driver should I use?
Beta
Was this translation helpful?
Give feedback.
0
You must be logged in to vote
üëé
1
All reactions
üëé
1
3 replies
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
R1chterScale
Sep 15, 2025
-
It explicitly says which one to use in the announcement. Read it
Beta
Was this translation helpful?
Give feedback.
All reactions
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
Gen5551
Sep 15, 2025
-
Okay..
Beta
Was this translation helpful?
Give feedback.
All reactions
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
dotjson01
Sep 16, 2025
-
I a using Nvidia üê• on ubuntu
Is it okay?
Beta
Was this translation helpful?
Give feedback.
All reactions
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
Laguna720
Sep 15, 2025
-
Is this a sign that RADV might come to Windows in the future? The Vulkan driver for Windows is based on AMDVLK...
Beta
Was this translation helpful?
Give feedback.
8
You must be logged in to vote
All reactions
1 reply
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
GreyXor
Sep 16, 2025
-
https://www.phoronix.com/news/RADV-Windows-XDC-2024
https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/29945
https://indico.freedesktop.org/event/6/contributions/296/attachments/227/307/2024-10-10%20XDC%202024%20-%20A%20Little%20Windows%20with%20your%20Mesa.pdf
Beta
Was this translation helpful?
Give feedback.
üöÄ
5
All reactions
üöÄ
5
Comment options
Uh oh!
There was an error while loading. Please reload this page.
Quote reply
Hammerklavier-cn
Sep 16, 2025
-
How about the pro vulkan driver?
Beta
Was this translation helpful?
Give feedback.
1
You must be logged in to vote
All reactions
0 replies
Sign up for free
to join this conversation on GitHub.
Already have an account?
Sign in to comment
Category
üì£
Announcements
Labels
None yet
10 participants

Next up...

Story 15: Launch HN: Rowboat (YC S24) ‚Äì Open-source IDE for multi-agent systems
‚ö° Build AI agents instantly with natural language | üîå Connect tools with one-click integrations | üìÇ Power with knowledge by adding documents for RAG | üîÑ Automate workflows by setting up triggers and actions | üöÄ Deploy anywhere via API or SDK



 ‚òÅÔ∏è Prefer a hosted version? Use our cloud to starting building agents right away!

To add tools, RAG, more LLMs, and triggers checkout the Advanced section below.

Chat with the copilot to build a meeting-prep workflow, then add a calendar invite as a trigger. Watch the full demo here.

Chat with the copilot to build a customer support assistant, then connect your MCP server, and data for RAG. Watch the full demo here.

Chat with the copilot to build a personal assistant. Watch the full demo here.
‚Ä¢ Native RAG Support: Enable file uploads and URL scraping with Rowboat's built-in RAG capabilities ‚Äì see RAG Guide.
‚Ä¢ Custom LLM Providers: Use any LLM provider, including aggregators like OpenRouter and LiteLLM - see Using more LLM providers.
‚Ä¢ Tools & Triggers: Add tools and event triggers (e.g., Gmail, Slack) for automation ‚Äì see Tools & Triggers.
‚Ä¢ API & SDK: Integrate Rowboat agents directly into your app ‚Äì see API & SDK docs.

Refer to Docs to learn how to start building agents with Rowboat.

Next up...

Story 16: How Container Filesystem Works: Building a Docker-Like Container from Scratch
This tutorial was prepared for you by the iximiuz Labs team. Tutorial
on¬† Linux,¬†Containers Published: Sep 12, 2025How Container Filesystem Works: Building a Docker-like Container From ScratchOne of the superpowers of containers is their isolated filesystem view -
from inside a container it can look like a full Linux distro, often different from the host.
Run docker run nginx, and Nginx lands in its familiar Debian userspace no matter what Linux flavor your host runs.
But how is that illusion built?In this post, we'll assemble a tiny but realistic, Docker-like container using only stock Linux tools: unshare, mount, and pivot_root.
No runtime magic and (almost) no cut corners.
Along the way, you'll see why the mount namespace is the bedrock of container isolation,
while other namespaces, such as PID, cgroup, UTS, and even network, play rather complementary roles.By the end - especially if you pair this with the container networking tutorial -
you'll be able to spin up fully featured, Docker-style containers using nothing but standard Linux commands.
The ultimate goal of every aspiring container guru.PrerequisitesSome prior familiarity with Docker (or Podman, or the like) containersBasic Linux knowledge (shell scripting, general namespace awareness)Filesystem fundamentals (single directory hierarchy, mount table, bind mount, etc.)Visualizing the end resultThe diagram below shows what filesystem isolation looks like when Docker creates a new container.
It's all right if the drawing feels overwhelming.
With the help of the hands-on exercises in this tutorial,
we'll build a comprehensive mental model of how containers work,
so when we revisit the diagram in the closing section,
it'll look much more digestible.Click to enlargeWhat exactly does Mount Namespace isolate?Let's do a quick experiment.
In Terminal 1, start a new shell session in its own mount namespace:sudo unshare --mount bash
Copy to clipboardNow in Terminal 2, create a file somewhere on the host's filesystem:echo "Hello from host's mount namespace" | sudo tee /opt/marker.txt
Copy to clipboardSurprisingly or not, when you try locating this file in the newly created mount namespace using the Terminal 1 tab, it'll be there:cat /opt/marker.txt
Copy to clipboardSo what exactly did we just isolate with unshare --mount? ü§îThe answer is - a mount table. Here is how to verify it.
From Terminal 1, mount something:sudo mount --bind /tmp /mnt
Copy to clipboardüí° The above command uses a bind mount for simplicity,
but a regular mount (of a block device) would do, too.Now if you list the contents of the /mnt folder in Terminal 1,
you should see the files of the /tmp folder:ls -l /mnt
Copy to clipboardtotal 12
drwx------ 3 root root 4096 Sep 11 14:16 file1
drwx------ 3 root root 4096 Sep 11 14:16 file2
...
Copy to clipboardBut at the same time, the /mnt folder remained empty in the host mount namespace.
If you run the same ls command from Terminal 2,
you'll see no files:ls -lah /mnt
Copy to clipboardtotal 0
Copy to clipboardFinally, the filesystem "views" started diverging between namespaces.
However, we could only achieve it by creating a new mount point.Mount namespaces, visualizedFrom the mount namespace man page:Mount namespaces provide isolation of the list of mounts seen by the processes in each namespace instance.
Thus, the processes in each of the mount namespace instances will see distinct single directory hierarchies.Compare the mount tables by running findmnt from Terminal 1 and Terminal 2:Host namespaceNew namespaceTARGET
SOURCE
FSTYPE
OPTIONS
/
/dev/vda
ext4
rw,...
‚îú‚îÄ/dev
devtmpfs
devtmpfs
rw,...
‚îÇ ‚îú‚îÄ/dev/shm
tmpfs
tmpfs
rw,...
‚îÇ ‚îú‚îÄ/dev/pts
devpts
devpts
rw,...
‚îÇ ‚îî‚îÄ/dev/mqueue
mqueue
mqueue
rw,...
‚îú‚îÄ/proc
proc
proc
rw,...
‚îú‚îÄ/sys
sysfs
sysfs
rw,...
‚îÇ ‚îú‚îÄ/sys/kernel/security
securityfs
securityfs
rw,...
‚îÇ ‚îú‚îÄ/sys/fs/cgroup
cgroup2
cgroup2
rw,...
‚îÇ ...
‚îî‚îÄ/run
tmpfs
tmpfs
rw,...
‚îú‚îÄ/run/lock
tmpfs
tmpfs
rw,...
‚îî‚îÄ/run/user/1001
tmpfs
tmpfs
rw,...
Copy to clipboardTARGET
SOURCE
FSTYPE
OPTIONS
/
/dev/vda
ext4
rw,...
‚îú‚îÄ/dev
devtmpfs
devtmpfs
rw,...
‚îÇ ‚îú‚îÄ/dev/shm
tmpfs
tmpfs
rw,...
‚îÇ ‚îú‚îÄ/dev/pts
devpts
devpts
rw,...
‚îÇ ‚îî‚îÄ/dev/mqueue
mqueue
mqueue
rw,...
‚îú‚îÄ/proc
proc
proc
rw,...
‚îú‚îÄ/sys
sysfs
sysfs
rw,...
‚îÇ ‚îú‚îÄ/sys/kernel/security
securityfs
securityfs
rw,...
‚îÇ ‚îú‚îÄ/sys/fs/cgroup
cgroup2
cgroup2
rw,...
‚îÇ ...
‚îú‚îÄ/run
tmpfs
tmpfs
rw,...
‚îÇ ‚îú‚îÄ/run/lock
tmpfs
tmpfs
rw,...
‚îÇ ‚îî‚îÄ/run/user/1001
tmpfs
tmpfs
rw,...
‚îî‚îÄ/mnt
/dev/vda[/tmp] ext4
rw,...
Copy to clipboardIn hindsight, it should probably make sense -
after all, we are playing with a mount namespace
(and there is no such thing as filesystem namespaces, for better or worse).üí° Interesting fact: Mount namespaces were the first namespace type added to Linux, appearing in Linux 2.4, ca. 2002.üí° Pro Tip: You can quickly check the current mount namespace of a process using the following command:readlink /proc/$PID/ns/mnt
Copy to clipboardDifferent inode numbers in the output will indicate different namespaces.
Try running readlink /proc/self/ns/mnt from Terminal 1 and Terminal 2.What the heck is Mount Propagation?Before we jump to how exactly mount namespaces are applied by Docker an OCI runtime
(e.g., runc) to create containers,
we need to learn about one more important (and related) concept - mount propagation.‚ö†Ô∏è Make sure to exit the namespaced shell in Terminal 1
before proceeding with the commands in this section.If you tried to re-do the experiment from the previous section using the unshare() system call instead of the unshare CLI command,
the results might look different.unshare_lite.gopackage main
import "os"
import "os/exec"
import "syscall"
func main() {
if err := syscall.Unshare(syscall.CLONE_NEWNS); err != nil {
panic(err)
}
cmd := exec.Command("bash")
cmd.Stdin = os.Stdin
cmd.Stdout = os.Stdout
cmd.Stderr = os.Stderr
cmd.Env = os.Environ()
cmd.Run()
}
Copy to clipboardBuild the above improvised unshare_lite program with:go build -o unshare_lite unshare_lite.go
Copy to clipboardAnd run it from Terminal 1:sudo ./unshare_lite
Copy to clipboardThen mount something:mount --bind /tmp /mnt
Copy to clipboardThis time, the results of the ls -l /mnt will look identical in Terminal 1 and Terminal 2.
Thus, the mount namespace alone may not be enough to provide the mount table isolation.If you compare the mount tables by running findmnt from Terminal 1 and Terminal 2,
they will look the same:Host namespaceNew namespaceTARGET
SOURCE
FSTYPE
OPTIONS
/
/dev/vda
ext4
rw,...
‚îú‚îÄ/dev
devtmpfs
devtmpfs
rw,...
‚îÇ ‚îú‚îÄ/dev/shm
tmpfs
tmpfs
rw,...
‚îÇ ‚îú‚îÄ/dev/pts
devpts
devpts
rw,...
‚îÇ ‚îî‚îÄ/dev/mqueue
mqueue
mqueue
rw,...
‚îú‚îÄ/proc
proc
proc
rw,...
‚îú‚îÄ/sys
sysfs
sysfs
rw,...
‚îÇ ‚îú‚îÄ/sys/kernel/security
securityfs
securityfs
rw,...
‚îÇ ‚îú‚îÄ/sys/fs/cgroup
cgroup2
cgroup2
rw,...
‚îÇ ...
‚îú‚îÄ/run
tmpfs
tmpfs
rw,...
‚îÇ ‚îú‚îÄ/run/lock
tmpfs
tmpfs
rw,...
‚îÇ ‚îî‚îÄ/run/user/1001
tmpfs
tmpfs
rw,...
‚îî‚îÄ/mnt
/dev/vda[/tmp] ext4
rw,...
Copy to clipboardTARGET
SOURCE
FSTYPE
OPTIONS
/
/dev/vda
ext4
rw,...
‚îú‚îÄ/dev
devtmpfs
devtmpfs
rw,...
‚îÇ ‚îú‚îÄ/dev/shm
tmpfs
tmpfs
rw,...
‚îÇ ‚îú‚îÄ/dev/pts
devpts
devpts
rw,...
‚îÇ ‚îî‚îÄ/dev/mqueue
mqueue
mqueue
rw,...
‚îú‚îÄ/proc
proc
proc
rw,...
‚îú‚îÄ/sys
sysfs
sysfs
rw,...
‚îÇ ‚îú‚îÄ/sys/kernel/security
securityfs
securityfs
rw,...
‚îÇ ‚îú‚îÄ/sys/fs/cgroup
cgroup2
cgroup2
rw,...
‚îÇ ...
‚îú‚îÄ/run
tmpfs
tmpfs
rw,...
‚îÇ ‚îú‚îÄ/run/lock
tmpfs
tmpfs
rw,...
‚îÇ ‚îî‚îÄ/run/user/1001
tmpfs
tmpfs
rw,...
‚îî‚îÄ/mnt
/dev/vda[/tmp] ext4
rw,...
Copy to clipboardWhen you unshare a new mount namespace, it gets a full copy of the mount table of the caller process.
However, changes to the caller's mount table may be propagated to the new mount table and vice versa.But why? ü§îToday, containers can easily be the main "consumer" of mount namespaces.
However, the applicability of mount namespaces is not limited to containerization use cases.
For example, they can be used to provide per-user views of the filesystem.The original implementation of mount namespaces came out too strict, and it led to tedious repetitive work for system administrators.
To alleviate the problem, the kernel was extended with the mechanism of shared subtrees,
which in particular introduced mount event propagation between peer groups (of mount points).Mount event propagation, visualizedFor instance, if multiple users on the system were using separate mount namespaces to isolate their root filesystems,
without mount event propagation, mounting a new shared volume would require N mount operations, where N is equal to the number of users.
While with mount event propagation, system administrators need to mount the volume only once,
and the change will be replicated in all peer groups, even across different mount namespaces.ü§ì Neither kernel documentation nor the mount namespace man page use the term mount propagation -
instead, they refer to it as propagation type (of a mount point).
However, the term mount propagation seems to be commonly used in the industry,
including in the Docker (example) and Kubernetes (example) documentation.Mount event propagation is exactly what we've just observed when we tried using the unshare system call directly from a Go program:
when the /tmp folder was bind-mounted to the /mnt folder in the new mount namespace,
the original namespace received a mount event and replicated the change creating a similar /tmp:/mnt mount.Hmm... Why didn't it happen when we used the standard unshare command-line tool? ü§îThe unshare CLI tool does slightly more than just the unshare() system call.
You can sneak a peek under the hood of the unshare CLI with the following strace trick (from a fresh terminal):sudo strace unshare --mount bash
Copy to clipboardWhen you cut through the noise of the trace, you'll spot these three important system calls done in a sequence:...
unshare(CLONE_NEWNS)
= 0
mount("none", "/", NULL, MS_REC|MS_PRIVATE, NULL) = 0
execve("/usr/bin/bash", ["bash"], 0x7fff03d0e038 /* 19 vars */) = 0
...
Copy to clipboardRight after creating a new mount namespace and before executing the bash binary,
the unshare command also changed the mount propagation type of the root mount point.
The above mount() call is equivalent to the following mount command:mount --make-rprivate /
Copy to clipboard...which means that in the new mount namespace, the root mount and all its sub-mounts (MS_REC and r in rprivate stand for recursive) become completely isolated from the outside world -
mounting new filesystems inside the mount namespace won't be noticeable in the caller's (i.e., the host's, in our case) mount namespace and vice versa.üí° Mount propagation type is a property of a mount point.
Since each mount point belongs to the corresponding mount namespace,
the mount propagation type is also a namespace-specific property.
For instance, the root mount / can have a shared mount propagation type in one namespace
and private in another.PrivateSharedSlaveNo mount event propagation between namespaces:sudo unshare --mount --propagation private
findmnt -o TARGET,SOURCE,FSTYPE,PROPAGATION
Copy to clipboardTARGET
SOURCE
FSTYPE
PROPAGATION
/
/dev/vda
ext4
private
‚îú‚îÄ/dev
devtmpfs
devtmpfs
private
‚îÇ ‚îú‚îÄ/dev/shm
tmpfs
tmpfs
private
‚îÇ ‚îú‚îÄ/dev/pts
devpts
devpts
private
‚îÇ ‚îî‚îÄ/dev/mqueue
mqueue
mqueue
private
‚îú‚îÄ/proc
proc
proc
private
‚îÇ ‚îî‚îÄ/proc/sys/fs/binfmt_misc
systemd-1
autofs
private
‚îÇ
‚îî‚îÄ/proc/sys/fs/binfmt_misc binfmt_misc binfmt_misc private
‚îú‚îÄ/sys
sysfs
sysfs
private
‚îÇ ‚îú‚îÄ/sys/kernel/security
securityfs
securityfs
private
‚îÇ ‚îú‚îÄ/sys/fs/selinux
selinuxfs
selinuxfs
private
‚îÇ ...
‚îî‚îÄ/run
tmpfs
tmpfs
private
‚îú‚îÄ/run/lock
tmpfs
tmpfs
private
‚îî‚îÄ/run/user/1001
tmpfs
tmpfs
private
Copy to clipboardMount events are propagated in both ways:sudo unshare --mount --propagation shared
findmnt -o TARGET,SOURCE,FSTYPE,PROPAGATION
Copy to clipboardTARGET
SOURCE
FSTYPE
PROPAGATION
/
/dev/vda
ext4
shared
‚îú‚îÄ/dev
devtmpfs
devtmpfs
shared
‚îÇ ‚îú‚îÄ/dev/shm
tmpfs
tmpfs
shared
‚îÇ ‚îú‚îÄ/dev/pts
devpts
devpts
shared
‚îÇ ‚îî‚îÄ/dev/mqueue
mqueue
mqueue
shared
‚îú‚îÄ/proc
proc
proc
shared
‚îÇ ‚îî‚îÄ/proc/sys/fs/binfmt_misc
systemd-1
autofs
shared
‚îÇ
‚îî‚îÄ/proc/sys/fs/binfmt_misc binfmt_misc binfmt_misc shared
‚îú‚îÄ/sys
sysfs
sysfs
shared
‚îÇ ‚îú‚îÄ/sys/kernel/security
securityfs
securityfs
shared
‚îÇ ‚îú‚îÄ/sys/fs/selinux
selinuxfs
selinuxfs
shared
‚îÇ ...
‚îî‚îÄ/run
tmpfs
tmpfs
shared
‚îú‚îÄ/run/lock
tmpfs
tmpfs
shared
‚îî‚îÄ/run/user/1001
tmpfs
tmpfs
shared
Copy to clipboardMount events are propagated from the caller's namespace to the new one (but not backward):sudo unshare --mount --propagation slave
findmnt -o TARGET,SOURCE,FSTYPE,PROPAGATION
Copy to clipboardTARGET
SOURCE
FSTYPE
PROPAGATION
/
/dev/vda
ext4
private,slave
‚îú‚îÄ/dev
devtmpfs
devtmpfs
private,slave
‚îÇ ‚îú‚îÄ/dev/shm
tmpfs
tmpfs
private,slave
‚îÇ ‚îú‚îÄ/dev/pts
devpts
devpts
private,slave
‚îÇ ‚îî‚îÄ/dev/mqueue
mqueue
mqueue
private,slave
‚îú‚îÄ/proc
proc
proc
private,slave
‚îÇ ‚îî‚îÄ/proc/sys/fs/binfmt_misc
systemd-1
autofs
private,slave
‚îÇ
‚îî‚îÄ/proc/sys/fs/binfmt_misc binfmt_misc binfmt_misc private,slave
‚îú‚îÄ/sys
sysfs
sysfs
private,slave
‚îÇ ‚îú‚îÄ/sys/kernel/security
securityfs
securityfs
private,slave
‚îÇ ‚îú‚îÄ/sys/fs/selinux
selinuxfs
selinuxfs
private,slave
‚îÇ ...
‚îî‚îÄ/run
tmpfs
tmpfs
private,slave
‚îú‚îÄ/run/lock
tmpfs
tmpfs
private,slave
‚îî‚îÄ/run/user/1001
tmpfs
tmpfs
private,slave
Copy to clipboardWhy does mount propagation matter for us? Two reasons:pivot_root, the modern chroot alternative most container runtimes rely on,
comes with its own requirements for the mount propagation type of the involved mount points (we'll see it in the next section).Some applications may want to mount filesystems on the host while running in a container and some others may need to spot the host (or peer containers) mounting filesystems in runtime (e.g., HostToContainer and Bidirectional mount propagations in Kubernetes). More on it later.A naive attempt to isolate container filesystemMount namespaces and propagation are great, but how is all this stuff used in containers?
Let's try creating a simple container to see this machinery in action.‚ö†Ô∏è Make sure to exit the namespaced shell in Terminal 1
before proceeding with the commands in this section.Preparing container rootfsFirst off, we'll need to prepare the future root filesystem.
From the host's standpoint, each container's rootfs is just a regular folder with some files inside:sudo mkdir -p /opt/container-1/rootfs
Copy to clipboardFor this experiment, we can "borrow" the Alpine filesystem by extracting the alpine:3 image into the directory we just created:crane export alpine:3 | sudo tar -xvC /opt/container-1/rootfs
Copy to clipboardIf you compare the contents of the /opt/container-1/rootfs and the host's / folders, they will look surprisingly similar:ContainerHosttree -L 1 /opt/container-1/rootfs
Copy to clipboard/opt/container-1/rootfs/
‚îú‚îÄ‚îÄ bin
‚îú‚îÄ‚îÄ dev
‚îú‚îÄ‚îÄ etc
‚îú‚îÄ‚îÄ home
‚îú‚îÄ‚îÄ lib
...
‚îú‚îÄ‚îÄ tmp
‚îú‚îÄ‚îÄ usr
‚îî‚îÄ‚îÄ var
18 directories, 0 files
Copy to clipboardtree -L 1 /
Copy to clipboard/
‚îú‚îÄ‚îÄ bin
‚îú‚îÄ‚îÄ boot
‚îú‚îÄ‚îÄ dev
‚îú‚îÄ‚îÄ etc
‚îú‚îÄ‚îÄ home
...
‚îú‚îÄ‚îÄ tmp
‚îú‚îÄ‚îÄ usr
‚îî‚îÄ‚îÄ var
24 directories, 0 files
Copy to clipboardHowever, upon closer inspection, you'll see that it's two different Linux distributions:ContainerHostcat /opt/container-1/rootfs/etc/os-release
Copy to clipboardNAME="Alpine Linux"
ID=alpine
VERSION_ID=3.22.1
PRETTY_NAME="Alpine Linux v3.22"
HOME_URL="https://alpinelinux.org/"
BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues"
Copy to clipboardcat /etc/os-release
Copy to clipboardPRETTY_NAME="Ubuntu 24.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.3 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
Copy to clipboardSwitching to new rootfs (pivot_root)The pivot_root(new_root, put_old) syscall changes the root mount in the mount namespace of the calling process.
More precisely, it moves the current root mount of the caller to the directory put_old and makes new_root the new root mount.What it practically means is that by calling pivot_root("/opt/container-1/rootfs") in a new mount namespace,
we'll switch to the new root filesystem.üí° From a layman's standpoint, pivot_root is a safer version of chroot - similar effect but no risk of breakouts via forgotten symlinks to the old root filesystem or the double-chroot trick.The pivot_root() call comes with a number of restrictions, in particular:The new_root path must be a mount point, but can't be / (well, attempting pivot_root("/") wouldn't make much sense anyway).The propagation type of the parent mount of new_root and the parent mount of the current root directory must not be shared.If put_old is an existing mount point, its propagation type must not be shared.Expectedly, we only want to perform such a disruptive operation from a separate mount namespace (otherwise, we'd damage the host),
and the last two restrictions ensure that pivot_root never propagates any mount table changes to another mount namespace:sudo unshare --mount bash
Copy to clipboardNow let's try satisfying the pivot_root's requirements.
The propagation type of the / mount (the parent mount of the current root directory) should not be shared.
The above unshare command has likely already set it to private but being explicit won't hurt:mount --make-rprivate /
Copy to clipboardüí° Interesting fact: runc uses rslave instead of rprivate citing a possibility of a race condition. But both values should be good enough for our demo example.In our case, the /opt/container-1/rootfs folder is not a mount point (it's a regular folder somewhere in the host's filesystem),
but we can easily make it a mount point by bind mounting the path onto itself (using a recursive bind mount because hypothetically the container rootfs folder itself can contain sub-mounts):mount --rbind /opt/container-1/rootfs /opt/container-1/rootfs
Copy to clipboardLastly, ensuring that the propagation type of the new_root itself isn't shared:mount --make-rprivate /opt/container-1/rootfs
Copy to clipboardNow we're ready to choort pivot the root filesystem:cd /opt/container-1/rootfs
mkdir .oldroot
Copy to clipboardpivot_root . .oldroot
Copy to clipboard...and immediately after that, switch to a shell from the new rootfs because the current bash process may get broken in subtle ways after a pivot_root into a completely different Linux distro (this part is only needed for our demo example -
real-world container runtimes usually don't have this issue because they communicate with the kernel directly, using syscalls instead of shell commands):exec /bin/sh
Copy to clipboardInterestingly, after the pivot_root operation,
container runtimes are free to set the propagation type of the new root filesystem to pretty much any value
(shared, slave, private, and even unbindable):mount --make-rslave /
Copy to clipboardüí° Propagation type of the container root filesystem should not be confused with the propagation type of bind mounts and volumes in Docker and Kubernetes respectively (see below).
This is an advanced setting that is often not even exposed through the user-facing APIs of the higher-level container runtimes,
and the most typical use case for it is nested containers (e.g., DinD).Finally, since you probably don't want the original root filesystem to be accessible in the container,
the .oldroot can (and should) be removed right after the pivot_root call:umount -l .oldroot
# -l stands for "lazy" because the fs can be busy
rm -rf .oldroot
Copy to clipboardYay! We've just pivoted into a new container. Let's look around:ls -lah /
Copy to clipboardls -l /
total 68
drwxr-xr-x
2 root
root
4096 Jul 15 10:42 bin
drwxr-xr-x
2 root
root
4096 Sep
7 12:40 dev
drwxr-xr-x
17 root
root
4096 Jul 15 10:42 etc
drwxr-xr-x
2 root
root
4096 Jul 15 10:42 home
...
drwxr-xr-x
11 root
root
4096 Jul 15 10:42 var
Copy to clipboardcat /etc/os-release
Copy to clipboardNAME="Alpine Linux"
ID=alpine
VERSION_ID=3.22.1
PRETTY_NAME="Alpine Linux v3.22"
HOME_URL="https://alpinelinux.org/"
BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues"
Copy to clipboardSo far so good! But if you try listing processes, the output will be empty (which of course can't be true):ps aux
Copy to clipboardPID
USER
TIME
COMMAND
Copy to clipboardAnd the df command also seems broken:df -ah
Copy to clipboardFilesystem
Size
Used Available Use% Mounted on
df: /proc/mounts: No such file or directory
Copy to clipboardPreparing a complete container filesystemThe df's error message contained a hint - the /proc folder is empty in the new mount namespace:ls -l /proc
Copy to clipboardtotal 0
Copy to clipboardHmm... How come?Well, apparently, not every part of the container root filesystem comes from its image!Similarly to the host, where /proc is populated by the corresponding kernel pseudo filesystem,
container's /proc needs to be set up separately. And the same goes for /dev and /sys virtual filesystems.On top of that, some special files like /etc/hosts, /etc/hostname, or /etc/resolv.conf should be crafted for each container individually
because the corresponding files in the image (if present) can only contain generic values (e.g., localhost) while Docker typically sets the hostname of a container to a prefix of its random ID and derives the resolv.conf from the eponymous file on the host.Populating /proc pseudo filesystemPopulating the /proc pseudo filesystem is as simple as:mount -t proc proc /proc
Copy to clipboardüí° In reality, container runtimes usually populate the /proc filesystem before the pivot_root call,
so the command would look like mount -t proc proc $ROOTFS/proc.However, if you run the above command right away, the /proc filesystem in the container will look exactly the same as the one on the host.
In particular, it means that the ps command will start showing the full list of processes on the server,
which is usually undesirable in a container.This is where the PID namespace comes into play.
We need to go a few steps back and adjust the unshare command to create not just the mount but also a new PID namespace,
so that the container's topmost process would become PID 1 and the process hierarchy in the container would start from it:# DO NOT RUN ME
sudo unshare --mount --pid --fork bash
Copy to clipboardBut let's not do it just yet...üí° The extra --fork flag above doesn't create any new namespaces, but rather makes unshare create a new process instead of exec'ing the bash command directly.
This is a requirement to make the --pid flag actually have the effect on the unshared command
because it's the first child that gets placed into the new PID namespace, not the process that called unshare(CLONE_NEWPID) itself.Populating /dev pseudo filesystemAnother special folder is /dev. On the host, it's typically provided by the devtmpfs and a number of subordinate virtual filesystems
(from a fresh terminal):findmnt
Copy to clipboardTARGET
SOURCE
FSTYPE
OPTIONS
/
/dev/vda ext4
rw,relatime,stripe=4
‚îú‚îÄ/dev
devtmpfs devtmpfs rw,relatime,size=4068368k,nr_inodes=1017092,mode=755
‚îÇ ‚îú‚îÄ/dev/shm
tmpfs
tmpfs
rw,nosuid,nodev
‚îÇ ‚îú‚îÄ/dev/pts
devpts
devpts
rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000
‚îÇ ‚îú‚îÄ/dev/mqueue
mqueue
mqueue
rw,nosuid,nodev,noexec,relatime
...
Copy to clipboardHowever, containers usually get a more limited version of the /dev folder, backed by a regular tmpfs.
Here is how it can be populated from inside the new mount namespace (back from Terminal 1):mkdir -p /dev
mount -t tmpfs -o nosuid,strictatime,mode=0755,size=65536k tmpfs /dev
Copy to clipboardüí° In reality, container runtimes usually populate the /dev filesystem before the pivot_root call,
so the command would look like mount -t tmpfs ... $ROOTFS/dev.After mounting the /dev tmpfs,
you'd need to create special character devices such as /dev/null, /dev/zero, /dev/random, etc.
Here is how you can do it using the mknod command:mknod -m 666 /dev/null
c 1 3
chown root:root /dev/null
mknod -m 666 /dev/zero
c 1 5
chown root:root /dev/zero
# etc.
Copy to clipboardThen, mount the subordinate filesystems (/dev/shm, /dev/pts, and /dev/mqueue):mkdir -p /dev/shm
mount -t tmpfs -o nosuid,nodev,noexec,mode=1777,size=67108864 tmpfs /dev/shm
Copy to clipboardmkdir -p /dev/pts
mount -t devpts -o newinstance,ptmxmode=0666,mode=0620 devpts /dev/pts
Copy to clipboardmkdir -p /dev/mqueue
mount -t mqueue -o nosuid,nodev,noexec mqueue /dev/mqueue
Copy to clipboardAnd lastly, set up some well-known symlinks:ln -sf /proc/self/fd
/dev/fd
ln -sf /proc/self/fd/0 /dev/stdin
ln -sf /proc/self/fd/1 /dev/stdout
ln -sf /proc/self/fd/2 /dev/stderr
ln -sf /proc/kcore
/dev/core
Copy to clipboardPopulating /sys pseudo filesystemThe most limited of the containers' pseudo filesystems is probably /sys.
It's usually mounted read-only and contains only a few nodes:mount -t sysfs -o ro,nosuid,nodev,noexec sysfs /sys
Copy to clipboardüí° In reality, container runtimes usually populate the /sys filesystem before the pivot_root call,
so the command would look like mount -t sysfs ... $ROOTFS/sys.A prominent part of the /sys filesystem is the virtual cgroup filesystem.
Since a few years ago, Docker and other popular container runtimes started fully isolating the container's cgroup hierarchy by default.
Similarly to the /proc filesystem that works best in combination with a new PID namespace,
a new cgroup namespace can be used to make the cgroup2 mount rooted at the host's cgroupfs node that corresponds to the container's topmost process.
Thus, the unshare command would need one more flag, --cgroup:# DO NOT RUN ME
sudo unshare --mount --pid --fork --cgroup bash
Copy to clipboardTo mount the cgroup2 filesystem, you can use the following command:mkdir -p /sys/fs/cgroup
mount -t cgroup2 -o ro,nosuid,nodev,noexec cgroup2 /sys/fs/cgroup
Copy to clipboardHardening pseudo filesystemsWhile it is not strictly necessary for a demo, real-world container root filesystems usually go through an extra round of hardening.
For instance, Docker typically marks a few parts of the /proc filesystem as read-only and masks others, making them completely inaccessible to the containerized app.Here is how you can get a list of sensitive locations that are made read-only by Docker
(from a fresh terminal):docker container inspect \
$(docker run --rm -d alpine:3 sleep 5) \
--format '{{join .HostConfig.ReadonlyPaths "\n"}}'
Copy to clipboard/proc/bus
/proc/fs
/proc/irq
/proc/sys
/proc/sysrq-trigger
Copy to clipboardYou can make any file or folder read-only by binding it to itself and remounting it using the ro option:RO_PATH=/proc/bus # or /proc/fs, /proc/irq, etc.
if [ -e "$RO_PATH" ]; then
mount --bind "$RO_PATH" "$RO_PATH"
mount -o remount,bind,ro "$RO_PATH"
fi
Copy to clipboardSimilarly, here is how you can get a list of locations that are typically made completely inaccessible (through masking) to the containerized app:docker container inspect \
$(docker run --rm -d alpine:3 sleep 5) \
--format '{{join .HostConfig.MaskedPaths "\n"}}'
Copy to clipboard/proc/asound
/proc/acpi
/proc/interrupts
/proc/kcore
/proc/keys
/proc/latency_stats
/proc/timer_list
/proc/timer_stats
/proc/sched_debug
/proc/scsi
/sys/firmware
/sys/devices/virtual/powercap
Copy to clipboardMasking of folders and regular files differs. To mask a folder, a read-only tmpfs filesystem can be mounted over it,
and to mask a regular file, the /dev/null device can be bound to its path.MASKED_FILE=/proc/asound
# or /proc/interrupts, /proc/kcore, etc.
mount --bind /dev/null $MASKED_FILE
MASKED_DIR=/proc/acpi
# or /proc/scsi, etc.
mount -t tmpfs -o ro tmpfs $MASKED_DIR
Copy to clipboardüí°
The above read-only and masked paths are Docker's defaults for non-privileged containers,
while the OCI Runtime Spec defines only the hardening mechanism and not the exact locations
(see Masked Paths and Readonly Paths).Preparing special /etc filesSome of the regular files in the container rootfs also require special treatment:/etc/hosts/etc/hostname/etc/resolv.confInspecting these files in the /opt/container-1/rootfs folder right after extracting the Alpine rootfs into it would reveal why:cat /opt/container-1/rootfs/etc/{hosts,hostname,resolv.conf}
Copy to clipboard# -- /opt/container-1/rootfs/etc/hosts
127.0.0.1
localhost localhost.localdomain
::1
localhost localhost.localdomain
# -- /opt/container-1/rootfs/etc/hostname
localhost
# -- /opt/container-1/rootfs/etc/resolv.conf
cat: /opt/container-1/rootfs/resolv.conf: No such file or directory
Copy to clipboardThe above are some generic values that come directly from the alpine:3 image,
which wouldn't make much sense in any particular container.
At the same time, these files would look very different when inspected from a running alpine:3 container:docker run --rm alpine:3 cat /etc/{hosts,hostname,resolv.conf}
Copy to clipboard# -- /etc/hosts
127.0.0.1
localhost
::1
localhost ip6-localhost ip6-loopback
172.17.0.2
2f26e97ae70c
# -- /etc/hostname
2f26e97ae70c
# -- /etc/resolv.conf
# Generated by Docker Engine.
# This file can be edited; Docker Engine will not make further changes once it
# has been modified.
nameserver 168.119.149.157
nameserver 8.8.8.8
nameserver 1.1.1.1
# Based on host file: '/etc/resolv.conf' (legacy)
# Overrides: []
Copy to clipboardThus, Docker (or one of its underlying runtimes) replaces the generic /etc/hosts,
/etc/hostname, and /etc/resolv.conf files from the image with container-specific variants.We can do it, too! Our container has no network interfaces (modulo loopback),
but it can still have a proper hostname set (from the host's terminal):cat <<EOF | sudo tee /opt/container-1/hosts
127.0.0.1
localhost container-1
::1
localhost ip6-localhost ip6-loopback
EOF
Copy to clipboardcat | sudo tee /opt/container-1/hostname <<EOF
container-1
EOF
Copy to clipboardsudo cp /etc/resolv.conf /opt/container-1/resolv.conf
Copy to clipboardüí° The /etc/resolv.conf file is usually based on the host's /etc/resolv.conf file,
and then potentially adjusted to the container's needs.The most interesting part is how these files are placed into the container's rootfs.
Instead of just overwriting the files from the image,
container runtimes usually mount the container-specific variants of these files on top of the original ones,
effectively masking them:sudo mount --bind /opt/container-1/hosts /opt/container-1/rootfs/etc/hosts
sudo mount --bind /opt/container-1/hostname /opt/container-1/rootfs/etc/hostname
sudo mount --bind /opt/container-1/resolv.conf /opt/container-1/rootfs/etc/resolv.conf
Copy to clipboardLast but not least, for the container to have its own hostname,
the container needs to use a new network and UTS namespaces,
so the unshare command would need to have two more flags (--uts and --net):# DO NOT RUN ME
sudo unshare --mount --pid --fork --cgroup --uts --net bash
Copy to clipboardüí° If we forget to use a new UTS namespace, setting the hostname in the new container will overwrite the host's hostname,
which is something we definitely don't want.
And without a new network namespace, the container simply cannot have its own hostname,
because then it technically has the same network stack as the host (which in particular includes the hostname).Finally, we're ready to prepare a fully isolated container filesystem!Creating a container from scratch (end-to-end example)With all the above lessons learned, let's try creating our second container,
this time applying all the necessary namespaces and rootfs adjustments.üí° The below commands are based on the real container preparation steps taken by the runc runtime
obtained with the following strace trick:# Terminal 1
sudo strace -f -qqq -e \
trace=/clone,/exec,/unshare,/mount,/mknod,/mkdir,/link,/chdir,/root \
-p $(pgrep containerd)
Copy to clipboard# Terminal 2
docker run alpine:3 sleep 9999
Copy to clipboardStep 1: Prepare rootfs files‚ö†Ô∏è Make sure to exit the namespaced shell in Terminal 1
before proceeding with the commands in this section.The second container will be stored in the /opt/container-2 directory:CONTAINER_DIR=/opt/container-2
ROOTFS_DIR=${CONTAINER_DIR}/rootfs
Copy to clipboardSimilar to the first container, we'll use the alpine:3 image to "borrow" the rootfs files:sudo mkdir -p $ROOTFS_DIR
crane export alpine:3 | sudo tar -xvC $ROOTFS_DIR
Copy to clipboardThis time, we'll create the /etc/hosts, /etc/hostname, and /etc/resolv.conf files beforehand
(but store them outside of the rootfs dir for now):cat <<EOF | sudo tee $CONTAINER_DIR/hosts
127.0.0.1
localhost container-2
::1
localhost ip6-localhost ip6-loopback
EOF
Copy to clipboardcat <<EOF | sudo tee $CONTAINER_DIR/hostname
container-2
EOF
Copy to clipboardsudo cp /etc/resolv.conf $CONTAINER_DIR/resolv.conf
Copy to clipboardStep 2: Create namespacesCreate all the required namespaces with the unshare command (mount, PID, cgroup, UTS, and network):sudo unshare --mount --pid --fork --cgroup --uts --net bash
Copy to clipboardüí° Other possible namespaces are:ipc - this namespace has no impact on the rootfs creation, so we're skipping it for brevitytime - (optional) not used by Docker or other mainstream container runtimes yetuser - (optional) rootless containers is an advanced topic that deserves its own tutorialStep 3: Isolate new mount namespaceFrom now on, all commands are executed as root and in the new namespaces, so we're skipping the sudo prefix,
and the CONTAINER_DIR and ROOTFS_DIR variables may need to be re-set:CONTAINER_DIR=/opt/container-2
ROOTFS_DIR=${CONTAINER_DIR}/rootfs
Copy to clipboardFirst, we need to make sure that no mount events are propagated back to the host's mount namespace:mount --make-rslave /
Copy to clipboardThen, we need to make sure that the root filesystem itself is a mount point:mount --rbind $ROOTFS_DIR $ROOTFS_DIR
Copy to clipboard...and that the propagation type of the root filesystem isn't shared:mount --make-private $ROOTFS_DIR
Copy to clipboardStep 4: Prepare /proc pseudo filesystemMount /proc pseudo filesystem:mkdir -p $ROOTFS_DIR/proc
mount -t proc proc $ROOTFS_DIR/proc
Copy to clipboard‚ö†Ô∏è Security Caveat: In untrusted rootfs, $ROOTFS_DIR/<path> can be a symlink pointing outside of $ROOTFS_DIR.
This can make the above and many of the below operations corrupt the host system.Real-world container runtimes typically use the openat2() syscall with the RESOLVE_NO_SYMLINKS flag to first open the target file or directory ensuring it's not a symlink,
and then use mount (or other filesystem operations) on an open file descriptor instead of a textual filename.
The latter helps to avoid TOCTTOU vulnerabilities
when the $ROOTFS_DIR contents are changed while the container is being created.However, in a demo context it should be relatively safe to operate with regular filenames.
So, we'll do it the simpler way for brevity.Step 5: Prepare /dev pseudo filesystemMount /dev pseudo filesystem as a regular tmpfs:mount -t tmpfs \
-o nosuid,strictatime,mode=0755,size=65536k tmpfs \
$ROOTFS_DIR/dev
Copy to clipboardCreate the standard character devices (/dev/null, /dev/zero, /dev/random, etc.):mknod -m 666 "$ROOTFS_DIR/dev/null"
c 1 3
mknod -m 666 "$ROOTFS_DIR/dev/zero"
c 1 5
mknod -m 666 "$ROOTFS_DIR/dev/full"
c 1 7
mknod -m 666 "$ROOTFS_DIR/dev/random"
c 1 8
mknod -m 666 "$ROOTFS_DIR/dev/urandom" c 1 9
mknod -m 666 "$ROOTFS_DIR/dev/tty"
c 5 0
chown root:root "$ROOTFS_DIR/dev/"{null,zero,full,random,urandom,tty}
Copy to clipboardCreate typical symlinks:ln -sf /proc/self/fd
"$ROOTFS_DIR/dev/fd"
ln -sf /proc/self/fd/0 "$ROOTFS_DIR/dev/stdin"
ln -sf /proc/self/fd/1 "$ROOTFS_DIR/dev/stdout"
ln -sf /proc/self/fd/2 "$ROOTFS_DIR/dev/stderr"
ln -sf /proc/kcore
"$ROOTFS_DIR/dev/core"
Copy to clipboardCreate subordinate filesystems (/dev/pts, /dev/shm, /dev/mqueue):mkdir -p "$ROOTFS_DIR/dev/pts"
mount -t devpts \
-o newinstance,ptmxmode=0666,mode=0620 devpts \
$ROOTFS_DIR/dev/pts
ln -sf /dev/pts/ptmx "$ROOTFS_DIR/dev/ptmx"
Copy to clipboardmkdir -p "$ROOTFS_DIR/dev/mqueue"
mount -t mqueue \
-o nosuid,nodev,noexec mqueue \
$ROOTFS_DIR/dev/mqueue
Copy to clipboardmkdir -p "$ROOTFS_DIR/dev/shm"
mount -t tmpfs \
-o nosuid,nodev,noexec,mode=1777,size=67108864 tmpfs \
$ROOTFS_DIR/dev/shm
Copy to clipboardStep 6: Prepare /sys pseudo filesystemMount a read-only /sys pseudo filesystem:mkdir -p "$ROOTFS_DIR/sys"
mount -t sysfs \
-o ro,nosuid,nodev,noexec sysfs \
$ROOTFS_DIR/sys
Copy to clipboardMount the subordinate cgroup2 filesystem as /sys/fs/cgroup:mkdir -p "$ROOTFS_DIR/sys/fs/cgroup"
mount -t cgroup2 \
-o ro,nosuid,nodev,noexec cgroup2 \
$ROOTFS_DIR/sys/fs/cgroup
Copy to clipboardStep 7: Bind hostname, hosts, and resolv.conf filesBind the container-specific hostname, hosts, and resolv.conf files from /opt/container-2,
masking the original files in the rootfs' /etc directory:for p in hostname hosts resolv.conf
do
touch $ROOTFS_DIR/etc/$p
mount --bind "$CONTAINER_DIR/$p" $ROOTFS_DIR/etc/$p
done
Copy to clipboardStep 8: Pivot into the new rootfsFinally, pivot into the fully prepared root filesystem:cd $ROOTFS_DIR
mkdir -p .oldroot
pivot_root . .oldroot
Copy to clipboardThis is not something a real runtime would do, but since we use a shell,
it's better to exec into the target container's shell
as soon as possible after the pivot_root call:exec /bin/sh
Copy to clipboardConfigure the propagation type of the container's root filesystem
(setting it arbitrarily to slave, but the OCI Runtime Specification supports private and even shared):mount --make-rslave /
Copy to clipboardAnd lastly, getting rid of the link to the old root filesystem:umount -l .oldroot
rmdir .oldroot
Copy to clipboardSet the hostname of the container using the value from the container's /etc/hostname file:hostname $(cat /etc/hostname)
Copy to clipboardStep 9: Harden container filesystemMaking a good part of the /proc filesystem read-only:for d in bus fs irq sys sysrq-trigger
do
if [ -e "/proc/$d" ]; then
mount --bind "/proc/$d" "/proc/$d"
mount -o remount,bind,ro "/proc/$d"
fi
done
Copy to clipboardMasking sensitive paths in the /proc and /sys filesystems:for p in \
/proc/asound \
/proc/interrupts \
/proc/kcore \
/proc/keys \
/proc/latency_stats \
/proc/timer_list \
/proc/timer_stats \
/proc/sched_debug \
/proc/acpi \
/proc/scsi \
/sys/firmware
do
if [ -d "$p" ]; then
# Masking a folder
mount -t tmpfs -o ro tmpfs $p
elif [ -f "$p" ]; then
# Masking a regular file
mount --bind /dev/null $p
fi
done
Copy to clipboardStep 10: Execute target applicationAt this point, the containerized environment is ready to be used.
Feel free to look around using the ps, ls, mount, df, hostname,
and any other commands you can think of, and then exec the containerized application:APP=${APP:-/bin/sh}
exec $APP
Copy to clipboardBonus: Sharing host files and folders with containersOne of the very common Docker use cases, especially during local development,
is sharing files and folders from the host into the container via bind mounts like this:# Traditional -v|--volume flag
docker run -v ./data:/data redis
# More modern but equivalent --mount form
docker run --mount type='bind,src=./data,dst=/data' redis
Copy to clipboardIn the previous section(s), we saw that regular files located on the host can be bind mounted into the future container's root filesystem.
This is exactly how Docker runc and similar container runtimes inject the customized /etc/hosts, /etc/hostname, and /etc/resolv.conf files.But the exact same technique can be used to inject any other files or folders from the host into the container.The strace -p $(pgrep containerd) command that we used to reverse engineer the rootfs preparation steps
will reveal that the bind mounts of the -v|--volume flag happen right after the pseudo filesystems preparation
and just before the mounts of the /etc/hosts, /etc/hostname, and /etc/resolv.conf files.And it's a good thing we invested some time in learning about the mount event propagation mechanism -
Docker allows configuring the propagation type for bind mounts,
so the following command should not look like a magic spell anymore:docker run -v .:/project:ro,rshared ...
Copy to clipboardIn the above example, if the containerized application would mount a sub-folder under /project,
it would be visible on the host as well (and vice versa).
However, the default propagation type of a Docker bind mount is rprivate,
so don't be surprised if you don't see sub-mounts showing up.Bonus: Adding support for data volumesWhile Docker docs position volumes as a distinct concept,
under the hood, they are just bind mounts,
but with a few extra features like naming, lifecycle management, and various data source drivers support:# Traditional -v|--volume flag
docker run --volume redis-data:/data redis
# More modern but equivalent --mount form
docker run --mount type='volume,src=redis-data,dst=/data' redis
Copy to clipboardInstead of arbitrary folders on the host, volume data is always stored in /var/lib/docker/volumes/CONTAINER_ID/_data,
and you can list all existing volumes with the docker volume ls command, or create new ones with docker volume create,
or even purge them with docker volume rm.
But at the end of the day, you're just listing, creating, or removing _data folders in the /var/lib/docker/volumes directory.Interesting that Docker always sets mount propagation for volumes to rprivate (for bind mounts you could tweak it),
while Kubernetes, despite relying on the exact same runc (or the like) runtime under the hood,
allows more flexible mount propagation configuration (HostToContainer, Bidirectional, etc.).So, in Docker, bind mounts vs. volumes is more of a semantic difference
(and induced artificial constraints on the data location and propagation type)
than an actual technical difference.Where do union filesystems come into play?One of the things we didn't talk about in this article is union filesystems like overlayfs -
simply because despite popular belief, they're not mandatory for containers.As we just proved with the above demo, it's possible to create a fully-fledged container without relying on a union filesystem at all.
Docker uses overlay2 (or an alternative) storage driver to unpack layered container images into "flat" local folders.
However, this is only an optimization, mainly focused on the disk space efficiency -
as we just saw, it's possible to extract a container image filesystem into a regular folder with crane export (or a similar command),
and the container runtime (e.g., runc) will happily use it as a root filesystem.SummarizingAt the heart of containers lies the mount namespace.
That's not an accident - Linux has long treated the filesystem as the central interface for managing processes, devices, and resources.
Once you start assembling a root filesystem for a container, it quickly becomes clear that other namespaces -
PID, cgroup, UTS, and network - are interconnected and much needed to complete the task.This is why walking through the rootfs exercise isn't just an impressive low-level demo you could give at a conference.
It's a way to build a comprehensive mental model of how containers work.
And with that model in place, higher-level topics like bind mounts, volumes, mount propagation,
and persistence in Docker or Kubernetes stop feeling like special cases - they become natural extensions of the same foundation.Ah, and if you made it this far, take another look at the diagram from the opening part -
it should make much more sense now!Click to enlargeResourcesnamespaces(7) ‚Äî Linux manual pagemount_namespaces(7) ‚Äî Linux manual pagemount(2) ‚Äî Linux manual pagepivot_root(2) ‚Äî Linux manual pageShared Subtrees - Linux kernel documentationMount namespaces and shared subtrees - LWN.netMount namespaces, mount propagation, and unbindable mounts - LWN.netMounts - OCI Runtime Specificationrootfs_linux.go - runc source codeBind mounts - Docker documentationVolumes - Docker documentationOverlayFS storage driver - Docker documentationMount propagation - Kubernetes documentationMounting into mount namespaces - Christian Brauner's blogAttach a volume to a container while it is running - J√©r√¥me Petazzoni's blogUnderstanding the various mounts setup by a Docker container - Sid Agrawal's blogPractice Level up your Server Side game ‚Äî Join 13,000 engineers who receive insightful learning materials straight to their inbox
Subscribe

Next up...

Story 17: Mystery in the Moon
In the mid-13th century, a Japanese noblewoman embarked on the long journey from Kyoto to Kamakura. Abutsu (c. 1225-83) was a troubled woman: a difficult break-up had forced her to take refuge in a nunnery, and she had experienced extreme poverty as a single mother. But then, she wrote, ‚ÄòI decided to forget my countless fears, abandon all thought of myself, and go forth abruptly, enticed by the waning Moon.‚Äô Throughout her journey on the road, it remained a companionable presence, marking time and inspiring her to poetry; often, she admitted, ‚ÄòI have gazed simply at the Moon all night long.‚Äô

Others found the Moon a more troubling presence, cold and inconstant. Some thought they could see mysterious creatures on its dappled face: the Korean poet Yun Seon-do (1587-1671) wrote of ‚ÄòA jade hare on the Moon [which] pounds out the medicine.‚Äô And, on several terrifying occasions in the late 1170s, the Moon briefly seemed to wriggle and writhe. The monk Gervase of Canterbury, who witnessed this strange phenomenon (which was probably caused by atmospheric turbulence), thought that it looked like ‚Äòa stricken snake‚Ä¶ twisted up as though in anxiety‚Äô.

Such fearsome illusions were, fortunately, extremely rare; in contrast, sightings of the man in the Moon were extremely common, though not everyone could see him. The 14th-century mathematician Albert of Saxony complained that, however hard he tried, he saw only black spots. But popular belief in his existence was reinforced by stories of a man who had been exiled to space, either because he had stolen something (maybe cabbages, maybe sheep), or because he had placed thorns on the path to stop people going to church.

Some people seem to have been disturbed by this unfortunate being and his cold, lonely life; in the Middle English poem ‚ÄòThe Man in the Moon‚Äô (c. 1300), the speaker laments that ‚ÄòThe Man hears me not, though I cry out to him.‚Äô Others were intrigued by the possibility that there was life out there, and dreamed of space travel. According to legend, the Emperor Xuanzong (r. 712-56) was sent to the Moon by the goddess Chang‚Äôe, while Ludovico Ariosto‚Äôs The Frenzy of Orlando (1532) imagined the knight Astolfo‚Äôs journey to the Moon to recover his cousin‚Äôs lost wits. There he discovered an impressive civilisation (‚ÄòCities and castles on the Moon abound/ The size of houses with amazement fills‚Äô), as well as vast amounts of earthly detritus; in one of these huge piles of broken promises, lovers‚Äô sighs, and tears, he found Orlando‚Äôs lost reason, safely stored as liquid in a corked flask.

Medieval writers also imagined what might happen if Moon-dwellers came to Earth. Some of these fantasies were rather sinister: Geoffrey of Monmouth‚Äôs History of the Kings of Britain (c. 1136) included stories of incubus demons who frequently travelled back and forth between Earth and the Moon, when they were not busy having sex with human women. But others, such as The Tale of the Bamboo Cutter, are rather moving. In this tenth-century Japanese story, an elderly couple find and raise a tiny child. She grows up to be a beautiful young woman, loving and popular; even the Emperor becomes her friend. But things are not as they seem, for Kaguya-hime is a Moon-princess, and soon her people arrive to rescue her from this ‚Äòfilthy place‚Äô. Carrying her off in a silk-canopied chariot, they drape her in a feather robe which makes her forget her earthly friends, so that she cannot remember those who grieve for her.

Nevertheless, it offers intriguing comparative insights; we learn, for example, that the Moon was a popular image in many religions, albeit one that was used in contradictory ways. Christian theologians liked to explain that, just as the Moon reflects the light of the Sun, so the Church reflects the light of God. Pope Innocent III came up with an alternative metaphor: for him, the Pope was the Sun, and the Holy Roman Emperor a mere moon reflecting the glorious papal light. Similarly, in Ancrene Wisse (a 13th-century handbook for female recluses) the Moon serves as a symbol of female foolishness ‚Äì yet it was also associated with the Virgin Mary, who was often shown standing on the full moon, a symbol of the Immaculate Conception.

Lunar imagery was equally prevalent in non-Christian traditions, with Sufi mystics using the Moon to represent Allah; for the Andalusian poet Ibn ‚ÄòArabƒ´ (1165-1240), his divine beloved was ‚Äòmajestic, a full Moon risen within me, a Moon that never sets‚Äô. Mƒ´rƒÅbƒÅƒ´ (1500-46), an Indian princess who left her husband to devote herself to Krishna, expressed her devotion to this Hindu god in similarly evocative terms. Life without him would, she claimed, be like ‚Äòa night without the Moon‚Äô; in other verses she compared herself to a lotus flower opening in the moonlight, and a bird enraptured at the sign of the Moon.

This influence was clearest in relation to human health, both physical and mental: the mad were widely described as ‚Äòlunatic, that is mad at certain times of the moon‚Äô, and some authorities even suggested that a lunar eclipse in March 1345 was to blame for the Black Death. Consequently, physicians consulted lunar tables before preparing drugs or administering treatments; in Baghdad, the physician Bukht√≠sh≈´ ibn Jibr√≠l (d. 870) would only administer enemas when the Moon was descending. Six centuries later, the English surgeon John of Arderne (a skilful and forward-thinking practitioner, who also advocated for anaesthesia, pain relief, and clean hands) recommended that ‚Äòa surgeon ought not cut or burn any member of a person‚Äôs body, nor do phlebotomy, while the Moon is in a sign governing or betokening that member‚Äô.

Of course, a cynic might suggest that there was a more straightforward explanation for the thief‚Äôs good luck. Medieval literature was full of shenanigans on moonless nights, and of sinners thwarted by an unusually bright moon ‚Äì as in Dafydd ap Gwilym‚Äôs poem Y lleuad (‚ÄòThe Moon‚Äô), in which a man curses the Moon because its bright light thwarts his plan to visit his mistress. Such literary japes remind the modern reader how much things have changed. In our highly electrified world, we are no longer dependent on the Moon as a source of light. And yet, reading this sensitive study, it is hard not to feel a deep sense of connection with those long-ago skygazers. This celestial sphere has lost none of its mystery and allure.

Next up...

Story 18: Show HN: A PSX/DOS style 3D game written in Rust with a custom software renderer
A Scavenging Trip is a short and challenging simulation game were you are tasked with visiting an unknown planet, collect samples from its surface and leave in time. There are 3 missions with 3 difficulty levels each.

A perfect speedrun through all difficulties should take around 10-15 minutes. A first playthrough might take 1-2 hours, especially because of the hardest difficulty.

There is not a Save feature, as all missions are unlocked and there is not any kind of progression whatsoever (although the order of the missions puts them into context).

The game uses minimal controls, which are rebindable through the Controls menu. Default scheme is WASDQE. Gamepads are currently not supported. The mouse is not used, so it's equivalently playable on a desktop or a laptop.

The system requirements are very low; if you are able to open this web page on a modern browser then most probably you can run the game at >30 fps. Any GPU from the last 25 years should do. It's recommended to have a Pentium M or better processor.

More exotic builds are possible for OSes supported by SDL2 and a relatively recent version of the Rust compiler (for example BSDs, Windows 7).

Some more technical info for the curious

The game is developed using a custom software renderer and custom engine written in the Rust programming language. "Software" rendering in this context means that all graphics-related calculations are done by the CPU. The GPU is used only to present the final image to the display (and scale it depending on the window size).

That wraps up today's HackerCast. Thank you for listening, and we'll see you tomorrow with more stories from the world of technology.